{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning for Citation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# from operator import add\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from dspy.evaluate import Evaluate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_candidate_data = pd.read_csv('~/test.qrel.cid', sep=' ', header=None, names=['query', 'candidate', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(query_papers): 115\n",
      "len(candidate_papers): 637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('/Users/jamie/qpaper_to_emb', 'r') as f:\n",
    "    query_papers = [line.strip() for line in f]\n",
    "\n",
    "with open('/Users/jamie/cpaper_to_emb', 'r') as f:\n",
    "    candidate_papers = [line.strip() for line in f]\n",
    "\n",
    "print(f'len(query_papers): {len(query_papers)}')\n",
    "print(f'len(candidate_papers): {len(candidate_papers)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     query candidate  bool\n",
      "0  3498240   1824499     1\n",
      "1  3498240  53645322     0\n",
      "2  3498240   1915951     0\n",
      "3  3498240   3048298     0\n",
      "4  3498240   3627503     0\n",
      "Number of query candidate pairs with valid files: 651\n"
     ]
    }
   ],
   "source": [
    "counter_4 = 0\n",
    "valid_rows = pd.DataFrame()\n",
    "query_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers'\n",
    "candidate_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined'\n",
    "# Iterate over the rows of the data\n",
    "for _, row in query_candidate_data.iterrows():\n",
    "    query_file = os.path.join(query_dir, str(row['query']) + '.pdf')\n",
    "    candidate_file = os.path.join(candidate_dir, str(row['candidate']) + '.pdf')\n",
    "\n",
    "    # Check if both files exist\n",
    "    if os.path.isfile(query_file) and os.path.isfile(candidate_file):\n",
    "        # If both files exist, append the row to valid_rows\n",
    "        valid_rows = valid_rows._append(row)\n",
    "        \n",
    "# Reset the index of valid_rows\n",
    "valid_rows.reset_index(drop=True, inplace=True)\n",
    "print(valid_rows.head())\n",
    "print(f'Number of query candidate pairs with valid files: {len(valid_rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"query_file\": query_file, \"candidate_file\": candidate_file, \"cites\": bool(bool_)} for query_file, candidate_file, bool_ in zip(valid_rows['query'], valid_rows['candidate'], valid_rows['bool'])]\n",
    "data = [dspy.Example(**x).with_inputs('query_file', 'candidate_file') for x in data]\n",
    "\n",
    "def split_data(data, split_ratio, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    split_index = int(split_ratio * len(data))\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "    trainset = [data[i] for i in train_indices]\n",
    "    testset = [data[i] for i in test_indices]\n",
    "    return trainset, testset\n",
    "\n",
    "# trainset, testset = split_data(data, 0)\n",
    "trainset = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "dspy.settings.configure(lm=llm, rm=None)\n",
    "\n",
    "client = OpenAI(\n",
    "    # this is also the default, it can be omitted\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunker:\n",
    "    def __init__(self, context_window=3000, max_windows=5):\n",
    "        self.context_window = context_window\n",
    "        self.max_windows = max_windows\n",
    "        self.window_overlap = 0.02\n",
    "\n",
    "    def __call__(self, paper):\n",
    "        snippet_idx = 0\n",
    "\n",
    "        while snippet_idx < self.max_windows and paper:\n",
    "            endpos = int(self.context_window * (1.0 + self.window_overlap))\n",
    "            snippet, paper = paper[:endpos], paper[endpos:]\n",
    "\n",
    "            next_newline_pos = snippet.rfind('\\n')\n",
    "            if paper and next_newline_pos != -1 and next_newline_pos >= self.context_window // 2:\n",
    "                paper = snippet[next_newline_pos+1:] + paper\n",
    "                snippet = snippet[:next_newline_pos]\n",
    "\n",
    "            yield snippet_idx, snippet.strip()\n",
    "            snippet_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\", save_file=None):\n",
    "    if save_file and Path(save_file).exists():\n",
    "        with open(save_file, 'r') as f:\n",
    "            # print(f\"Loading embeddings from {save_file}\")\n",
    "            embeddings = [ast.literal_eval(line.strip()) for line in f]\n",
    "        return embeddings\n",
    "        \n",
    "    try:\n",
    "        response = client.embeddings.create(input=texts, model=model)\n",
    "        embeddings = [embedding.embedding for embedding in response.data]\n",
    "        if save_file: # Save the embeddings to a file\n",
    "            with open(save_file, 'w') as f:\n",
    "                # print(f\"Saving embeddings to {save_file}\")\n",
    "                for embedding in embeddings:\n",
    "                    f.write(str(embedding) + '\\n')\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(\"Error during API call:\", e)\n",
    "        return []\n",
    "    \n",
    "def get_most_similar_chunk(query_embedding, candidate_embeddings, candidate_chunks):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return candidate_chunks[most_similar_idx]\n",
    "    \n",
    "    \n",
    "class PredictCitation(dspy.Signature):\n",
    "    __doc__ = \"\"\"Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\"\"\"   \n",
    "    query_chunk: str = dspy.InputField(desc='Query chunk to compare to the candidate chunk.')\n",
    "    candidate_chunk: str = dspy.InputField(desc='Candidate chunk to compare to the query chunk.')\n",
    "    answer: bool = dspy.OutputField(desc=\"either True or False\", prefix=\"Answer:\")\n",
    "\n",
    "\n",
    "class PredictCitationAndResolve(dspy.Module):\n",
    "    def __init__(self, context_window=3000, max_windows=5, resolve_function=any,\n",
    "                 candidate_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined', \n",
    "                 query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers',\n",
    "                 reset_embedding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chunk = Chunker(context_window=context_window, max_windows=max_windows)\n",
    "        # self.predict = dspy.TypedPredictor(PredictCitation)\n",
    "        # self.predict = dspy.TypedChainOfThought(PredictCitation)\n",
    "        self.predict = dspy.ChainOfThought(PredictCitation)\n",
    "        self.resolve_function = resolve_function\n",
    "        self.query_folder = query_folder\n",
    "        self.candidate_folder = candidate_folder\n",
    "        os.makedirs('embeddings', exist_ok=True)\n",
    "        if reset_embedding:\n",
    "            for emb_file in os.listdir('embeddings'):\n",
    "                os.remove(f'embeddings/{emb_file}')\n",
    "\n",
    "    def forward(self, query_file, candidate_file):\n",
    "        predictions = []\n",
    "        \n",
    "        # Get the text from the pdfs\n",
    "        query_pdf = PdfReader(f'{self.query_folder}/{query_file}.pdf')\n",
    "        query_text = \"\"\n",
    "        for page in query_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "        query_text = query_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        candidate_pdf = PdfReader(f'{self.candidate_folder}/{candidate_file}.pdf')\n",
    "        candidate_text = \"\"\n",
    "        for page in candidate_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                candidate_text += page_text + \" \"\n",
    "        candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # for each chunk in the paper\n",
    "        query_chunks = [snippet for _, snippet in self.chunk(query_text)]\n",
    "        candidate_chunks = [snippet for _, snippet in self.chunk(candidate_text)]\n",
    "        \n",
    "        # Create embeddings for the chunks\n",
    "        candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "        query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "        \n",
    "        for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "            # Get the candidate chunk that is most similar to the snippet\n",
    "            candidate_chunk = get_most_similar_chunk(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "            prediction = self.predict(query_chunk=snippet, candidate_chunk=candidate_chunk)\n",
    "            # print(prediction)\n",
    "            predictions.append(prediction.answer=='True')\n",
    "\n",
    "        return dspy.Prediction(predictions=predictions, resolved=self.resolve_function(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_chunking = PredictCitationAndResolve(max_windows=15, context_window=1000, reset_embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker = Chunker(context_window=1000, max_windows=15)\n",
    "# query_pdf = PdfReader(f'/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers/1323414.pdf')\n",
    "# query_text = \"\"\n",
    "# for page in query_pdf.pages:\n",
    "#     page_text = page.extract_text()\n",
    "#     if page_text:\n",
    "#         query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "# query_text = query_text.replace(\"\\n\", \" \")\n",
    "# query_chunks = [snippet for _, snippet in chunker(query_text)]\n",
    "# print(query_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(query_chunks[0]))\n",
    "# print(len(query_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get an example\n",
    "# example = trainset[-2]\n",
    "# example_x = example.inputs()\n",
    "# example_y = example.labels()\n",
    "# print(example_x)\n",
    "# print(example_y)\n",
    "\n",
    "# prediction = pipeline_chunking(**example_x)\n",
    "# print(prediction)\n",
    "# print(example_y.cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, result):\n",
    "    '''Match metric'''\n",
    "    return 1 if example.cites == result.resolved else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate = Evaluate(devset=trainset, metric=metric, num_threads=8, display_progress=True, display_table=0, max_errors=100, return_outputs=True)\n",
    "# outputs = evaluate(pipeline_chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions = []\n",
    "# for x in outputs[1]:\n",
    "#     if type(x[1])==dspy.Prediction:\n",
    "#         all_predictions.append(x[1].resolved)\n",
    "#     else:\n",
    "#         all_predictions.append(np.nan)\n",
    "    \n",
    "\n",
    "# all_labels = [x[0].cites for x in outputs[1]]\n",
    "# print(len(all_predictions))\n",
    "\n",
    "# with open('darwin/eval/predictions_COT_large_prompt_1000.txt', 'w') as f:\n",
    "#     for pred in all_predictions:\n",
    "#         f.write(str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the accuracy of the final predictions\n",
    "# correct_predictions = [prediction == label for prediction, label in zip(all_predictions, all_labels)]\n",
    "# accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# # Compute the recall of the final predictions\n",
    "# true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# false_negatives = sum([not prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# recall = true_positives / (true_positives + false_negatives)\n",
    "# print(f'Recall: {recall: .2f}')\n",
    "\n",
    "# # Compute the precision of the final predictions\n",
    "# true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# false_positives = sum([prediction and not label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# precision = true_positives / (true_positives + false_positives)\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "\n",
    "# # F1 score\n",
    "# f1 = 2 * (precision * recall) / (precision + recall)\n",
    "# print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_predictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PdfReader('darwin/query_papers/53079158.pdf').pages[-1].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the dataset where dspy will retrieve from\n",
    "#### Each sample has the following format. \"Query Chunk: ...\\n Candidate Chunk: ...\\n Answer: ...\\n  \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_papers = []\n",
    "r_papers = []\n",
    "with open('/Users/jamie/link-recorder-final-1', 'r') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        tpaper = temp[0].strip()\n",
    "        rpaper = temp[1].strip()\n",
    "        test_papers.append(tpaper)\n",
    "        r_papers.append(rpaper)\n",
    "test_retrieved_data = pd.DataFrame({'tpaper': test_papers, 'rpaper': r_papers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tpaper     rpaper\n",
      "0  16897790     252854\n",
      "1   2538574  253145517\n",
      "2  11633392  113541825\n",
      "3   4655781    2011582\n",
      "4   6833818  189960050\n",
      "Number of query candidate pairs with valid files: 738\n"
     ]
    }
   ],
   "source": [
    "valid_rows_retrieved = pd.DataFrame()\n",
    "retrieved_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/r-paper-final'\n",
    "for _, row in test_retrieved_data.iterrows():\n",
    "    test_file_1 = os.path.join(query_dir, str(row['tpaper']) + '.pdf')\n",
    "    test_file_2 = os.path.join(candidate_dir, str(row['tpaper']) + '.pdf')\n",
    "    r_file = os.path.join(retrieved_dir, str(row['rpaper']) + '.pdf')\n",
    "\n",
    "    # Check if both files exist\n",
    "    if (os.path.isfile(test_file_1) or os.path.isfile(test_file_2)) and os.path.isfile(r_file):\n",
    "        # If both files exist, append the row to valid_rows\n",
    "        valid_rows_retrieved = valid_rows_retrieved._append(row)\n",
    "valid_rows_retrieved.reset_index(drop=True, inplace=True)\n",
    "print(valid_rows_retrieved.head())\n",
    "print(f'Number of query candidate pairs with valid files: {len(valid_rows_retrieved)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     query  candidate label\n",
      "0  6869636  235125640     1\n",
      "1  6869636  114250615     0\n",
      "2  6869636  111334241     0\n",
      "3  6869636  189960050     0\n",
      "4  6869636    2439435     0\n",
      "Number of query candidate pairs in dspy retrieval set: 900\n"
     ]
    }
   ],
   "source": [
    "# Randoly select 100 to set up the retrieval dataset\n",
    "dspy_r_set = pd.DataFrame(columns=['query', 'candidate', 'label'])\n",
    "valid_r_papers = valid_rows_retrieved['rpaper'].to_numpy()\n",
    "sampled_df = valid_rows_retrieved.sample(n=100)\n",
    "for _, row in sampled_df.iterrows():\n",
    "    test_set_paper = row['tpaper']\n",
    "    retrieved_paper = row['rpaper']\n",
    "    new_row = {'query': test_set_paper, 'candidate': retrieved_paper, 'label': 1}\n",
    "    dspy_r_set = pd.concat([dspy_r_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Get 8 negative samples for one positive sample as in the SPECTER svm experiment.\n",
    "    neg_papers = np.random.choice(valid_r_papers, size=8)\n",
    "    for neg_p in neg_papers:\n",
    "        new_row = {'query': test_set_paper, 'candidate': neg_p, 'label': 0}\n",
    "        dspy_r_set = pd.concat([dspy_r_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(dspy_r_set.head())\n",
    "print(f'Number of query candidate pairs in dspy retrieval set: {len(dspy_r_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return candidate_chunks[most_similar_idx], candidate_embeddings[most_similar_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_emb_idx(query_embedding, candidate_embeddings):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return most_similar_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce retrieval set\n",
    "query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers'\n",
    "query_folder_2 = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined'\n",
    "candidate_folder= '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/r-paper-final'\n",
    "chunk = Chunker(context_window=1000, max_windows=15)\n",
    "dspy_r_emb = []\n",
    "dspy_r_text = []\n",
    "for _, row in dspy_r_set.iterrows():\n",
    "    # Get the text from the pdfs\n",
    "    query_file_1 = os.path.join(query_folder, str(row['query']) + '.pdf')\n",
    "    query_file_2 = os.path.join(query_folder_2, str(row['query']) + '.pdf')\n",
    "    if os.path.isfile(query_file_1):\n",
    "        query_file_path = query_file_1\n",
    "    if os.path.isfile(query_file_2):\n",
    "        query_file_path = query_file_2\n",
    "    try:\n",
    "        query_pdf = PdfReader(query_file_path)\n",
    "    except:\n",
    "        cotinue\n",
    "    query_text = \"\"\n",
    "    for page in query_pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "    query_text = query_text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    candidate_file = row['candidate']\n",
    "    try:\n",
    "        candidate_pdf = PdfReader(f'{candidate_folder}/{candidate_file}.pdf')\n",
    "    except:\n",
    "        continue\n",
    "    candidate_text = \"\"\n",
    "    for page in candidate_pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            candidate_text += page_text + \" \"\n",
    "    candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # for each chunk in the paper\n",
    "    query_chunks = [snippet for _, snippet in chunk(query_text)]\n",
    "    candidate_chunks = [snippet for _, snippet in chunk(candidate_text)]\n",
    "    \n",
    "    # Create embeddings for the chunks\n",
    "    candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "    query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "    \n",
    "    for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "        # Get the candidate chunk that is most similar to the snippet\n",
    "        candidate_chunk, c_emb = get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "        dspy_r_emb.append((query_embedding, c_emb, row['label']))\n",
    "        dspy_r_text.append((snippet, candidate_chunk, row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1080)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dspy_r_emb), len(dspy_r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_r_emb_concat = []\n",
    "for q_emb, c_emb, label in dspy_r_emb:\n",
    "    concat_emb = q_emb + c_emb\n",
    "    dspy_r_emb_concat.append(concat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCitationWithRetrieval(dspy.Signature):\n",
    "    __doc__ = \"\"\"Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\"\"\"   \n",
    "    query_chunk: str = dspy.InputField(desc='Query chunk to compare to the candidate chunk.')\n",
    "    candidate_chunk: str = dspy.InputField(desc='Candidate chunk to compare to the query chunk.')\n",
    "    answer: bool = dspy.OutputField(desc=\"either True or False\", prefix=\"Answer:\")\n",
    "    context: str = dspy.InputField(desc=\"A good example to learn from.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PredictCitationRetrieveAndResolve(dspy.Module):\n",
    "    def __init__(self, context_window=3000, max_windows=5, resolve_function=any,\n",
    "                 candidate_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined', \n",
    "                 query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers',\n",
    "                 reset_embedding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chunk = Chunker(context_window=context_window, max_windows=max_windows)\n",
    "        # self.predict = dspy.TypedPredictor(PredictCitation)\n",
    "        # self.predict = dspy.TypedChainOfThought(PredictCitation)\n",
    "        self.predict = dspy.Predict(PredictCitationWithRetrieval)\n",
    "        self.resolve_function = resolve_function\n",
    "        self.query_folder = query_folder\n",
    "        self.candidate_folder = candidate_folder\n",
    "        os.makedirs('embeddings', exist_ok=True)\n",
    "        if reset_embedding:\n",
    "            for emb_file in os.listdir('embeddings'):\n",
    "                os.remove(f'embeddings/{emb_file}')\n",
    "\n",
    "    def forward(self, query_file, candidate_file):\n",
    "        predictions = []     \n",
    "        # Get the text from the pdfs\n",
    "        query_pdf = PdfReader(f'{self.query_folder}/{query_file}.pdf')\n",
    "        query_text = \"\"\n",
    "        for page in query_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "        query_text = query_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        candidate_pdf = PdfReader(f'{self.candidate_folder}/{candidate_file}.pdf')\n",
    "        candidate_text = \"\"\n",
    "        for page in candidate_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                candidate_text += page_text + \" \"\n",
    "        candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # for each chunk in the paper\n",
    "        query_chunks = [snippet for _, snippet in self.chunk(query_text)]\n",
    "        candidate_chunks = [snippet for _, snippet in self.chunk(candidate_text)]\n",
    "        \n",
    "        # Create embeddings for the chunks\n",
    "        candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "        query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "        \n",
    "        for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "            # Get the candidate chunk that is most similar to the snippet\n",
    "            candidate_chunk, candidate_chunk_emb = get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "            original_emb_concat = query_embedding + candidate_chunk_emb\n",
    "            context_idx = get_most_similar_emb_idx(original_emb_concat, dspy_r_emb_concat)\n",
    "            context_text = dspy_r_text[context_idx]\n",
    "            if context_text[2]:\n",
    "                context_answer = \"True\"\n",
    "            else:\n",
    "                context_answer = \"False\"\n",
    "            context = f\"Query Chunk: {context_text[0]}\\nCandidate Chunk: {context_text[1]}\\nAnswer: {context_answer}\\n\"\n",
    "            prediction = self.predict(query_chunk=snippet, candidate_chunk=candidate_chunk, context=context)\n",
    "            # print(prediction)\n",
    "            predictions.append(prediction.answer=='True')\n",
    "        return dspy.Prediction(context=context, predictions=predictions, resolved=self.resolve_function(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, result):\n",
    "    '''Match metric'''\n",
    "    return 1 if example.cites == result.resolved else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_chunking_retrieval = PredictCitationRetrieveAndResolve(max_windows=15, context_window=1000, reset_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                   | 0/651 [00:00<?, ?it/s]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.16s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.88s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.07s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.87s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.70s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.06s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 1 / 1  (100.0):   0%|                   | 0/651 [00:53<?, ?it/s]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.54s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.48s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.45s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.95s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 1 / 1  (100.0):   0%|        | 1/651 [01:07<11:25:04, 63.24s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.44s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.15s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.25s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.54s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 2 / 2  (100.0):   0%|        | 1/651 [01:17<11:25:04, 63.24s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.69s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.40s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 2 / 2  (100.0):   0%|         | 2/651 [01:27<6:51:39, 38.06s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.23s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.23s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 3 / 3  (100.0):   0%|         | 2/651 [01:34<6:51:39, 38.06s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.19s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 3 / 3  (100.0):   0%|         | 3/651 [01:38<4:53:29, 27.18s/it]\u001b[A\n",
      "Average Metric: 4 / 4  (100.0):   0%|         | 3/651 [01:41<4:53:29, 27.18s/it]\u001b[A\n",
      "Average Metric: 4 / 4  (100.0):   1%|         | 4/651 [01:44<3:21:07, 18.65s/it]\u001b[A\n",
      "Average Metric: 5 / 5  (100.0):   1%|         | 4/651 [01:46<3:21:07, 18.65s/it]\u001b[A\n",
      "Average Metric: 5 / 5  (100.0):   1%|         | 5/651 [01:48<2:24:58, 13.46s/it]\u001b[A\n",
      "Average Metric: 6 / 6  (100.0):   1%|         | 5/651 [01:51<2:24:58, 13.46s/it]\u001b[A\n",
      "Average Metric: 6 / 6  (100.0):   1%|         | 6/651 [01:54<1:57:10, 10.90s/it]\u001b[A\n",
      "Average Metric: 7 / 7  (100.0):   1%|         | 6/651 [01:57<1:57:10, 10.90s/it]\u001b[A\n",
      "Average Metric: 7 / 7  (100.0):   1%|         | 7/651 [02:03<1:43:28,  9.64s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.72s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.76s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 7 / 8  (87.5):   1%|          | 7/651 [02:10<1:43:28,  9.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 7 / 8  (87.5):   1%|          | 8/651 [02:17<1:53:02, 10.55s/it]\u001b[A\n",
      "Average Metric: 7 / 9  (77.8):   1%|          | 8/651 [02:24<1:53:02, 10.55s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.35s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 7 / 9  (77.8):   1%|▏         | 9/651 [02:29<2:09:10, 12.07s/it]\u001b[A\n",
      "Average Metric: 8 / 10  (80.0):   1%|         | 9/651 [02:36<2:09:10, 12.07s/it]\u001b[A\n",
      "Average Metric: 8 / 10  (80.0):   2%|        | 10/651 [02:38<1:59:49, 11.22s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.14s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 9 / 11  (81.8):   2%|        | 10/651 [02:43<1:59:49, 11.22s/it]\u001b[A\n",
      "Average Metric: 9 / 11  (81.8):   2%|▏       | 11/651 [02:48<1:54:11, 10.71s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.92s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.96s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.09s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.02s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 9 / 12  (75.0):   2%|▏       | 11/651 [02:59<1:54:11, 10.71s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.76s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.97s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.73s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 9 / 12  (75.0):   2%|▏       | 12/651 [03:05<2:12:27, 12.44s/it]\u001b[A\n",
      "Average Metric: 10 / 13  (76.9):   2%|▏      | 12/651 [03:11<2:12:27, 12.44s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 10 / 13  (76.9):   2%|▏      | 13/651 [03:18<2:13:14, 12.53s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.01s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 11 / 14  (78.6):   2%|▏      | 13/651 [03:26<2:13:14, 12.53s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.98s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 11 / 14  (78.6):   2%|▏      | 14/651 [03:30<2:14:07, 12.63s/it]\u001b[A\n",
      "Average Metric: 11 / 15  (73.3):   2%|▏      | 14/651 [03:33<2:14:07, 12.63s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.84s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 11 / 15  (73.3):   2%|▏      | 15/651 [03:36<1:51:41, 10.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 12 / 16  (75.0):   2%|▏      | 15/651 [03:41<1:51:41, 10.54s/it]\u001b[A\n",
      "Average Metric: 12 / 16  (75.0):   2%|▏      | 16/651 [03:44<1:41:35,  9.60s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.64s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 12 / 17  (70.6):   2%|▏      | 16/651 [03:55<1:41:35,  9.60s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.49s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.62s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 12 / 17  (70.6):   3%|▏      | 17/651 [04:04<2:04:06, 11.75s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.20s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 13 / 18  (72.2):   3%|▏      | 17/651 [04:13<2:04:06, 11.75s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.92s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 13 / 18  (72.2):   3%|▏      | 18/651 [04:21<2:29:07, 14.14s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.61s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 13 / 19  (68.4):   3%|▏      | 18/651 [04:27<2:29:07, 14.14s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.32s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 13 / 19  (68.4):   3%|▏      | 19/651 [04:31<2:18:18, 13.13s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.89s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 13 / 20  (65.0):   3%|▏      | 19/651 [04:40<2:18:18, 13.13s/it]\u001b[A\n",
      "Average Metric: 13 / 20  (65.0):   3%|▏      | 20/651 [04:44<2:18:11, 13.14s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 14 / 21  (66.7):   3%|▏      | 20/651 [04:52<2:18:11, 13.14s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.91s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 14 / 21  (66.7):   3%|▏      | 21/651 [04:57<2:18:16, 13.17s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.68s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.03s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.88s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 14 / 22  (63.6):   3%|▏      | 21/651 [05:06<2:18:16, 13.17s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.78s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 5.22s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.67s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 14 / 22  (63.6):   3%|▏      | 22/651 [05:15<2:28:34, 14.17s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.28s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.34s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.77s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 15 / 23  (65.2):   3%|▏      | 22/651 [05:23<2:28:34, 14.17s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 15 / 23  (65.2):   4%|▏      | 23/651 [05:27<2:22:22, 13.60s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.33s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 16 / 24  (66.7):   4%|▏      | 23/651 [05:33<2:22:22, 13.60s/it]\u001b[A\n",
      "Average Metric: 16 / 24  (66.7):   4%|▎      | 24/651 [05:37<2:13:55, 12.82s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.68s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 17 / 25  (68.0):   4%|▎      | 24/651 [05:44<2:13:55, 12.82s/it]\u001b[A\n",
      "Average Metric: 17 / 25  (68.0):   4%|▎      | 25/651 [05:50<2:11:26, 12.60s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.13s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 17 / 26  (65.4):   4%|▎      | 25/651 [05:55<2:11:26, 12.60s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.05s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 17 / 26  (65.4):   4%|▎      | 26/651 [06:00<2:06:26, 12.14s/it]\u001b[A\n",
      "Average Metric: 18 / 27  (66.7):   4%|▎      | 26/651 [06:04<2:06:26, 12.14s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.27s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 18 / 27  (66.7):   4%|▎      | 27/651 [06:07<1:49:26, 10.52s/it]\u001b[A\n",
      "Average Metric: 18 / 28  (64.3):   4%|▎      | 27/651 [06:08<1:49:26, 10.52s/it]\u001b[A\n",
      "Average Metric: 18 / 28  (64.3):   4%|▎      | 28/651 [06:13<1:36:19,  9.28s/it]\u001b[A\n",
      "Average Metric: 19 / 29  (65.5):   4%|▎      | 28/651 [06:18<1:36:19,  9.28s/it]\u001b[A\n",
      "Average Metric: 19 / 29  (65.5):   4%|▎      | 29/651 [06:25<1:40:02,  9.65s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.48s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 20 / 30  (66.7):   4%|▎      | 29/651 [06:32<1:40:02,  9.65s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.45s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 20 / 30  (66.7):   5%|▎      | 30/651 [06:39<1:52:04, 10.83s/it]\u001b[A\n",
      "Average Metric: 21 / 31  (67.7):   5%|▎      | 30/651 [06:45<1:52:04, 10.83s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.15s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 21 / 31  (67.7):   5%|▎      | 31/651 [06:49<1:51:20, 10.77s/it]\u001b[A\n",
      "Average Metric: 22 / 32  (68.8):   5%|▎      | 31/651 [06:52<1:51:20, 10.77s/it]\u001b[A\n",
      "Average Metric: 22 / 32  (68.8):   5%|▎      | 32/651 [06:56<1:40:35,  9.75s/it]\u001b[A\n",
      "Average Metric: 23 / 33  (69.7):   5%|▎      | 32/651 [06:58<1:40:35,  9.75s/it]\u001b[A\n",
      "Average Metric: 23 / 33  (69.7):   5%|▎      | 33/651 [07:03<1:31:02,  8.84s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.72s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 24 / 34  (70.6):   5%|▎      | 33/651 [07:10<1:31:02,  8.84s/it]\u001b[A\n",
      "Average Metric: 24 / 34  (70.6):   5%|▎      | 34/651 [07:12<1:35:55,  9.33s/it]\u001b[A\n",
      "Average Metric: 25 / 35  (71.4):   5%|▎      | 34/651 [07:15<1:35:55,  9.33s/it]\u001b[A\n",
      "Average Metric: 25 / 35  (71.4):   5%|▍      | 35/651 [07:18<1:26:57,  8.47s/it]\u001b[A\n",
      "Average Metric: 26 / 36  (72.2):   5%|▍      | 35/651 [07:24<1:26:57,  8.47s/it]\u001b[A\n",
      "Average Metric: 26 / 36  (72.2):   6%|▍      | 36/651 [07:28<1:29:37,  8.74s/it]\u001b[A\n",
      "Average Metric: 27 / 37  (73.0):   6%|▍      | 36/651 [07:30<1:29:37,  8.74s/it]\u001b[A\n",
      "Average Metric: 27 / 37  (73.0):   6%|▍      | 37/651 [07:32<1:18:00,  7.62s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 28 / 38  (73.7):   6%|▍      | 37/651 [07:36<1:18:00,  7.62s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.83s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 28 / 38  (73.7):   6%|▍      | 38/651 [07:42<1:16:15,  7.46s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.45s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 29 / 39  (74.4):   6%|▍      | 38/651 [07:48<1:16:15,  7.46s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.37s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 29 / 39  (74.4):   6%|▍      | 39/651 [07:59<1:50:15, 10.81s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.67s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 30 / 40  (75.0):   6%|▍      | 39/651 [08:04<1:50:15, 10.81s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.14s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.79s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 30 / 40  (75.0):   6%|▍      | 40/651 [08:16<2:06:23, 12.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31 / 41  (75.6):   6%|▍      | 40/651 [08:20<2:06:23, 12.41s/it]\u001b[A\n",
      "Average Metric: 31 / 41  (75.6):   6%|▍      | 41/651 [08:24<1:54:30, 11.26s/it]\u001b[A\n",
      "Average Metric: 32 / 42  (76.2):   6%|▍      | 41/651 [08:26<1:54:30, 11.26s/it]\u001b[A\n",
      "Average Metric: 32 / 42  (76.2):   6%|▍      | 42/651 [08:28<1:33:50,  9.25s/it]\u001b[A\n",
      "Average Metric: 33 / 43  (76.7):   6%|▍      | 42/651 [08:30<1:33:50,  9.25s/it]\u001b[A\n",
      "Average Metric: 33 / 43  (76.7):   7%|▍      | 43/651 [08:35<1:25:46,  8.47s/it]\u001b[A\n",
      "Average Metric: 34 / 44  (77.3):   7%|▍      | 43/651 [08:40<1:25:46,  8.47s/it]\u001b[A\n",
      "Average Metric: 34 / 44  (77.3):   7%|▍      | 44/651 [08:43<1:24:06,  8.31s/it]\u001b[A\n",
      "Average Metric: 35 / 45  (77.8):   7%|▍      | 44/651 [08:47<1:24:06,  8.31s/it]\u001b[A\n",
      "Average Metric: 35 / 45  (77.8):   7%|▍      | 45/651 [08:55<1:28:43,  8.78s/it]\u001b[A\n",
      "Average Metric: 36 / 46  (78.3):   7%|▍      | 45/651 [09:00<1:28:43,  8.78s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.27s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 5.35s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 36 / 46  (78.3):   7%|▍      | 46/651 [09:11<1:41:40, 10.08s/it]\u001b[A\n",
      "Average Metric: 37 / 47  (78.7):   7%|▍      | 46/651 [09:18<1:41:40, 10.08s/it]\u001b[A\n",
      "Average Metric: 37 / 47  (78.7):   7%|▌      | 47/651 [09:21<1:55:12, 11.44s/it]\u001b[A\n",
      "Average Metric: 38 / 48  (79.2):   7%|▌      | 47/651 [09:24<1:55:12, 11.44s/it]\u001b[A\n",
      "Average Metric: 38 / 48  (79.2):   7%|▌      | 48/651 [09:26<1:39:42,  9.92s/it]\u001b[A\n",
      "Average Metric: 39 / 49  (79.6):   7%|▌      | 48/651 [09:29<1:39:42,  9.92s/it]\u001b[A\n",
      "Average Metric: 39 / 49  (79.6):   8%|▌      | 49/651 [09:35<1:29:44,  8.94s/it]\u001b[A\n",
      "Average Metric: 39 / 50  (78.0):   8%|▌      | 49/651 [09:38<1:29:44,  8.94s/it]\u001b[A\n",
      "Average Metric: 39 / 50  (78.0):   8%|▌      | 50/651 [09:41<1:27:21,  8.72s/it]\u001b[A\n",
      "Average Metric: 40 / 51  (78.4):   8%|▌      | 50/651 [09:43<1:27:21,  8.72s/it]\u001b[A\n",
      "Average Metric: 40 / 51  (78.4):   8%|▌      | 51/651 [09:46<1:14:36,  7.46s/it]\u001b[A\n",
      "Average Metric: 40 / 52  (76.9):   8%|▌      | 51/651 [09:48<1:14:36,  7.46s/it]\u001b[A\n",
      "Average Metric: 40 / 52  (76.9):   8%|▌      | 52/651 [09:51<1:07:31,  6.76s/it]\u001b[A\n",
      "Average Metric: 41 / 53  (77.4):   8%|▌      | 52/651 [09:52<1:07:31,  6.76s/it]\u001b[A\n",
      "Average Metric: 41 / 53  (77.4):   8%|▌      | 53/651 [09:56<1:01:56,  6.22s/it]\u001b[A\n",
      "Average Metric: 42 / 54  (77.8):   8%|▌      | 53/651 [09:58<1:01:56,  6.22s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.32s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 42 / 54  (77.8):   8%|▌      | 54/651 [10:03<1:00:40,  6.10s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 43 / 55  (78.2):   8%|▌      | 54/651 [10:12<1:00:40,  6.10s/it]\u001b[A\n",
      "Average Metric: 43 / 55  (78.2):   8%|▌      | 55/651 [10:17<1:26:56,  8.75s/it]\u001b[A\n",
      "Average Metric: 43 / 56  (76.8):   8%|▌      | 55/651 [10:25<1:26:56,  8.75s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.93s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.16s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 43 / 56  (76.8):   9%|▌      | 56/651 [10:31<1:43:37, 10.45s/it]\u001b[A\n",
      "Average Metric: 43 / 57  (75.4):   9%|▌      | 56/651 [10:35<1:43:37, 10.45s/it]\u001b[A\n",
      "Average Metric: 43 / 57  (75.4):   9%|▌      | 57/651 [10:37<1:28:55,  8.98s/it]\u001b[A\n",
      "Average Metric: 44 / 58  (75.9):   9%|▌      | 57/651 [10:39<1:28:55,  8.98s/it]\u001b[A\n",
      "Average Metric: 44 / 58  (75.9):   9%|▌      | 58/651 [10:42<1:19:12,  8.01s/it]\u001b[A\n",
      "Average Metric: 45 / 59  (76.3):   9%|▌      | 58/651 [10:45<1:19:12,  8.01s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 45 / 59  (76.3):   9%|▋      | 59/651 [10:47<1:10:25,  7.14s/it]\u001b[A\n",
      "Average Metric: 46 / 60  (76.7):   9%|▋      | 59/651 [10:59<1:10:25,  7.14s/it]\u001b[A\n",
      "Average Metric: 46 / 60  (76.7):   9%|▋      | 60/651 [11:06<1:39:00, 10.05s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.94s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 47 / 61  (77.0):   9%|▋      | 60/651 [11:16<1:39:00, 10.05s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.86s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 47 / 61  (77.0):   9%|▋      | 61/651 [11:25<2:03:26, 12.55s/it]\u001b[A\n",
      "Average Metric: 47 / 62  (75.8):   9%|▋      | 61/651 [11:28<2:03:26, 12.55s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.46s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 47 / 62  (75.8):  10%|▋      | 62/651 [11:33<1:55:41, 11.79s/it]\u001b[A\n",
      "Average Metric: 47 / 63  (74.6):  10%|▋      | 62/651 [11:37<1:55:41, 11.79s/it]\u001b[A\n",
      "Average Metric: 47 / 63  (74.6):  10%|▋      | 63/651 [11:39<1:40:46, 10.28s/it]\u001b[A\n",
      "Average Metric: 48 / 64  (75.0):  10%|▋      | 63/651 [11:41<1:40:46, 10.28s/it]\u001b[A\n",
      "Average Metric: 48 / 64  (75.0):  10%|▋      | 64/651 [11:43<1:22:56,  8.48s/it]\u001b[A\n",
      "Average Metric: 49 / 65  (75.4):  10%|▋      | 64/651 [11:48<1:22:56,  8.48s/it]\u001b[A\n",
      "Average Metric: 49 / 65  (75.4):  10%|▋      | 65/651 [11:58<1:32:35,  9.48s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.41s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.82s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 50 / 66  (75.8):  10%|▋      | 65/651 [12:07<1:32:35,  9.48s/it]\u001b[A\n",
      "Average Metric: 50 / 66  (75.8):  10%|▋      | 66/651 [12:09<1:44:12, 10.69s/it]\u001b[A\n",
      "Average Metric: 51 / 67  (76.1):  10%|▋      | 66/651 [12:13<1:44:12, 10.69s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.60s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 51 / 67  (76.1):  10%|▋      | 67/651 [12:20<1:40:37, 10.34s/it]\u001b[A\n",
      "Average Metric: 52 / 68  (76.5):  10%|▋      | 67/651 [12:34<1:40:37, 10.34s/it]\u001b[A\n",
      "Average Metric: 52 / 68  (76.5):  10%|▋      | 68/651 [12:37<2:02:25, 12.60s/it]\u001b[A\n",
      "Average Metric: 53 / 69  (76.8):  10%|▋      | 68/651 [12:38<2:02:25, 12.60s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.22s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.49s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 53 / 69  (76.8):  11%|▋      | 69/651 [12:42<1:42:16, 10.54s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.17s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 70  (77.1):  11%|▋      | 69/651 [12:47<1:42:16, 10.54s/it]\u001b[A\n",
      "Average Metric: 54 / 70  (77.1):  11%|▊      | 70/651 [12:52<1:40:34, 10.39s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.87s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.32s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 55 / 71  (77.5):  11%|▊      | 70/651 [13:02<1:40:34, 10.39s/it]\u001b[A\n",
      "Average Metric: 55 / 71  (77.5):  11%|▊      | 71/651 [13:08<1:51:46, 11.56s/it]\u001b[A\n",
      "Average Metric: 56 / 72  (77.8):  11%|▊      | 71/651 [13:13<1:51:46, 11.56s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.42s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 56 / 72  (77.8):  11%|▊      | 72/651 [13:19<1:51:33, 11.56s/it]\u001b[A\n",
      "Average Metric: 56 / 73  (76.7):  11%|▊      | 72/651 [13:26<1:51:33, 11.56s/it]\u001b[A\n",
      "Average Metric: 56 / 73  (76.7):  11%|▊      | 73/651 [13:31<1:50:36, 11.48s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.38s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 56 / 74  (75.7):  11%|▊      | 73/651 [13:35<1:50:36, 11.48s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.17s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 56 / 74  (75.7):  11%|▊      | 74/651 [13:38<1:42:15, 10.63s/it]\u001b[A\n",
      "Average Metric: 57 / 75  (76.0):  11%|▊      | 74/651 [13:43<1:42:15, 10.63s/it]\u001b[A\n",
      "Average Metric: 57 / 75  (76.0):  12%|▊      | 75/651 [13:51<1:45:39, 11.01s/it]\u001b[A\n",
      "Average Metric: 58 / 76  (76.3):  12%|▊      | 75/651 [13:57<1:45:39, 11.01s/it]\u001b[A\n",
      "Average Metric: 58 / 76  (76.3):  12%|▊      | 76/651 [14:05<1:47:25, 11.21s/it]\u001b[A\n",
      "Average Metric: 59 / 77  (76.6):  12%|▊      | 76/651 [14:10<1:47:25, 11.21s/it]\u001b[A\n",
      "Average Metric: 59 / 77  (76.6):  12%|▊      | 77/651 [14:15<1:52:45, 11.79s/it]\u001b[A\n",
      "Average Metric: 60 / 78  (76.9):  12%|▊      | 77/651 [14:18<1:52:45, 11.79s/it]\u001b[A\n",
      "Average Metric: 60 / 78  (76.9):  12%|▊      | 78/651 [14:25<1:47:04, 11.21s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 61 / 79  (77.2):  12%|▊      | 78/651 [14:30<1:47:04, 11.21s/it]\u001b[A\n",
      "Average Metric: 61 / 79  (77.2):  12%|▊      | 79/651 [14:33<1:39:26, 10.43s/it]\u001b[A\n",
      "Average Metric: 62 / 80  (77.5):  12%|▊      | 79/651 [14:37<1:39:26, 10.43s/it]\u001b[A\n",
      "Average Metric: 62 / 80  (77.5):  12%|▊      | 80/651 [14:42<1:32:27,  9.72s/it]\u001b[A\n",
      "Average Metric: 63 / 81  (77.8):  12%|▊      | 80/651 [14:48<1:32:27,  9.72s/it]\u001b[A\n",
      "Average Metric: 63 / 81  (77.8):  12%|▊      | 81/651 [14:51<1:32:34,  9.75s/it]\u001b[A\n",
      "Average Metric: 64 / 82  (78.0):  12%|▊      | 81/651 [14:53<1:32:34,  9.75s/it]\u001b[A\n",
      "Average Metric: 64 / 82  (78.0):  13%|▉      | 82/651 [14:57<1:20:07,  8.45s/it]\u001b[A\n",
      "Average Metric: 65 / 83  (78.3):  13%|▉      | 82/651 [15:00<1:20:07,  8.45s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.80s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.67s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 65 / 83  (78.3):  13%|▉      | 83/651 [15:12<1:34:25,  9.97s/it]\u001b[A\n",
      "Average Metric: 66 / 84  (78.6):  13%|▉      | 83/651 [15:21<1:34:25,  9.97s/it]\u001b[A\n",
      "Average Metric: 66 / 84  (78.6):  13%|▉      | 84/651 [15:25<1:46:26, 11.26s/it]\u001b[A\n",
      "Average Metric: 66 / 85  (77.6):  13%|▉      | 84/651 [15:30<1:46:26, 11.26s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 66 / 85  (77.6):  13%|▉      | 85/651 [15:35<1:41:17, 10.74s/it]\u001b[A\n",
      "Average Metric: 67 / 86  (77.9):  13%|▉      | 85/651 [15:40<1:41:17, 10.74s/it]\u001b[A\n",
      "Average Metric: 67 / 86  (77.9):  13%|▉      | 86/651 [15:43<1:38:09, 10.42s/it]\u001b[A\n",
      "Average Metric: 68 / 87  (78.2):  13%|▉      | 86/651 [15:47<1:38:09, 10.42s/it]\u001b[A\n",
      "Average Metric: 68 / 87  (78.2):  13%|▉      | 87/651 [15:52<1:26:41,  9.22s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.62s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.61s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.01s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t negative seek value -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 69 / 88  (78.4):  13%|▉      | 87/651 [16:07<1:26:41,  9.22s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.88s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 69 / 88  (78.4):  14%|▉      | 88/651 [16:15<2:03:30, 13.16s/it]\u001b[A\n",
      "Average Metric: 69.0 / 89  (77.5):  14%|▋    | 88/651 [16:19<2:03:30, 13.16s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.95s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.99s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.60s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 69.0 / 89  (77.5):  14%|▋    | 89/651 [16:24<1:53:58, 12.17s/it]\u001b[A\n",
      "Average Metric: 70.0 / 90  (77.8):  14%|▋    | 89/651 [16:27<1:53:58, 12.17s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.04s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 70.0 / 90  (77.8):  14%|▋    | 90/651 [16:32<1:45:40, 11.30s/it]\u001b[A\n",
      "Average Metric: 70.0 / 91  (76.9):  14%|▋    | 90/651 [16:36<1:45:40, 11.30s/it]\u001b[A\n",
      "Average Metric: 70.0 / 91  (76.9):  14%|▋    | 91/651 [16:38<1:34:08, 10.09s/it]\u001b[A\n",
      "Average Metric: 71.0 / 92  (77.2):  14%|▋    | 91/651 [16:41<1:34:08, 10.09s/it]\u001b[A\n",
      "Average Metric: 71.0 / 92  (77.2):  14%|▋    | 92/651 [16:45<1:23:04,  8.92s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.53s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 72.0 / 93  (77.4):  14%|▋    | 92/651 [16:52<1:23:04,  8.92s/it]\u001b[A\n",
      "Average Metric: 72.0 / 93  (77.4):  14%|▋    | 93/651 [16:56<1:26:20,  9.28s/it]\u001b[A\n",
      "Average Metric: 72.0 / 94  (76.6):  14%|▋    | 93/651 [17:01<1:26:20,  9.28s/it]\u001b[A\n",
      "Average Metric: 72.0 / 94  (76.6):  14%|▋    | 94/651 [17:04<1:25:23,  9.20s/it]\u001b[A\n",
      "Average Metric: 72.0 / 95  (75.8):  14%|▋    | 94/651 [17:08<1:25:23,  9.20s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.80s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 72.0 / 95  (75.8):  15%|▋    | 95/651 [17:13<1:23:07,  8.97s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.69s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 73.0 / 96  (76.0):  15%|▋    | 95/651 [17:22<1:23:07,  8.97s/it]\u001b[A\n",
      "Average Metric: 73.0 / 96  (76.0):  15%|▋    | 96/651 [17:25<1:30:08,  9.75s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.43s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 6.48s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 73.0 / 97  (75.3):  15%|▋    | 96/651 [17:59<1:30:08,  9.75s/it]\u001b[A\n",
      "Average Metric: 73.0 / 97  (75.3):  15%|▋    | 97/651 [18:06<2:57:14, 19.20s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.05s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.07s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 73.0 / 98  (74.5):  15%|▋    | 97/651 [18:22<2:57:14, 19.20s/it]\u001b[A\n",
      "Average Metric: 73.0 / 98  (74.5):  15%|▊    | 98/651 [18:24<2:57:31, 19.26s/it]\u001b[A\n",
      "Average Metric: 74.0 / 99  (74.7):  15%|▊    | 98/651 [18:30<2:57:31, 19.26s/it]\u001b[A\n",
      "Average Metric: 74.0 / 99  (74.7):  15%|▊    | 99/651 [18:33<2:23:08, 15.56s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.90s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.53s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 75.0 / 100  (75.0):  15%|▌   | 99/651 [18:39<2:23:08, 15.56s/it]\u001b[A\n",
      "Average Metric: 75.0 / 100  (75.0):  15%|▍  | 100/651 [18:44<2:12:34, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during API call: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Average Metric: 76.0 / 101  (75.2):  15%|▍  | 100/651 [18:48<2:12:34, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t shapes (0,) and (1536,) not aligned: 0 (dim 0) != 1536 (dim 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Average Metric: 76.0 / 101  (75.2):  16%|▍  | 101/651 [18:51<1:53:17, 12.36s/it]\u001b[A\n",
      "Average Metric: 77.0 / 102  (75.5):  16%|▍  | 101/651 [18:54<1:53:17, 12.36s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.65s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 77.0 / 102  (75.5):  16%|▍  | 102/651 [18:58<1:37:20, 10.64s/it]\u001b[A\n",
      "Average Metric: 78.0 / 103  (75.7):  16%|▍  | 102/651 [19:00<1:37:20, 10.64s/it]\u001b[A\n",
      "Average Metric: 78.0 / 103  (75.7):  16%|▍  | 103/651 [19:05<1:27:55,  9.63s/it]\u001b[A\n",
      "Average Metric: 79.0 / 104  (76.0):  16%|▍  | 103/651 [19:10<1:27:55,  9.63s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.12s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 79.0 / 104  (76.0):  16%|▍  | 104/651 [19:16<1:27:23,  9.59s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.07s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 79.0 / 105  (75.2):  16%|▍  | 104/651 [19:22<1:27:23,  9.59s/it]\u001b[A\n",
      "Average Metric: 79.0 / 105  (75.2):  16%|▍  | 105/651 [19:26<1:32:04, 10.12s/it]\u001b[A\n",
      "Average Metric: 80.0 / 106  (75.5):  16%|▍  | 105/651 [19:31<1:32:04, 10.12s/it]\u001b[A\n",
      "Average Metric: 80.0 / 106  (75.5):  16%|▍  | 106/651 [19:35<1:30:22,  9.95s/it]\u001b[A\n",
      "Average Metric: 81.0 / 107  (75.7):  16%|▍  | 106/651 [19:39<1:30:22,  9.95s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 5.01s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 81.0 / 107  (75.7):  16%|▍  | 107/651 [19:49<1:26:00,  9.49s/it]\u001b[A\n",
      "Average Metric: 82.0 / 108  (75.9):  16%|▍  | 107/651 [19:56<1:26:00,  9.49s/it]\u001b[A\n",
      "Average Metric: 82.0 / 108  (75.9):  17%|▍  | 108/651 [20:01<1:45:38, 11.67s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.30s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.86s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 83.0 / 109  (76.1):  17%|▍  | 108/651 [20:06<1:45:38, 11.67s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.86s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.03s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 83.0 / 109  (76.1):  17%|▌  | 109/651 [20:17<1:52:44, 12.48s/it]\u001b[A\n",
      "Average Metric: 84.0 / 110  (76.4):  17%|▌  | 109/651 [20:22<1:52:44, 12.48s/it]\u001b[A\n",
      "Average Metric: 84.0 / 110  (76.4):  17%|▌  | 110/651 [20:28<1:51:15, 12.34s/it]\u001b[A\n",
      "Average Metric: 84.0 / 111  (75.7):  17%|▌  | 110/651 [20:30<1:51:15, 12.34s/it]\u001b[A\n",
      "Average Metric: 84.0 / 111  (75.7):  17%|▌  | 111/651 [20:32<1:31:28, 10.16s/it]\u001b[A\n",
      "Average Metric: 85.0 / 112  (75.9):  17%|▌  | 111/651 [20:35<1:31:28, 10.16s/it]\u001b[A\n",
      "Average Metric: 85.0 / 112  (75.9):  17%|▌  | 112/651 [20:42<1:24:14,  9.38s/it]\u001b[A\n",
      "Average Metric: 86.0 / 113  (76.1):  17%|▌  | 112/651 [20:47<1:24:14,  9.38s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.11s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 86.0 / 113  (76.1):  17%|▌  | 113/651 [20:53<1:27:34,  9.77s/it]\u001b[A\n",
      "Average Metric: 87.0 / 114  (76.3):  17%|▌  | 113/651 [20:59<1:27:34,  9.77s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.93s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 87.0 / 114  (76.3):  18%|▌  | 114/651 [21:07<1:41:49, 11.38s/it]\u001b[A\n",
      "Average Metric: 87.0 / 115  (75.7):  18%|▌  | 114/651 [21:13<1:41:49, 11.38s/it]\u001b[A\n",
      "Average Metric: 87.0 / 115  (75.7):  18%|▌  | 115/651 [21:17<1:42:31, 11.48s/it]\u001b[A\n",
      "Average Metric: 88.0 / 116  (75.9):  18%|▌  | 115/651 [21:19<1:42:31, 11.48s/it]\u001b[A\n",
      "Average Metric: 88.0 / 116  (75.9):  18%|▌  | 116/651 [21:21<1:24:01,  9.42s/it]\u001b[A\n",
      "Average Metric: 89.0 / 117  (76.1):  18%|▌  | 116/651 [21:53<1:24:01,  9.42s/it]\u001b[A\n",
      "Average Metric: 89.0 / 117  (76.1):  18%|▌  | 117/651 [21:58<2:29:58, 16.85s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.66s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 90.0 / 118  (76.3):  18%|▌  | 117/651 [22:01<2:29:58, 16.85s/it]\u001b[A\n",
      "Average Metric: 90.0 / 118  (76.3):  18%|▌  | 118/651 [22:04<2:08:30, 14.47s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.21s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 91.0 / 119  (76.5):  18%|▌  | 118/651 [22:46<2:08:30, 14.47s/it]\u001b[A\n",
      "Average Metric: 91.0 / 119  (76.5):  18%|▌  | 119/651 [22:54<3:38:47, 24.68s/it]\u001b[A\n",
      "Average Metric: 92.0 / 120  (76.7):  18%|▌  | 119/651 [23:01<3:38:47, 24.68s/it]\u001b[A\n",
      "Average Metric: 92.0 / 120  (76.7):  18%|▌  | 120/651 [23:04<3:01:08, 20.47s/it]\u001b[A\n",
      "Average Metric: 92.0 / 121  (76.0):  18%|▌  | 120/651 [23:09<3:01:08, 20.47s/it]\u001b[A\n",
      "Average Metric: 92.0 / 121  (76.0):  19%|▌  | 121/651 [23:11<2:27:02, 16.65s/it]\u001b[A\n",
      "Average Metric: 93.0 / 122  (76.2):  19%|▌  | 121/651 [23:36<2:27:02, 16.65s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.60s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 93.0 / 122  (76.2):  19%|▌  | 122/651 [23:46<3:04:34, 20.93s/it]\u001b[A\n",
      "Average Metric: 94.0 / 123  (76.4):  19%|▌  | 122/651 [23:50<3:04:34, 20.93s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.95s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.48s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 94.0 / 123  (76.4):  19%|▌  | 123/651 [23:56<2:42:23, 18.45s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.89s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Average Metric: 95.0 / 124  (76.6):  19%|▌  | 123/651 [24:02<2:42:23, 18.45s/it]\u001b[A\n",
      "Average Metric: 95.0 / 124  (76.6):  19%|▌  | 124/651 [24:11<2:28:32, 16.91s/it]\u001b[A\n",
      "Average Metric: 96.0 / 125  (76.8):  19%|▌  | 124/651 [24:17<2:28:32, 16.91s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.77s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 96.0 / 125  (76.8):  19%|▌  | 125/651 [24:28<2:30:29, 17.17s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.04s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.98s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 97.0 / 126  (77.0):  19%|▌  | 125/651 [24:37<2:30:29, 17.17s/it]\u001b[A\n",
      "Average Metric: 97.0 / 126  (77.0):  19%|▌  | 126/651 [24:42<2:28:07, 16.93s/it]\u001b[A\n",
      "Average Metric: 98.0 / 127  (77.2):  19%|▌  | 126/651 [24:45<2:28:07, 16.93s/it]\u001b[A\n",
      "Average Metric: 98.0 / 127  (77.2):  20%|▌  | 127/651 [24:49<2:01:37, 13.93s/it]\u001b[A\n",
      "Average Metric: 99.0 / 128  (77.3):  20%|▌  | 127/651 [24:53<2:01:37, 13.93s/it]\u001b[A\n",
      "Average Metric: 99.0 / 128  (77.3):  20%|▌  | 128/651 [25:00<1:49:48, 12.60s/it]\u001b[A\n",
      "Average Metric: 100.0 / 129  (77.5):  20%|▍ | 128/651 [25:06<1:49:48, 12.60s/it]\u001b[A\n",
      "Average Metric: 100.0 / 129  (77.5):  20%|▍ | 129/651 [25:11<1:47:17, 12.33s/it]\u001b[A\n",
      "Average Metric: 101.0 / 130  (77.7):  20%|▍ | 129/651 [25:15<1:47:17, 12.33s/it]\u001b[A\n",
      "Average Metric: 101.0 / 130  (77.7):  20%|▍ | 130/651 [25:19<1:35:29, 11.00s/it]\u001b[A\n",
      "Average Metric: 102.0 / 131  (77.9):  20%|▍ | 130/651 [25:24<1:35:29, 11.00s/it]\u001b[A\n",
      "Average Metric: 102.0 / 131  (77.9):  20%|▍ | 131/651 [25:30<1:32:16, 10.65s/it]\u001b[A\n",
      "Average Metric: 103.0 / 132  (78.0):  20%|▍ | 131/651 [25:35<1:32:16, 10.65s/it]\u001b[A\n",
      "Average Metric: 103.0 / 132  (78.0):  20%|▍ | 132/651 [25:39<1:28:25, 10.22s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.52s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 104.0 / 133  (78.2):  20%|▍ | 132/651 [25:49<1:28:25, 10.22s/it]\u001b[A\n",
      "Average Metric: 104.0 / 133  (78.2):  20%|▍ | 133/651 [25:55<1:42:41, 11.89s/it]\u001b[A\n",
      "Average Metric: 105.0 / 134  (78.4):  20%|▍ | 133/651 [26:03<1:42:41, 11.89s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.38s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 105.0 / 134  (78.4):  21%|▍ | 134/651 [26:14<1:50:56, 12.88s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.31s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 106.0 / 135  (78.5):  21%|▍ | 134/651 [26:20<1:50:56, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.64s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 106.0 / 135  (78.5):  21%|▍ | 135/651 [26:31<2:10:57, 15.23s/it]\u001b[A\n",
      "Average Metric: 106.0 / 136  (77.9):  21%|▍ | 135/651 [26:36<2:10:57, 15.23s/it]\u001b[A\n",
      "Average Metric: 106.0 / 136  (77.9):  21%|▍ | 136/651 [26:41<1:59:46, 13.95s/it]\u001b[A\n",
      "Average Metric: 107.0 / 137  (78.1):  21%|▍ | 136/651 [26:46<1:59:46, 13.95s/it]\u001b[A\n",
      "Average Metric: 107.0 / 137  (78.1):  21%|▍ | 137/651 [26:49<1:43:24, 12.07s/it]\u001b[A\n",
      "Average Metric: 108.0 / 138  (78.3):  21%|▍ | 137/651 [26:53<1:43:24, 12.07s/it]\u001b[A\n",
      "Average Metric: 108.0 / 138  (78.3):  21%|▍ | 138/651 [26:55<1:30:12, 10.55s/it]\u001b[A\n",
      "Average Metric: 109.0 / 139  (78.4):  21%|▍ | 138/651 [26:58<1:30:12, 10.55s/it]\u001b[A\n",
      "Average Metric: 109.0 / 139  (78.4):  21%|▍ | 139/651 [27:00<1:16:26,  8.96s/it]\u001b[A\n",
      "Average Metric: 109.0 / 140  (77.9):  21%|▍ | 139/651 [27:28<1:16:26,  8.96s/it]\u001b[A\n",
      "Average Metric: 109.0 / 140  (77.9):  22%|▍ | 140/651 [27:33<2:17:22, 16.13s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.82s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 110.0 / 141  (78.0):  22%|▍ | 140/651 [27:45<2:17:22, 16.13s/it]\u001b[A\n",
      "Average Metric: 110.0 / 141  (78.0):  22%|▍ | 141/651 [27:51<2:21:24, 16.64s/it]\u001b[A\n",
      "Average Metric: 111.0 / 142  (78.2):  22%|▍ | 141/651 [27:58<2:21:24, 16.64s/it]\u001b[A\n",
      "Average Metric: 111.0 / 142  (78.2):  22%|▍ | 142/651 [28:04<2:08:41, 15.17s/it]\u001b[A\n",
      "Average Metric: 112.0 / 143  (78.3):  22%|▍ | 142/651 [28:09<2:08:41, 15.17s/it]\u001b[A\n",
      "Average Metric: 112.0 / 143  (78.3):  22%|▍ | 143/651 [28:14<1:59:19, 14.09s/it]\u001b[A\n",
      "Average Metric: 113.0 / 144  (78.5):  22%|▍ | 143/651 [28:18<1:59:19, 14.09s/it]\u001b[A\n",
      "Average Metric: 113.0 / 144  (78.5):  22%|▍ | 144/651 [28:21<1:40:40, 11.91s/it]\u001b[A\n",
      "Average Metric: 114.0 / 145  (78.6):  22%|▍ | 144/651 [28:28<1:40:40, 11.91s/it]\u001b[A\n",
      "Average Metric: 114.0 / 145  (78.6):  22%|▍ | 145/651 [28:31<1:34:48, 11.24s/it]\u001b[A\n",
      "Average Metric: 115.0 / 146  (78.8):  22%|▍ | 145/651 [28:37<1:34:48, 11.24s/it]\u001b[A\n",
      "Average Metric: 115.0 / 146  (78.8):  22%|▍ | 146/651 [28:41<1:33:51, 11.15s/it]\u001b[A\n",
      "Average Metric: 116.0 / 147  (78.9):  22%|▍ | 146/651 [28:45<1:33:51, 11.15s/it]\u001b[A\n",
      "Average Metric: 116.0 / 147  (78.9):  23%|▍ | 147/651 [28:48<1:22:05,  9.77s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.35s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.43s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 117.0 / 148  (79.1):  23%|▍ | 147/651 [29:00<1:22:05,  9.77s/it]\u001b[A\n",
      "Average Metric: 117.0 / 148  (79.1):  23%|▍ | 148/651 [29:04<1:36:38, 11.53s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.47s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.83s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 118.0 / 149  (79.2):  23%|▍ | 148/651 [29:19<1:36:38, 11.53s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 118.0 / 149  (79.2):  23%|▍ | 149/651 [29:25<1:54:47, 13.72s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.13s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.97s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.20s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 119.0 / 150  (79.3):  23%|▍ | 149/651 [29:37<1:54:47, 13.72s/it]\u001b[A\n",
      "Average Metric: 119.0 / 150  (79.3):  23%|▍ | 150/651 [29:47<2:11:27, 15.74s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.97s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 120.0 / 151  (79.5):  23%|▍ | 150/651 [29:54<2:11:27, 15.74s/it]\u001b[A\n",
      "Average Metric: 120.0 / 151  (79.5):  23%|▍ | 151/651 [29:58<2:07:16, 15.27s/it]\u001b[A\n",
      "Average Metric: 121.0 / 152  (79.6):  23%|▍ | 151/651 [30:03<2:07:16, 15.27s/it]\u001b[A\n",
      "Average Metric: 121.0 / 152  (79.6):  23%|▍ | 152/651 [30:05<1:47:54, 12.97s/it]\u001b[A\n",
      "Average Metric: 121.0 / 153  (79.1):  23%|▍ | 152/651 [30:07<1:47:54, 12.97s/it]\u001b[A\n",
      "Average Metric: 121.0 / 153  (79.1):  24%|▍ | 153/651 [30:11<1:30:14, 10.87s/it]\u001b[A\n",
      "Average Metric: 122.0 / 154  (79.2):  24%|▍ | 153/651 [30:18<1:30:14, 10.87s/it]\u001b[A\n",
      "Average Metric: 122.0 / 154  (79.2):  24%|▍ | 154/651 [30:23<1:30:04, 10.87s/it]\u001b[A\n",
      "Average Metric: 123.0 / 155  (79.4):  24%|▍ | 154/651 [30:28<1:30:04, 10.87s/it]\u001b[A\n",
      "Average Metric: 123.0 / 155  (79.4):  24%|▍ | 155/651 [30:31<1:28:23, 10.69s/it]\u001b[A\n",
      "Average Metric: 124.0 / 156  (79.5):  24%|▍ | 155/651 [30:39<1:28:23, 10.69s/it]\u001b[A\n",
      "Average Metric: 124.0 / 156  (79.5):  24%|▍ | 156/651 [30:47<1:34:32, 11.46s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.36s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 125.0 / 157  (79.6):  24%|▍ | 156/651 [31:02<1:34:32, 11.46s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.94s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 125.0 / 157  (79.6):  24%|▍ | 157/651 [31:09<2:00:24, 14.62s/it]\u001b[A\n",
      "Average Metric: 126.0 / 158  (79.7):  24%|▍ | 157/651 [31:14<2:00:24, 14.62s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.61s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 126.0 / 158  (79.7):  24%|▍ | 158/651 [31:21<1:52:11, 13.65s/it]\u001b[A\n",
      "Average Metric: 127.0 / 159  (79.9):  24%|▍ | 158/651 [31:25<1:52:11, 13.65s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.08s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 127.0 / 159  (79.9):  24%|▍ | 159/651 [31:27<1:40:23, 12.24s/it]\u001b[A\n",
      "Average Metric: 128.0 / 160  (80.0):  24%|▍ | 159/651 [31:29<1:40:23, 12.24s/it]\u001b[A\n",
      "Average Metric: 128.0 / 160  (80.0):  25%|▍ | 160/651 [31:33<1:23:38, 10.22s/it]\u001b[A\n",
      "Average Metric: 129.0 / 161  (80.1):  25%|▍ | 160/651 [31:37<1:23:38, 10.22s/it]\u001b[A\n",
      "Average Metric: 129.0 / 161  (80.1):  25%|▍ | 161/651 [31:41<1:15:33,  9.25s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.62s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 130.0 / 162  (80.2):  25%|▍ | 161/651 [31:52<1:15:33,  9.25s/it]\u001b[A\n",
      "Average Metric: 130.0 / 162  (80.2):  25%|▍ | 162/651 [31:59<1:33:51, 11.52s/it]\u001b[A\n",
      "Average Metric: 131.0 / 163  (80.4):  25%|▍ | 162/651 [32:05<1:33:51, 11.52s/it]\u001b[A\n",
      "Average Metric: 131.0 / 163  (80.4):  25%|▌ | 163/651 [32:16<1:44:19, 12.83s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.12s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 132.0 / 164  (80.5):  25%|▌ | 163/651 [32:22<1:44:19, 12.83s/it]\u001b[A\n",
      "Average Metric: 132.0 / 164  (80.5):  25%|▌ | 164/651 [32:25<1:41:54, 12.56s/it]\u001b[A\n",
      "Average Metric: 132.0 / 165  (80.0):  25%|▌ | 164/651 [32:28<1:41:54, 12.56s/it]\u001b[A\n",
      "Average Metric: 132.0 / 165  (80.0):  25%|▌ | 165/651 [32:33<1:30:43, 11.20s/it]\u001b[A\n",
      "Average Metric: 133.0 / 166  (80.1):  25%|▌ | 165/651 [32:38<1:30:43, 11.20s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 133.0 / 166  (80.1):  25%|▌ | 166/651 [32:48<1:29:54, 11.12s/it]\u001b[A\n",
      "Average Metric: 134.0 / 167  (80.2):  25%|▌ | 166/651 [32:56<1:29:54, 11.12s/it]\u001b[A\n",
      "Average Metric: 134.0 / 167  (80.2):  26%|▌ | 167/651 [33:03<1:49:26, 13.57s/it]\u001b[A\n",
      "Average Metric: 135.0 / 168  (80.4):  26%|▌ | 167/651 [33:08<1:49:26, 13.57s/it]\u001b[A\n",
      "Average Metric: 135.0 / 168  (80.4):  26%|▌ | 168/651 [33:14<1:37:43, 12.14s/it]\u001b[A\n",
      "Average Metric: 135.0 / 169  (79.9):  26%|▌ | 168/651 [33:23<1:37:43, 12.14s/it]\u001b[A\n",
      "Average Metric: 135.0 / 169  (79.9):  26%|▌ | 169/651 [33:31<1:47:13, 13.35s/it]\u001b[A\n",
      "Average Metric: 136.0 / 170  (80.0):  26%|▌ | 169/651 [33:38<1:47:13, 13.35s/it]\u001b[A\n",
      "Average Metric: 136.0 / 170  (80.0):  26%|▌ | 170/651 [33:42<1:48:39, 13.55s/it]\u001b[A\n",
      "Average Metric: 137.0 / 171  (80.1):  26%|▌ | 170/651 [33:45<1:48:39, 13.55s/it]\u001b[A\n",
      "Average Metric: 137.0 / 171  (80.1):  26%|▌ | 171/651 [33:48<1:28:45, 11.10s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.10s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 138.0 / 172  (80.2):  26%|▌ | 171/651 [33:52<1:28:45, 11.10s/it]\u001b[A\n",
      "Average Metric: 138.0 / 172  (80.2):  26%|▌ | 172/651 [33:56<1:21:31, 10.21s/it]\u001b[A\n",
      "Average Metric: 138.0 / 173  (79.8):  26%|▌ | 172/651 [34:08<1:21:31, 10.21s/it]\u001b[A\n",
      "Average Metric: 138.0 / 173  (79.8):  27%|▌ | 173/651 [34:13<1:32:53, 11.66s/it]\u001b[A\n",
      "Average Metric: 139.0 / 174  (79.9):  27%|▌ | 173/651 [34:18<1:32:53, 11.66s/it]\u001b[A\n",
      "Average Metric: 139.0 / 174  (79.9):  27%|▌ | 174/651 [34:22<1:29:26, 11.25s/it]\u001b[A\n",
      "Average Metric: 139.0 / 175  (79.4):  27%|▌ | 174/651 [34:24<1:29:26, 11.25s/it]\u001b[A\n",
      "Average Metric: 139.0 / 175  (79.4):  27%|▌ | 175/651 [34:31<1:23:11, 10.49s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.38s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 140.0 / 176  (79.5):  27%|▌ | 175/651 [34:46<1:23:11, 10.49s/it]\u001b[A\n",
      "Average Metric: 140.0 / 176  (79.5):  27%|▌ | 176/651 [34:51<1:46:54, 13.51s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.21s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 141.0 / 177  (79.7):  27%|▌ | 176/651 [35:01<1:46:54, 13.51s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.40s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 141.0 / 177  (79.7):  27%|▌ | 177/651 [35:06<1:49:29, 13.86s/it]\u001b[A\n",
      "Average Metric: 141.0 / 178  (79.2):  27%|▌ | 177/651 [35:12<1:49:29, 13.86s/it]\u001b[A\n",
      "Average Metric: 141.0 / 178  (79.2):  27%|▌ | 178/651 [35:16<1:41:59, 12.94s/it]\u001b[A\n",
      "Average Metric: 142.0 / 179  (79.3):  27%|▌ | 178/651 [35:20<1:41:59, 12.94s/it]\u001b[A\n",
      "Average Metric: 142.0 / 179  (79.3):  27%|▌ | 179/651 [35:26<1:34:33, 12.02s/it]\u001b[A\n",
      "Average Metric: 143.0 / 180  (79.4):  27%|▌ | 179/651 [35:30<1:34:33, 12.02s/it]\u001b[A\n",
      "Average Metric: 143.0 / 180  (79.4):  28%|▌ | 180/651 [35:37<1:29:49, 11.44s/it]\u001b[A\n",
      "Average Metric: 144.0 / 181  (79.6):  28%|▌ | 180/651 [35:44<1:29:49, 11.44s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.00s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 144.0 / 181  (79.6):  28%|▌ | 181/651 [35:51<1:36:17, 12.29s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.44s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 145.0 / 182  (79.7):  28%|▌ | 181/651 [36:00<1:36:17, 12.29s/it]\u001b[A\n",
      "Average Metric: 145.0 / 182  (79.7):  28%|▌ | 182/651 [36:03<1:37:53, 12.52s/it]\u001b[A\n",
      "Average Metric: 145.0 / 183  (79.2):  28%|▌ | 182/651 [36:08<1:37:53, 12.52s/it]\u001b[A\n",
      "Average Metric: 145.0 / 183  (79.2):  28%|▌ | 183/651 [36:14<1:32:18, 11.83s/it]\u001b[A\n",
      "Average Metric: 145.0 / 184  (78.8):  28%|▌ | 183/651 [36:17<1:32:18, 11.83s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.64s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 145.0 / 184  (78.8):  28%|▌ | 184/651 [36:21<1:19:57, 10.27s/it]\u001b[A\n",
      "Average Metric: 146.0 / 185  (78.9):  28%|▌ | 184/651 [36:29<1:19:57, 10.27s/it]\u001b[A\n",
      "Average Metric: 146.0 / 185  (78.9):  28%|▌ | 185/651 [36:34<1:26:24, 11.12s/it]\u001b[A\n",
      "Average Metric: 146.0 / 186  (78.5):  28%|▌ | 185/651 [36:40<1:26:24, 11.12s/it]\u001b[A\n",
      "Average Metric: 146.0 / 186  (78.5):  29%|▌ | 186/651 [36:44<1:24:59, 10.97s/it]\u001b[A\n",
      "Average Metric: 147.0 / 187  (78.6):  29%|▌ | 186/651 [36:47<1:24:59, 10.97s/it]\u001b[A\n",
      "Average Metric: 147.0 / 187  (78.6):  29%|▌ | 187/651 [36:54<1:20:52, 10.46s/it]\u001b[A\n",
      "Average Metric: 148.0 / 188  (78.7):  29%|▌ | 187/651 [37:00<1:20:52, 10.46s/it]\u001b[A\n",
      "Average Metric: 148.0 / 188  (78.7):  29%|▌ | 188/651 [37:04<1:25:23, 11.07s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 149.0 / 189  (78.8):  29%|▌ | 188/651 [37:10<1:25:23, 11.07s/it]\u001b[A\n",
      "Average Metric: 149.0 / 189  (78.8):  29%|▌ | 189/651 [37:12<1:13:59,  9.61s/it]\u001b[A\n",
      "Average Metric: 150.0 / 190  (78.9):  29%|▌ | 189/651 [37:15<1:13:59,  9.61s/it]\u001b[A\n",
      "Average Metric: 150.0 / 190  (78.9):  29%|▌ | 190/651 [37:18<1:06:38,  8.67s/it]\u001b[A\n",
      "Average Metric: 151.0 / 191  (79.1):  29%|▌ | 190/651 [37:21<1:06:38,  8.67s/it]\u001b[A\n",
      "Average Metric: 151.0 / 191  (79.1):  29%|▌ | 191/651 [37:24<1:00:32,  7.90s/it]\u001b[A\n",
      "Average Metric: 151.0 / 192  (78.6):  29%|▌ | 191/651 [37:41<1:00:32,  7.90s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.39s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 151.0 / 192  (78.6):  29%|▌ | 192/651 [37:48<1:34:10, 12.31s/it]\u001b[A\n",
      "Average Metric: 152.0 / 193  (78.8):  29%|▌ | 192/651 [37:52<1:34:10, 12.31s/it]\u001b[A\n",
      "Average Metric: 152.0 / 193  (78.8):  30%|▌ | 193/651 [37:57<1:25:45, 11.23s/it]\u001b[A\n",
      "Average Metric: 153.0 / 194  (78.9):  30%|▌ | 193/651 [38:14<1:25:45, 11.23s/it]\u001b[A\n",
      "Average Metric: 153.0 / 194  (78.9):  30%|▌ | 194/651 [38:16<1:47:26, 14.11s/it]\u001b[A\n",
      "Average Metric: 154.0 / 195  (79.0):  30%|▌ | 194/651 [38:50<1:47:26, 14.11s/it]\u001b[A\n",
      "Average Metric: 154.0 / 195  (79.0):  30%|▌ | 195/651 [38:54<2:43:21, 21.49s/it]\u001b[A\n",
      "Average Metric: 155.0 / 196  (79.1):  30%|▌ | 195/651 [38:59<2:43:21, 21.49s/it]\u001b[A\n",
      "Average Metric: 155.0 / 196  (79.1):  30%|▌ | 196/651 [39:01<2:08:54, 17.00s/it]\u001b[A\n",
      "Average Metric: 156.0 / 197  (79.2):  30%|▌ | 196/651 [39:04<2:08:54, 17.00s/it]\u001b[A\n",
      "Average Metric: 156.0 / 197  (79.2):  30%|▌ | 197/651 [39:07<1:45:03, 13.88s/it]\u001b[A\n",
      "Average Metric: 157.0 / 198  (79.3):  30%|▌ | 197/651 [39:33<1:45:03, 13.88s/it]\u001b[A\n",
      "Average Metric: 157.0 / 198  (79.3):  30%|▌ | 198/651 [39:38<2:15:39, 17.97s/it]\u001b[A\n",
      "Average Metric: 157.0 / 199  (78.9):  30%|▌ | 198/651 [39:48<2:15:39, 17.97s/it]\u001b[A\n",
      "Average Metric: 157.0 / 199  (78.9):  31%|▌ | 199/651 [39:51<2:09:35, 17.20s/it]\u001b[A\n",
      "Average Metric: 158.0 / 200  (79.0):  31%|▌ | 199/651 [40:14<2:09:35, 17.20s/it]\u001b[A\n",
      "Average Metric: 158.0 / 200  (79.0):  31%|▌ | 200/651 [40:18<2:31:05, 20.10s/it]\u001b[A\n",
      "Average Metric: 159.0 / 201  (79.1):  31%|▌ | 200/651 [40:23<2:31:05, 20.10s/it]\u001b[A\n",
      "Average Metric: 159.0 / 201  (79.1):  31%|▌ | 201/651 [40:30<2:08:08, 17.09s/it]\u001b[A\n",
      "Average Metric: 160.0 / 202  (79.2):  31%|▌ | 201/651 [40:35<2:08:08, 17.09s/it]\u001b[A\n",
      "Average Metric: 160.0 / 202  (79.2):  31%|▌ | 202/651 [40:41<1:57:53, 15.75s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.11s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 161.0 / 203  (79.3):  31%|▌ | 202/651 [40:50<1:57:53, 15.75s/it]\u001b[A\n",
      "Average Metric: 161.0 / 203  (79.3):  31%|▌ | 203/651 [40:54<1:53:14, 15.17s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.06s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 162.0 / 204  (79.4):  31%|▌ | 203/651 [41:12<1:53:14, 15.17s/it]\u001b[A\n",
      "Average Metric: 162.0 / 204  (79.4):  31%|▋ | 204/651 [41:15<2:04:29, 16.71s/it]\u001b[A\n",
      "Average Metric: 162.0 / 205  (79.0):  31%|▋ | 204/651 [41:31<2:04:29, 16.71s/it]\u001b[A\n",
      "Average Metric: 162.0 / 205  (79.0):  31%|▋ | 205/651 [41:39<2:13:50, 18.01s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.77s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 163.0 / 206  (79.1):  31%|▋ | 205/651 [41:52<2:13:50, 18.01s/it]\u001b[A\n",
      "Average Metric: 163.0 / 206  (79.1):  32%|▋ | 206/651 [41:56<2:18:36, 18.69s/it]\u001b[A\n",
      "Average Metric: 164.0 / 207  (79.2):  32%|▋ | 206/651 [42:01<2:18:36, 18.69s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.00s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 164.0 / 207  (79.2):  32%|▋ | 207/651 [42:06<2:01:05, 16.36s/it]\u001b[A\n",
      "Average Metric: 165.0 / 208  (79.3):  32%|▋ | 207/651 [42:10<2:01:05, 16.36s/it]\u001b[A\n",
      "Average Metric: 165.0 / 208  (79.3):  32%|▋ | 208/651 [42:14<1:39:01, 13.41s/it]\u001b[A\n",
      "Average Metric: 166.0 / 209  (79.4):  32%|▋ | 208/651 [42:24<1:39:01, 13.41s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.24s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 166.0 / 209  (79.4):  32%|▋ | 209/651 [42:28<1:40:46, 13.68s/it]\u001b[A\n",
      "Average Metric: 167.0 / 210  (79.5):  32%|▋ | 209/651 [42:37<1:40:46, 13.68s/it]\u001b[A\n",
      "Average Metric: 167.0 / 210  (79.5):  32%|▋ | 210/651 [42:40<1:38:10, 13.36s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.42s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 167.0 / 211  (79.1):  32%|▋ | 210/651 [43:02<1:38:10, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Average Metric: 167.0 / 211  (79.1):  32%|▋ | 211/651 [43:08<2:05:48, 17.15s/it]\u001b[A\n",
      "Average Metric: 168.0 / 212  (79.2):  32%|▋ | 211/651 [43:15<2:05:48, 17.15s/it]\u001b[A\n",
      "Average Metric: 168.0 / 212  (79.2):  33%|▋ | 212/651 [43:19<1:55:01, 15.72s/it]\u001b[A\n",
      "Average Metric: 169.0 / 213  (79.3):  33%|▋ | 212/651 [43:25<1:55:01, 15.72s/it]\u001b[A\n",
      "Average Metric: 169.0 / 213  (79.3):  33%|▋ | 213/651 [43:33<1:45:05, 14.40s/it]\u001b[A\n",
      "Average Metric: 169.0 / 214  (79.0):  33%|▋ | 213/651 [43:42<1:45:05, 14.40s/it]\u001b[A\n",
      "Average Metric: 169.0 / 214  (79.0):  33%|▋ | 214/651 [43:48<1:51:51, 15.36s/it]\u001b[A\n",
      "Average Metric: 170.0 / 215  (79.1):  33%|▋ | 214/651 [43:50<1:51:51, 15.36s/it]\u001b[A\n",
      "Average Metric: 170.0 / 215  (79.1):  33%|▋ | 215/651 [43:52<1:28:11, 12.14s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.24s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.54s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 171.0 / 216  (79.2):  33%|▋ | 215/651 [43:59<1:28:11, 12.14s/it]\u001b[A\n",
      "Average Metric: 171.0 / 216  (79.2):  33%|▋ | 216/651 [44:06<1:28:44, 12.24s/it]\u001b[A\n",
      "Average Metric: 172.0 / 217  (79.3):  33%|▋ | 216/651 [44:14<1:28:44, 12.24s/it]\u001b[A\n",
      "Average Metric: 172.0 / 217  (79.3):  33%|▋ | 217/651 [44:17<1:26:59, 12.03s/it]\u001b[A\n",
      "Average Metric: 173.0 / 218  (79.4):  33%|▋ | 217/651 [44:24<1:26:59, 12.03s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.69s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 173.0 / 218  (79.4):  33%|▋ | 218/651 [44:31<1:28:16, 12.23s/it]\u001b[A\n",
      "Average Metric: 174.0 / 219  (79.5):  33%|▋ | 218/651 [44:37<1:28:16, 12.23s/it]\u001b[A\n",
      "Average Metric: 174.0 / 219  (79.5):  34%|▋ | 219/651 [44:41<1:25:23, 11.86s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 174.0 / 220  (79.1):  34%|▋ | 219/651 [44:48<1:25:23, 11.86s/it]\u001b[A\n",
      "Average Metric: 174.0 / 220  (79.1):  34%|▋ | 220/651 [44:50<1:21:32, 11.35s/it]\u001b[A\n",
      "Average Metric: 175.0 / 221  (79.2):  34%|▋ | 220/651 [44:55<1:21:32, 11.35s/it]\u001b[A\n",
      "Average Metric: 175.0 / 221  (79.2):  34%|▋ | 221/651 [44:58<1:15:29, 10.53s/it]\u001b[A\n",
      "Average Metric: 176.0 / 222  (79.3):  34%|▋ | 221/651 [45:01<1:15:29, 10.53s/it]\u001b[A\n",
      "Average Metric: 176.0 / 222  (79.3):  34%|▋ | 222/651 [45:06<1:02:21,  8.72s/it]\u001b[A\n",
      "Average Metric: 177.0 / 223  (79.4):  34%|▋ | 222/651 [45:13<1:02:21,  8.72s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.39s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 177.0 / 223  (79.4):  34%|▋ | 223/651 [45:21<1:18:03, 10.94s/it]\u001b[A\n",
      "Average Metric: 177.0 / 224  (79.0):  34%|▋ | 223/651 [45:24<1:18:03, 10.94s/it]\u001b[A\n",
      "Average Metric: 177.0 / 224  (79.0):  34%|▋ | 224/651 [45:31<1:19:05, 11.11s/it]\u001b[A\n",
      "Average Metric: 178.0 / 225  (79.1):  34%|▋ | 224/651 [45:37<1:19:05, 11.11s/it]\u001b[A\n",
      "Average Metric: 178.0 / 225  (79.1):  35%|▋ | 225/651 [45:42<1:17:14, 10.88s/it]\u001b[A\n",
      "Average Metric: 178.0 / 226  (78.8):  35%|▋ | 225/651 [45:46<1:17:14, 10.88s/it]\u001b[A\n",
      "Average Metric: 178.0 / 226  (78.8):  35%|▋ | 226/651 [45:51<1:16:00, 10.73s/it]\u001b[A\n",
      "Average Metric: 179.0 / 227  (78.9):  35%|▋ | 226/651 [45:53<1:16:00, 10.73s/it]\u001b[A\n",
      "Average Metric: 179.0 / 227  (78.9):  35%|▋ | 227/651 [45:57<1:04:56,  9.19s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.99s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 179.0 / 228  (78.5):  35%|▋ | 227/651 [46:15<1:04:56,  9.19s/it]\u001b[A\n",
      "Average Metric: 179.0 / 228  (78.5):  35%|▋ | 228/651 [46:20<1:29:57, 12.76s/it]\u001b[A\n",
      "Average Metric: 180.0 / 229  (78.6):  35%|▋ | 228/651 [46:22<1:29:57, 12.76s/it]\u001b[A\n",
      "Average Metric: 180.0 / 229  (78.6):  35%|▋ | 229/651 [46:25<1:18:12, 11.12s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.08s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.91s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 5.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.73s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 181.0 / 230  (78.7):  35%|▋ | 229/651 [47:00<1:18:12, 11.12s/it]\u001b[A\n",
      "Average Metric: 181.0 / 230  (78.7):  35%|▋ | 230/651 [47:03<2:12:55, 18.94s/it]\u001b[A\n",
      "Average Metric: 181.0 / 231  (78.4):  35%|▋ | 230/651 [47:05<2:12:55, 18.94s/it]\u001b[A\n",
      "Average Metric: 181.0 / 231  (78.4):  35%|▋ | 231/651 [47:10<1:48:15, 15.47s/it]\u001b[A\n",
      "Average Metric: 182.0 / 232  (78.4):  35%|▋ | 231/651 [47:17<1:48:15, 15.47s/it]\u001b[A\n",
      "Average Metric: 182.0 / 232  (78.4):  36%|▋ | 232/651 [47:21<1:36:29, 13.82s/it]\u001b[A\n",
      "Average Metric: 182.0 / 233  (78.1):  36%|▋ | 232/651 [47:24<1:36:29, 13.82s/it]\u001b[A\n",
      "Average Metric: 182.0 / 233  (78.1):  36%|▋ | 233/651 [47:28<1:23:40, 12.01s/it]\u001b[A\n",
      "Average Metric: 183.0 / 234  (78.2):  36%|▋ | 233/651 [47:34<1:23:40, 12.01s/it]\u001b[A\n",
      "Average Metric: 183.0 / 234  (78.2):  36%|▋ | 234/651 [47:39<1:18:55, 11.36s/it]\u001b[A\n",
      "Average Metric: 184.0 / 235  (78.3):  36%|▋ | 234/651 [47:44<1:18:55, 11.36s/it]\u001b[A\n",
      "Average Metric: 184.0 / 235  (78.3):  36%|▋ | 235/651 [47:46<1:13:56, 10.67s/it]\u001b[A\n",
      "Average Metric: 185.0 / 236  (78.4):  36%|▋ | 235/651 [47:49<1:13:56, 10.67s/it]\u001b[A\n",
      "Average Metric: 185.0 / 236  (78.4):  36%|▋ | 236/651 [47:53<1:04:49,  9.37s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 186.0 / 237  (78.5):  36%|▋ | 236/651 [48:01<1:04:49,  9.37s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.13s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.15s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 186.0 / 237  (78.5):  36%|▋ | 237/651 [48:11<1:16:58, 11.16s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.06s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 187.0 / 238  (78.6):  36%|▋ | 237/651 [48:17<1:16:58, 11.16s/it]\u001b[A\n",
      "Average Metric: 187.0 / 238  (78.6):  37%|▋ | 238/651 [48:23<1:21:43, 11.87s/it]\u001b[A\n",
      "Average Metric: 188.0 / 239  (78.7):  37%|▋ | 238/651 [48:27<1:21:43, 11.87s/it]\u001b[A\n",
      "Average Metric: 188.0 / 239  (78.7):  37%|▋ | 239/651 [48:32<1:17:02, 11.22s/it]\u001b[A\n",
      "Average Metric: 189.0 / 240  (78.8):  37%|▋ | 239/651 [48:38<1:17:02, 11.22s/it]\u001b[A\n",
      "Average Metric: 189.0 / 240  (78.8):  37%|▋ | 240/651 [48:41<1:11:04, 10.38s/it]\u001b[A\n",
      "Average Metric: 190.0 / 241  (78.8):  37%|▋ | 240/651 [48:44<1:11:04, 10.38s/it]\u001b[A\n",
      "Average Metric: 190.0 / 241  (78.8):  37%|▋ | 241/651 [48:48<1:03:00,  9.22s/it]\u001b[A\n",
      "Average Metric: 191.0 / 242  (78.9):  37%|▋ | 241/651 [48:53<1:03:00,  9.22s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.10s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 191.0 / 242  (78.9):  37%|▋ | 242/651 [49:04<1:07:28,  9.90s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.98s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 191.0 / 243  (78.6):  37%|▋ | 242/651 [49:12<1:07:28,  9.90s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.30s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 191.0 / 243  (78.6):  37%|▋ | 243/651 [49:17<1:22:36, 12.15s/it]\u001b[A\n",
      "Average Metric: 192.0 / 244  (78.7):  37%|▋ | 243/651 [49:23<1:22:36, 12.15s/it]\u001b[A\n",
      "Average Metric: 192.0 / 244  (78.7):  37%|▋ | 244/651 [49:26<1:19:29, 11.72s/it]\u001b[A\n",
      "Average Metric: 192.0 / 245  (78.4):  37%|▋ | 244/651 [49:32<1:19:29, 11.72s/it]\u001b[A\n",
      "Average Metric: 192.0 / 245  (78.4):  38%|▊ | 245/651 [49:42<1:22:01, 12.12s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 6.70s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "IOStream.flush timed out\n",
      "\n",
      "Average Metric: 192.0 / 246  (78.0):  38%|▊ | 245/651 [50:22<1:22:01, 12.12s/it]\u001b[A\n",
      "Average Metric: 192.0 / 246  (78.0):  38%|▊ | 246/651 [50:27<2:30:39, 22.32s/it]\u001b[A\n",
      "Average Metric: 193.0 / 247  (78.1):  38%|▊ | 246/651 [50:31<2:30:39, 22.32s/it]\u001b[A\n",
      "Average Metric: 193.0 / 247  (78.1):  38%|▊ | 247/651 [50:34<2:02:36, 18.21s/it]\u001b[A\n",
      "Average Metric: 194.0 / 248  (78.2):  38%|▊ | 247/651 [50:38<2:02:36, 18.21s/it]\u001b[A\n",
      "Average Metric: 194.0 / 248  (78.2):  38%|▊ | 248/651 [50:43<1:40:27, 14.96s/it]\u001b[A\n",
      "Average Metric: 195.0 / 249  (78.3):  38%|▊ | 248/651 [50:45<1:40:27, 14.96s/it]\u001b[A\n",
      "Average Metric: 195.0 / 249  (78.3):  38%|▊ | 249/651 [50:49<1:25:38, 12.78s/it]\u001b[A\n",
      "Average Metric: 195.0 / 250  (78.0):  38%|▊ | 249/651 [50:58<1:25:38, 12.78s/it]\u001b[A\n",
      "Average Metric: 195.0 / 250  (78.0):  38%|▊ | 250/651 [51:04<1:27:29, 13.09s/it]\u001b[A\n",
      "Average Metric: 196.0 / 251  (78.1):  38%|▊ | 250/651 [51:18<1:27:29, 13.09s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 196.0 / 251  (78.1):  39%|▊ | 251/651 [51:23<1:40:10, 15.03s/it]\u001b[A\n",
      "Average Metric: 197.0 / 252  (78.2):  39%|▊ | 251/651 [51:29<1:40:10, 15.03s/it]\u001b[A\n",
      "Average Metric: 197.0 / 252  (78.2):  39%|▊ | 252/651 [51:34<1:31:47, 13.80s/it]\u001b[A\n",
      "Average Metric: 198.0 / 253  (78.3):  39%|▊ | 252/651 [51:42<1:31:47, 13.80s/it]\u001b[A\n",
      "Average Metric: 198.0 / 253  (78.3):  39%|▊ | 253/651 [51:45<1:25:53, 12.95s/it]\u001b[A\n",
      "Average Metric: 199.0 / 254  (78.3):  39%|▊ | 253/651 [52:11<1:25:53, 12.95s/it]\u001b[A\n",
      "Average Metric: 199.0 / 254  (78.3):  39%|▊ | 254/651 [52:13<1:57:12, 17.71s/it]\u001b[A\n",
      "Average Metric: 200.0 / 255  (78.4):  39%|▊ | 254/651 [52:35<1:57:12, 17.71s/it]\u001b[A\n",
      "Average Metric: 200.0 / 255  (78.4):  39%|▊ | 255/651 [52:38<2:11:21, 19.90s/it]\u001b[A\n",
      "Average Metric: 201.0 / 256  (78.5):  39%|▊ | 255/651 [52:43<2:11:21, 19.90s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.34s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 201.0 / 256  (78.5):  39%|▊ | 256/651 [52:48<1:49:46, 16.68s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.27s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 202.0 / 257  (78.6):  39%|▊ | 256/651 [53:02<1:49:46, 16.68s/it]\u001b[A\n",
      "Average Metric: 202.0 / 257  (78.6):  39%|▊ | 257/651 [53:08<1:55:35, 17.60s/it]\u001b[A\n",
      "Average Metric: 203.0 / 258  (78.7):  39%|▊ | 257/651 [53:16<1:55:35, 17.60s/it]\u001b[A\n",
      "Average Metric: 203.0 / 258  (78.7):  40%|▊ | 258/651 [53:26<1:50:55, 16.94s/it]\u001b[A\n",
      "Average Metric: 204.0 / 259  (78.8):  40%|▊ | 258/651 [53:33<1:50:55, 16.94s/it]\u001b[A\n",
      "Average Metric: 204.0 / 259  (78.8):  40%|▊ | 259/651 [53:39<1:46:59, 16.38s/it]\u001b[A\n",
      "Average Metric: 204.0 / 260  (78.5):  40%|▊ | 259/651 [53:41<1:46:59, 16.38s/it]\u001b[A\n",
      "Average Metric: 204.0 / 260  (78.5):  40%|▊ | 260/651 [53:45<1:28:43, 13.61s/it]\u001b[A\n",
      "Average Metric: 205.0 / 261  (78.5):  40%|▊ | 260/651 [53:47<1:28:43, 13.61s/it]\u001b[A\n",
      "Average Metric: 205.0 / 261  (78.5):  40%|▊ | 261/651 [53:49<1:10:57, 10.92s/it]\u001b[A\n",
      "Average Metric: 206.0 / 262  (78.6):  40%|▊ | 261/651 [53:52<1:10:57, 10.92s/it]\u001b[A\n",
      "Average Metric: 206.0 / 262  (78.6):  40%|▊ | 262/651 [53:56<1:00:20,  9.31s/it]\u001b[A\n",
      "Average Metric: 207.0 / 263  (78.7):  40%|▊ | 262/651 [54:09<1:00:20,  9.31s/it]\u001b[A\n",
      "Average Metric: 207.0 / 263  (78.7):  40%|▊ | 263/651 [54:14<1:16:51, 11.89s/it]\u001b[A\n",
      "Average Metric: 208.0 / 264  (78.8):  40%|▊ | 263/651 [54:22<1:16:51, 11.89s/it]\u001b[A\n",
      "Average Metric: 208.0 / 264  (78.8):  41%|▊ | 264/651 [54:26<1:17:29, 12.01s/it]\u001b[A\n",
      "Average Metric: 209.0 / 265  (78.9):  41%|▊ | 264/651 [54:34<1:17:29, 12.01s/it]\u001b[A\n",
      "Average Metric: 209.0 / 265  (78.9):  41%|▊ | 265/651 [54:39<1:20:11, 12.47s/it]\u001b[A\n",
      "Average Metric: 210.0 / 266  (78.9):  41%|▊ | 265/651 [54:46<1:20:11, 12.47s/it]\u001b[A\n",
      "Average Metric: 210.0 / 266  (78.9):  41%|▊ | 266/651 [54:53<1:22:15, 12.82s/it]\u001b[A\n",
      "Average Metric: 211.0 / 267  (79.0):  41%|▊ | 266/651 [55:01<1:22:15, 12.82s/it]\u001b[A\n",
      "Average Metric: 211.0 / 267  (79.0):  41%|▊ | 267/651 [55:07<1:20:24, 12.56s/it]\u001b[A\n",
      "Average Metric: 212.0 / 268  (79.1):  41%|▊ | 267/651 [55:11<1:20:24, 12.56s/it]\u001b[A\n",
      "Average Metric: 212.0 / 268  (79.1):  41%|▊ | 268/651 [55:16<1:17:30, 12.14s/it]\u001b[A\n",
      "Average Metric: 213.0 / 269  (79.2):  41%|▊ | 268/651 [55:20<1:17:30, 12.14s/it]\u001b[A\n",
      "Average Metric: 213.0 / 269  (79.2):  41%|▊ | 269/651 [55:23<1:08:19, 10.73s/it]\u001b[A\n",
      "Average Metric: 214.0 / 270  (79.3):  41%|▊ | 269/651 [55:27<1:08:19, 10.73s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 214.0 / 270  (79.3):  41%|▊ | 270/651 [55:36<1:05:35, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Average Metric: 215.0 / 271  (79.3):  41%|▊ | 270/651 [55:43<1:05:35, 10.33s/it]\u001b[A\n",
      "Average Metric: 215.0 / 271  (79.3):  42%|▊ | 271/651 [55:49<1:15:49, 11.97s/it]\u001b[A\n",
      "Average Metric: 215.0 / 272  (79.0):  42%|▊ | 271/651 [55:53<1:15:49, 11.97s/it]\u001b[A\n",
      "Average Metric: 215.0 / 272  (79.0):  42%|▊ | 272/651 [55:55<1:07:49, 10.74s/it]\u001b[A\n",
      "Average Metric: 215.0 / 273  (78.8):  42%|▊ | 272/651 [55:58<1:07:49, 10.74s/it]\u001b[A\n",
      "Average Metric: 215.0 / 273  (78.8):  42%|█▋  | 273/651 [56:01<56:03,  8.90s/it]\u001b[A\n",
      "Average Metric: 216.0 / 274  (78.8):  42%|█▋  | 273/651 [56:03<56:03,  8.90s/it]\u001b[A\n",
      "Average Metric: 216.0 / 274  (78.8):  42%|█▋  | 274/651 [56:07<48:57,  7.79s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.41s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 217.0 / 275  (78.9):  42%|█▋  | 274/651 [56:25<48:57,  7.79s/it]\u001b[A\n",
      "Average Metric: 217.0 / 275  (78.9):  42%|▊ | 275/651 [56:30<1:17:01, 12.29s/it]\u001b[A\n",
      "Average Metric: 218.0 / 276  (79.0):  42%|▊ | 275/651 [56:34<1:17:01, 12.29s/it]\u001b[A\n",
      "Average Metric: 218.0 / 276  (79.0):  42%|▊ | 276/651 [56:38<1:11:12, 11.39s/it]\u001b[A\n",
      "Average Metric: 219.0 / 277  (79.1):  42%|▊ | 276/651 [56:43<1:11:12, 11.39s/it]\u001b[A\n",
      "Average Metric: 219.0 / 277  (79.1):  43%|▊ | 277/651 [56:47<1:06:36, 10.68s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 4.02s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 219.0 / 278  (78.8):  43%|▊ | 277/651 [56:55<1:06:36, 10.68s/it]\u001b[A\n",
      "Average Metric: 219.0 / 278  (78.8):  43%|▊ | 278/651 [57:03<1:10:40, 11.37s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 220.0 / 279  (78.9):  43%|▊ | 278/651 [57:08<1:10:40, 11.37s/it]\u001b[A\n",
      "Average Metric: 220.0 / 279  (78.9):  43%|▊ | 279/651 [57:12<1:10:44, 11.41s/it]\u001b[A\n",
      "Average Metric: 221.0 / 280  (78.9):  43%|▊ | 279/651 [57:15<1:10:44, 11.41s/it]\u001b[A\n",
      "Average Metric: 221.0 / 280  (78.9):  43%|▊ | 280/651 [57:21<1:03:00, 10.19s/it]\u001b[A\n",
      "Average Metric: 222.0 / 281  (79.0):  43%|▊ | 280/651 [57:26<1:03:00, 10.19s/it]\u001b[A\n",
      "Average Metric: 222.0 / 281  (79.0):  43%|▊ | 281/651 [57:33<1:07:58, 11.02s/it]\u001b[A\n",
      "Average Metric: 223.0 / 282  (79.1):  43%|▊ | 281/651 [57:40<1:07:58, 11.02s/it]\u001b[A\n",
      "Average Metric: 223.0 / 282  (79.1):  43%|▊ | 282/651 [57:43<1:07:42, 11.01s/it]\u001b[A\n",
      "Average Metric: 224.0 / 283  (79.2):  43%|▊ | 282/651 [57:47<1:07:42, 11.01s/it]\u001b[A\n",
      "Average Metric: 224.0 / 283  (79.2):  43%|█▋  | 283/651 [57:50<59:52,  9.76s/it]\u001b[A\n",
      "Average Metric: 225.0 / 284  (79.2):  43%|█▋  | 283/651 [57:55<59:52,  9.76s/it]\u001b[A\n",
      "Average Metric: 225.0 / 284  (79.2):  44%|█▋  | 284/651 [57:58<55:52,  9.13s/it]\u001b[A\n",
      "Average Metric: 226.0 / 285  (79.3):  44%|█▋  | 284/651 [58:03<55:52,  9.13s/it]\u001b[A\n",
      "Average Metric: 226.0 / 285  (79.3):  44%|█▊  | 285/651 [58:08<55:50,  9.15s/it]\u001b[A\n",
      "Average Metric: 227.0 / 286  (79.4):  44%|█▊  | 285/651 [58:14<55:50,  9.15s/it]\u001b[A\n",
      "Average Metric: 227.0 / 286  (79.4):  44%|█▊  | 286/651 [58:17<56:51,  9.35s/it]\u001b[A\n",
      "Average Metric: 228.0 / 287  (79.4):  44%|█▊  | 286/651 [58:23<56:51,  9.35s/it]\u001b[A\n",
      "Average Metric: 228.0 / 287  (79.4):  44%|█▊  | 287/651 [58:28<58:20,  9.62s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.73s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "Average Metric: 228.0 / 288  (79.2):  44%|█▊  | 287/651 [58:38<58:20,  9.62s/it]\u001b[A\n",
      "Average Metric: 228.0 / 288  (79.2):  44%|▉ | 288/651 [58:43<1:08:53, 11.39s/it]\u001b[A\n",
      "Average Metric: 229.0 / 289  (79.2):  44%|▉ | 288/651 [58:46<1:08:53, 11.39s/it]\u001b[A\n",
      "Average Metric: 229.0 / 289  (79.2):  44%|▉ | 289/651 [58:50<1:02:00, 10.28s/it]\u001b[A\n",
      "Average Metric: 230.0 / 290  (79.3):  44%|▉ | 289/651 [58:54<1:02:00, 10.28s/it]\u001b[A\n",
      "Average Metric: 230.0 / 290  (79.3):  45%|█▊  | 290/651 [58:57<56:42,  9.42s/it]\u001b[A\n",
      "Average Metric: 230.0 / 291  (79.0):  45%|█▊  | 290/651 [59:02<56:42,  9.42s/it]\u001b[A\n",
      "Average Metric: 230.0 / 291  (79.0):  45%|█▊  | 291/651 [59:07<56:55,  9.49s/it]\u001b[A\n",
      "Average Metric: 231.0 / 292  (79.1):  45%|█▊  | 291/651 [59:12<56:55,  9.49s/it]\u001b[A\n",
      "Average Metric: 231.0 / 292  (79.1):  45%|█▊  | 292/651 [59:13<53:10,  8.89s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 3.11s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 232.0 / 293  (79.2):  45%|█▊  | 292/651 [59:41<53:10,  8.89s/it]\u001b[A\n",
      "Average Metric: 232.0 / 293  (79.2):  45%|▉ | 293/651 [59:43<1:29:22, 14.98s/it]\u001b[A\n",
      "Average Metric: 233.0 / 294  (79.3):  45%|▉ | 293/651 [59:52<1:29:22, 14.98s/it]\u001b[A\n",
      "Average Metric: 233.0 / 294  (79.3):  45%|▉ | 294/651 [59:53<1:21:11, 13.65s/it]\u001b[A\n",
      "Average Metric: 234.0 / 295  (79.3):  45%|▍| 294/651 [1:00:14<1:21:11, 13.65s/it\u001b[A\n",
      "Average Metric: 234.0 / 295  (79.3):  45%|▍| 295/651 [1:00:15<1:36:21, 16.24s/it\u001b[A\n",
      "Average Metric: 235.0 / 296  (79.4):  45%|▍| 295/651 [1:00:37<1:36:21, 16.24s/it\u001b[A\n",
      "Average Metric: 235.0 / 296  (79.4):  45%|▍| 296/651 [1:00:40<1:49:29, 18.50s/it\u001b[A\n",
      "Average Metric: 235.0 / 297  (79.1):  45%|▍| 296/651 [1:00:41<1:49:29, 18.50s/it\u001b[A\n",
      "Average Metric: 235.0 / 297  (79.1):  46%|▍| 297/651 [1:00:43<1:23:53, 14.22s/it\u001b[A\n",
      "Average Metric: 236.0 / 298  (79.2):  46%|▍| 297/651 [1:01:11<1:23:53, 14.22s/it\u001b[A\n",
      "Average Metric: 236.0 / 298  (79.2):  46%|▍| 298/651 [1:01:13<1:50:03, 18.71s/it\u001b[A\n",
      "Average Metric: 237.0 / 299  (79.3):  46%|▍| 298/651 [1:01:24<1:50:03, 18.71s/it\u001b[A\n",
      "Average Metric: 237.0 / 299  (79.3):  46%|▍| 299/651 [1:01:26<1:38:55, 16.86s/it\u001b[A\n",
      "Average Metric: 238.0 / 300  (79.3):  46%|▍| 299/651 [1:01:39<1:38:55, 16.86s/it\u001b[A\n",
      "Average Metric: 238.0 / 300  (79.3):  46%|▍| 300/651 [1:01:41<1:36:22, 16.47s/it\u001b[A\n",
      "Average Metric: 239.0 / 301  (79.4):  46%|▍| 300/651 [1:01:42<1:36:22, 16.47s/it\u001b[A\n",
      "Average Metric: 239.0 / 301  (79.4):  46%|▍| 301/651 [1:01:45<1:13:18, 12.57s/it\u001b[A\n",
      "Average Metric: 240.0 / 302  (79.5):  46%|▍| 301/651 [1:01:55<1:13:18, 12.57s/it\u001b[A\n",
      "Average Metric: 240.0 / 302  (79.5):  46%|▍| 302/651 [1:01:56<1:11:48, 12.35s/it\u001b[A\n",
      "Average Metric: 241.0 / 303  (79.5):  46%|▍| 302/651 [1:02:05<1:11:48, 12.35s/it\u001b[A\n",
      "Average Metric: 241.0 / 303  (79.5):  47%|▍| 303/651 [1:02:06<1:06:50, 11.52s/it\u001b[A\n",
      "Average Metric: 242.0 / 304  (79.6):  47%|▍| 303/651 [1:02:15<1:06:50, 11.52s/it\u001b[A\n",
      "Average Metric: 242.0 / 304  (79.6):  47%|▍| 304/651 [1:02:17<1:06:01, 11.42s/it\u001b[A\n",
      "Average Metric: 243.0 / 305  (79.7):  47%|▍| 304/651 [1:02:30<1:06:01, 11.42s/it\u001b[A\n",
      "Average Metric: 243.0 / 305  (79.7):  47%|▍| 305/651 [1:02:34<1:13:35, 12.76s/it\u001b[A\n",
      "Average Metric: 243.0 / 306  (79.4):  47%|▍| 305/651 [1:02:36<1:13:35, 12.76s/it\u001b[A\n",
      "Average Metric: 243.0 / 306  (79.4):  47%|▉ | 306/651 [1:02:37<59:36, 10.37s/it]\u001b[A\n",
      "Average Metric: 244.0 / 307  (79.5):  47%|▉ | 306/651 [1:03:27<59:36, 10.37s/it]\u001b[A\n",
      "Average Metric: 244.0 / 307  (79.5):  47%|▍| 307/651 [1:03:28<2:08:47, 22.46s/it\u001b[A\n",
      "Average Metric: 245.0 / 308  (79.5):  47%|▍| 307/651 [1:03:36<2:08:47, 22.46s/it\u001b[A\n",
      "Average Metric: 245.0 / 308  (79.5):  47%|▍| 308/651 [1:03:37<1:45:14, 18.41s/it\u001b[A\n",
      "Average Metric: 246.0 / 309  (79.6):  47%|▍| 308/651 [1:03:42<1:45:14, 18.41s/it\u001b[A\n",
      "Average Metric: 246.0 / 309  (79.6):  47%|▍| 309/651 [1:03:44<1:24:11, 14.77s/it\u001b[A\n",
      "Average Metric: 247.0 / 310  (79.7):  47%|▍| 309/651 [1:03:50<1:24:11, 14.77s/it\u001b[A\n",
      "Average Metric: 247.0 / 310  (79.7):  48%|▍| 310/651 [1:03:53<1:15:53, 13.35s/it\u001b[A\n",
      "Average Metric: 248.0 / 311  (79.7):  48%|▍| 310/651 [1:03:59<1:15:53, 13.35s/it\u001b[A\n",
      "Average Metric: 248.0 / 311  (79.7):  48%|▍| 311/651 [1:04:01<1:06:57, 11.82s/it\u001b[A\n",
      "Average Metric: 249.0 / 312  (79.8):  48%|▍| 311/651 [1:04:36<1:06:57, 11.82s/it\u001b[A\n",
      "Average Metric: 249.0 / 312  (79.8):  48%|▍| 312/651 [1:04:38<1:47:20, 19.00s/it\u001b[A\n",
      "Average Metric: 250.0 / 313  (79.9):  48%|▍| 312/651 [1:04:44<1:47:20, 19.00s/it\u001b[A\n",
      "Average Metric: 250.0 / 313  (79.9):  48%|▍| 313/651 [1:04:46<1:29:30, 15.89s/it\u001b[A\n",
      "Average Metric: 251.0 / 314  (79.9):  48%|▍| 313/651 [1:04:48<1:29:30, 15.89s/it\u001b[A\n",
      "Average Metric: 251.0 / 314  (79.9):  48%|▍| 314/651 [1:04:49<1:08:09, 12.13s/it\u001b[A\n",
      "Average Metric: 252.0 / 315  (80.0):  48%|▍| 314/651 [1:05:24<1:08:09, 12.13s/it\u001b[A\n",
      "Average Metric: 252.0 / 315  (80.0):  48%|▍| 315/651 [1:05:25<1:47:05, 19.12s/it\u001b[A\n",
      "Average Metric: 253.0 / 316  (80.1):  48%|▍| 315/651 [1:05:32<1:47:05, 19.12s/it\u001b[A\n",
      "Average Metric: 253.0 / 316  (80.1):  49%|▍| 316/651 [1:05:34<1:28:00, 15.76s/it\u001b[A\n",
      "Average Metric: 254.0 / 317  (80.1):  49%|▍| 316/651 [1:05:42<1:28:00, 15.76s/it\u001b[A\n",
      "Average Metric: 254.0 / 317  (80.1):  49%|▍| 317/651 [1:05:46<1:22:15, 14.78s/it\u001b[A\n",
      "Average Metric: 255.0 / 318  (80.2):  49%|▍| 317/651 [1:06:15<1:22:15, 14.78s/it\u001b[A\n",
      "Average Metric: 255.0 / 318  (80.2):  49%|▍| 318/651 [1:06:18<1:51:21, 20.07s/it\u001b[A\n",
      "Average Metric: 255.0 / 319  (79.9):  49%|▍| 318/651 [1:06:31<1:51:21, 20.07s/it\u001b[A\n",
      "Average Metric: 255.0 / 319  (79.9):  49%|▍| 319/651 [1:06:33<1:43:34, 18.72s/it\u001b[A\n",
      "Average Metric: 256.0 / 320  (80.0):  49%|▍| 319/651 [1:06:37<1:43:34, 18.72s/it\u001b[A\n",
      "Average Metric: 256.0 / 320  (80.0):  49%|▍| 320/651 [1:06:38<1:19:30, 14.41s/it\u001b[A\n",
      "Average Metric: 256.0 / 321  (79.8):  49%|▍| 320/651 [1:06:40<1:19:30, 14.41s/it\u001b[A\n",
      "Average Metric: 256.0 / 321  (79.8):  49%|▍| 321/651 [1:06:42<1:01:10, 11.12s/it\u001b[A\n",
      "Average Metric: 257.0 / 322  (79.8):  49%|▍| 321/651 [1:06:47<1:01:10, 11.12s/it\u001b[A\n",
      "Average Metric: 257.0 / 322  (79.8):  49%|▉ | 322/651 [1:06:50<55:59, 10.21s/it]\u001b[A\n",
      "Average Metric: 258.0 / 323  (79.9):  49%|▉ | 322/651 [1:07:32<55:59, 10.21s/it]\u001b[A\n",
      "Average Metric: 258.0 / 323  (79.9):  50%|▍| 323/651 [1:07:32<1:49:17, 19.99s/it\u001b[A\n",
      "Average Metric: 258.0 / 324  (79.6):  50%|▍| 323/651 [1:07:32<1:49:17, 19.99s/it\u001b[A\n",
      "Average Metric: 258.0 / 324  (79.6):  50%|▍| 324/651 [1:07:33<1:17:15, 14.18s/it\u001b[A\n",
      "Average Metric: 259.0 / 325  (79.7):  50%|▍| 324/651 [1:07:37<1:17:15, 14.18s/it\u001b[A\n",
      "Average Metric: 259.0 / 325  (79.7):  50%|▍| 325/651 [1:07:38<1:02:41, 11.54s/it\u001b[A\n",
      "Average Metric: 260.0 / 326  (79.8):  50%|▍| 325/651 [1:08:30<1:02:41, 11.54s/it\u001b[A\n",
      "Average Metric: 260.0 / 326  (79.8):  50%|▌| 326/651 [1:08:31<2:08:55, 23.80s/it\u001b[A\n",
      "Average Metric: 261.0 / 327  (79.8):  50%|▌| 326/651 [1:08:52<2:08:55, 23.80s/it\u001b[A\n",
      "Average Metric: 261.0 / 327  (79.8):  50%|▌| 327/651 [1:08:54<2:06:30, 23.43s/it\u001b[A\n",
      "Average Metric: 262.0 / 328  (79.9):  50%|▌| 327/651 [1:08:55<2:06:30, 23.43s/it\u001b[A\n",
      "Average Metric: 262.0 / 328  (79.9):  50%|▌| 328/651 [1:08:58<1:36:33, 17.94s/it\u001b[A\n",
      "Average Metric: 263.0 / 329  (79.9):  50%|▌| 328/651 [1:09:03<1:36:33, 17.94s/it\u001b[A\n",
      "Average Metric: 263.0 / 329  (79.9):  51%|▌| 329/651 [1:09:09<1:20:38, 15.03s/it\u001b[A\n",
      "Average Metric: 264.0 / 330  (80.0):  51%|▌| 329/651 [1:09:12<1:20:38, 15.03s/it\u001b[A\n",
      "Average Metric: 264.0 / 330  (80.0):  51%|▌| 330/651 [1:09:14<1:07:59, 12.71s/it\u001b[A\n",
      "Average Metric: 265.0 / 331  (80.1):  51%|▌| 330/651 [1:09:35<1:07:59, 12.71s/it\u001b[A\n",
      "Average Metric: 265.0 / 331  (80.1):  51%|▌| 331/651 [1:09:38<1:26:59, 16.31s/it\u001b[A\n",
      "Average Metric: 266.0 / 332  (80.1):  51%|▌| 331/651 [1:09:40<1:26:59, 16.31s/it\u001b[A\n",
      "Average Metric: 266.0 / 332  (80.1):  51%|▌| 332/651 [1:09:42<1:06:25, 12.49s/it\u001b[A\n",
      "Average Metric: 267.0 / 333  (80.2):  51%|▌| 332/651 [1:09:45<1:06:25, 12.49s/it\u001b[A\n",
      "Average Metric: 267.0 / 333  (80.2):  51%|█ | 333/651 [1:09:48<54:19, 10.25s/it]\u001b[A\n",
      "Average Metric: 268.0 / 334  (80.2):  51%|█ | 333/651 [1:10:30<54:19, 10.25s/it]\u001b[A\n",
      "Average Metric: 268.0 / 334  (80.2):  51%|▌| 334/651 [1:10:30<1:46:50, 20.22s/it\u001b[A\n",
      "Average Metric: 269.0 / 335  (80.3):  51%|▌| 334/651 [1:11:16<1:46:50, 20.22s/it\u001b[A\n",
      "Average Metric: 269.0 / 335  (80.3):  51%|▌| 335/651 [1:11:16<2:26:34, 27.83s/it\u001b[A\n",
      "Average Metric: 269.0 / 336  (80.1):  51%|▌| 335/651 [1:11:18<2:26:34, 27.83s/it\u001b[A\n",
      "Average Metric: 269.0 / 336  (80.1):  52%|▌| 336/651 [1:11:21<1:48:36, 20.69s/it\u001b[A\n",
      "Average Metric: 269.0 / 337  (79.8):  52%|▌| 336/651 [1:11:23<1:48:36, 20.69s/it\u001b[A\n",
      "Average Metric: 269.0 / 337  (79.8):  52%|▌| 337/651 [1:11:25<1:23:44, 16.00s/it\u001b[A\n",
      "Average Metric: 270.0 / 338  (79.9):  52%|▌| 337/651 [1:11:35<1:23:44, 16.00s/it\u001b[A\n",
      "Average Metric: 270.0 / 338  (79.9):  52%|▌| 338/651 [1:11:36<1:15:16, 14.43s/it\u001b[A\n",
      "Average Metric: 271.0 / 339  (79.9):  52%|▌| 338/651 [1:11:52<1:15:16, 14.43s/it\u001b[A\n",
      "Average Metric: 271.0 / 339  (79.9):  52%|▌| 339/651 [1:11:57<1:22:04, 15.78s/it\u001b[A\n",
      "Average Metric: 272.0 / 340  (80.0):  52%|▌| 339/651 [1:12:01<1:22:04, 15.78s/it\u001b[A\n",
      "Average Metric: 272.0 / 340  (80.0):  52%|▌| 340/651 [1:12:05<1:13:17, 14.14s/it\u001b[A\n",
      "Average Metric: 273.0 / 341  (80.1):  52%|▌| 340/651 [1:12:13<1:13:17, 14.14s/it\u001b[A\n",
      "Average Metric: 273.0 / 341  (80.1):  52%|▌| 341/651 [1:12:16<1:05:59, 12.77s/it\u001b[A\n",
      "Average Metric: 274.0 / 342  (80.1):  52%|▌| 341/651 [1:13:18<1:05:59, 12.77s/it\u001b[A\n",
      "Average Metric: 274.0 / 342  (80.1):  53%|▌| 342/651 [1:13:19<2:25:24, 28.24s/it\u001b[A\n",
      "Average Metric: 275.0 / 343  (80.2):  53%|▌| 342/651 [1:13:30<2:25:24, 28.24s/it\u001b[A\n",
      "Average Metric: 275.0 / 343  (80.2):  53%|▌| 343/651 [1:13:33<2:01:48, 23.73s/it\u001b[A\n",
      "Average Metric: 276.0 / 344  (80.2):  53%|▌| 343/651 [1:13:33<2:01:48, 23.73s/it\u001b[A\n",
      "Average Metric: 276.0 / 344  (80.2):  53%|▌| 344/651 [1:13:36<1:31:04, 17.80s/it\u001b[A\n",
      "Average Metric: 277.0 / 345  (80.3):  53%|▌| 344/651 [1:13:49<1:31:04, 17.80s/it\u001b[A\n",
      "Average Metric: 277.0 / 345  (80.3):  53%|▌| 345/651 [1:13:50<1:24:30, 16.57s/it\u001b[A\n",
      "Average Metric: 278.0 / 346  (80.3):  53%|▌| 345/651 [1:13:52<1:24:30, 16.57s/it\u001b[A\n",
      "Average Metric: 278.0 / 346  (80.3):  53%|▌| 346/651 [1:13:54<1:03:55, 12.57s/it\u001b[A\n",
      "Average Metric: 279.0 / 347  (80.4):  53%|▌| 346/651 [1:13:58<1:03:55, 12.57s/it\u001b[A\n",
      "Average Metric: 279.0 / 347  (80.4):  53%|█ | 347/651 [1:14:00<54:31, 10.76s/it]\u001b[A\n",
      "Average Metric: 280.0 / 348  (80.5):  53%|█ | 347/651 [1:14:03<54:31, 10.76s/it]\u001b[A\n",
      "Average Metric: 280.0 / 348  (80.5):  53%|█ | 348/651 [1:14:07<47:09,  9.34s/it]\u001b[A\n",
      "Average Metric: 281.0 / 349  (80.5):  53%|█ | 348/651 [1:14:51<47:09,  9.34s/it]\u001b[A\n",
      "Average Metric: 281.0 / 349  (80.5):  54%|▌| 349/651 [1:14:52<1:42:40, 20.40s/it\u001b[A\n",
      "Average Metric: 282.0 / 350  (80.6):  54%|▌| 349/651 [1:14:57<1:42:40, 20.40s/it\u001b[A\n",
      "Average Metric: 282.0 / 350  (80.6):  54%|▌| 350/651 [1:14:58<1:20:24, 16.03s/it\u001b[A\n",
      "Average Metric: 282.0 / 351  (80.3):  54%|▌| 350/651 [1:15:00<1:20:24, 16.03s/it\u001b[A\n",
      "Average Metric: 282.0 / 351  (80.3):  54%|▌| 351/651 [1:15:01<1:01:57, 12.39s/it\u001b[A\n",
      "Average Metric: 283.0 / 352  (80.4):  54%|▌| 351/651 [1:15:19<1:01:57, 12.39s/it\u001b[A\n",
      "Average Metric: 283.0 / 352  (80.4):  54%|▌| 352/651 [1:15:21<1:11:54, 14.43s/it\u001b[A\n",
      "Average Metric: 284.0 / 353  (80.5):  54%|▌| 352/651 [1:15:31<1:11:54, 14.43s/it\u001b[A\n",
      "Average Metric: 284.0 / 353  (80.5):  54%|▌| 353/651 [1:15:32<1:06:21, 13.36s/it\u001b[A\n",
      "Average Metric: 285.0 / 354  (80.5):  54%|▌| 353/651 [1:15:35<1:06:21, 13.36s/it\u001b[A\n",
      "Average Metric: 285.0 / 354  (80.5):  54%|█ | 354/651 [1:15:35<51:43, 10.45s/it]\u001b[A\n",
      "Average Metric: 286.0 / 355  (80.6):  54%|█ | 354/651 [1:15:46<51:43, 10.45s/it]\u001b[A\n",
      "Average Metric: 286.0 / 355  (80.6):  55%|█ | 355/651 [1:15:47<52:57, 10.74s/it]\u001b[A\n",
      "Average Metric: 287.0 / 356  (80.6):  55%|█ | 355/651 [1:16:36<52:57, 10.74s/it]\u001b[A\n",
      "Average Metric: 287.0 / 356  (80.6):  55%|▌| 356/651 [1:16:38<1:51:56, 22.77s/it\u001b[A\n",
      "Average Metric: 288.0 / 357  (80.7):  55%|▌| 356/651 [1:16:39<1:51:56, 22.77s/it\u001b[A\n",
      "Average Metric: 288.0 / 357  (80.7):  55%|▌| 357/651 [1:16:41<1:22:12, 16.78s/it\u001b[A\n",
      "Average Metric: 289.0 / 358  (80.7):  55%|▌| 357/651 [1:16:42<1:22:12, 16.78s/it\u001b[A\n",
      "Average Metric: 289.0 / 358  (80.7):  55%|▌| 358/651 [1:16:44<1:01:58, 12.69s/it\u001b[A\n",
      "Average Metric: 290.0 / 359  (80.8):  55%|▌| 358/651 [1:16:46<1:01:58, 12.69s/it\u001b[A\n",
      "Average Metric: 290.0 / 359  (80.8):  55%|█ | 359/651 [1:16:49<50:56, 10.47s/it]\u001b[A\n",
      "Average Metric: 291.0 / 360  (80.8):  55%|█ | 359/651 [1:17:09<50:56, 10.47s/it]\u001b[A\n",
      "Average Metric: 291.0 / 360  (80.8):  55%|▌| 360/651 [1:17:11<1:06:38, 13.74s/it\u001b[A\n",
      "Average Metric: 292.0 / 361  (80.9):  55%|▌| 360/651 [1:17:19<1:06:38, 13.74s/it\u001b[A\n",
      "Average Metric: 292.0 / 361  (80.9):  55%|▌| 361/651 [1:17:22<1:03:16, 13.09s/it\u001b[A\n",
      "Average Metric: 293.0 / 362  (80.9):  55%|▌| 361/651 [1:18:01<1:03:16, 13.09s/it\u001b[A\n",
      "Average Metric: 293.0 / 362  (80.9):  56%|▌| 362/651 [1:18:03<1:44:19, 21.66s/it\u001b[A\n",
      "Average Metric: 294.0 / 363  (81.0):  56%|▌| 362/651 [1:18:21<1:44:19, 21.66s/it\u001b[A\n",
      "Average Metric: 294.0 / 363  (81.0):  56%|▌| 363/651 [1:18:23<1:41:58, 21.25s/it\u001b[A\n",
      "Average Metric: 294.0 / 364  (80.8):  56%|▌| 363/651 [1:18:28<1:41:58, 21.25s/it\u001b[A\n",
      "Average Metric: 294.0 / 364  (80.8):  56%|▌| 364/651 [1:18:30<1:20:41, 16.87s/it\u001b[A\n",
      "Average Metric: 295.0 / 365  (80.8):  56%|▌| 364/651 [1:18:41<1:20:41, 16.87s/it\u001b[A\n",
      "Average Metric: 295.0 / 365  (80.8):  56%|▌| 365/651 [1:18:42<1:13:02, 15.32s/it\u001b[A\n",
      "Average Metric: 296.0 / 366  (80.9):  56%|▌| 365/651 [1:18:52<1:13:02, 15.32s/it\u001b[A\n",
      "Average Metric: 296.0 / 366  (80.9):  56%|▌| 366/651 [1:18:54<1:06:17, 13.95s/it\u001b[A\n",
      "Average Metric: 297.0 / 367  (80.9):  56%|▌| 366/651 [1:19:05<1:06:17, 13.95s/it\u001b[A\n",
      "Average Metric: 297.0 / 367  (80.9):  56%|▌| 367/651 [1:19:06<1:04:58, 13.73s/it\u001b[A\n",
      "Average Metric: 298.0 / 368  (81.0):  56%|▌| 367/651 [1:19:17<1:04:58, 13.73s/it\u001b[A\n",
      "Average Metric: 298.0 / 368  (81.0):  57%|▌| 368/651 [1:19:19<1:02:26, 13.24s/it\u001b[A\n",
      "Average Metric: 299.0 / 369  (81.0):  57%|▌| 368/651 [1:19:46<1:02:26, 13.24s/it\u001b[A\n",
      "Average Metric: 299.0 / 369  (81.0):  57%|▌| 369/651 [1:19:48<1:25:33, 18.20s/it\u001b[A\n",
      "Average Metric: 299.0 / 370  (80.8):  57%|▌| 369/651 [1:20:12<1:25:33, 18.20s/it\u001b[A\n",
      "Average Metric: 299.0 / 370  (80.8):  57%|▌| 370/651 [1:20:13<1:34:22, 20.15s/it\u001b[A\n",
      "Average Metric: 300.0 / 371  (80.9):  57%|▌| 370/651 [1:20:20<1:34:22, 20.15s/it\u001b[A\n",
      "Average Metric: 300.0 / 371  (80.9):  57%|▌| 371/651 [1:20:22<1:18:11, 16.76s/it\u001b[A\n",
      "Average Metric: 301.0 / 372  (80.9):  57%|▌| 371/651 [1:20:24<1:18:11, 16.76s/it\u001b[A\n",
      "Average Metric: 301.0 / 372  (80.9):  57%|█▏| 372/651 [1:20:26<59:49, 12.87s/it]\u001b[A\n",
      "Average Metric: 302.0 / 373  (81.0):  57%|█▏| 372/651 [1:20:29<59:49, 12.87s/it]\u001b[A\n",
      "Average Metric: 302.0 / 373  (81.0):  57%|█▏| 373/651 [1:20:30<48:03, 10.37s/it]\u001b[A\n",
      "Average Metric: 303.0 / 374  (81.0):  57%|█▏| 373/651 [1:20:34<48:03, 10.37s/it]\u001b[A\n",
      "Average Metric: 303.0 / 374  (81.0):  57%|█▏| 374/651 [1:20:38<43:32,  9.43s/it]\u001b[A\n",
      "Average Metric: 304.0 / 375  (81.1):  57%|█▏| 374/651 [1:20:57<43:32,  9.43s/it]\u001b[A\n",
      "Average Metric: 304.0 / 375  (81.1):  58%|█▏| 375/651 [1:20:57<58:02, 12.62s/it]\u001b[A\n",
      "Average Metric: 305.0 / 376  (81.1):  58%|█▏| 375/651 [1:21:08<58:02, 12.62s/it]\u001b[A\n",
      "Average Metric: 305.0 / 376  (81.1):  58%|█▏| 376/651 [1:21:11<58:26, 12.75s/it]\u001b[A\n",
      "Average Metric: 306.0 / 377  (81.2):  58%|█▏| 376/651 [1:21:25<58:26, 12.75s/it]\u001b[A\n",
      "Average Metric: 306.0 / 377  (81.2):  58%|▌| 377/651 [1:21:27<1:03:26, 13.89s/it\u001b[A\n",
      "Average Metric: 306.0 / 378  (81.0):  58%|▌| 377/651 [1:21:42<1:03:26, 13.89s/it\u001b[A\n",
      "Average Metric: 306.0 / 378  (81.0):  58%|▌| 378/651 [1:21:44<1:08:28, 15.05s/it\u001b[A\n",
      "Average Metric: 306.0 / 379  (80.7):  58%|▌| 378/651 [1:22:06<1:08:28, 15.05s/it\u001b[A\n",
      "Average Metric: 306.0 / 379  (80.7):  58%|▌| 379/651 [1:22:08<1:18:51, 17.39s/it\u001b[A\n",
      "Average Metric: 307.0 / 380  (80.8):  58%|▌| 379/651 [1:22:19<1:18:51, 17.39s/it\u001b[A\n",
      "Average Metric: 307.0 / 380  (80.8):  58%|▌| 380/651 [1:22:20<1:12:34, 16.07s/it\u001b[A\n",
      "Average Metric: 307.0 / 381  (80.6):  58%|▌| 380/651 [1:22:23<1:12:34, 16.07s/it\u001b[A\n",
      "Average Metric: 307.0 / 381  (80.6):  59%|█▏| 381/651 [1:22:24<55:58, 12.44s/it]\u001b[A\n",
      "Average Metric: 307.0 / 382  (80.4):  59%|█▏| 381/651 [1:22:34<55:58, 12.44s/it]\u001b[A\n",
      "Average Metric: 307.0 / 382  (80.4):  59%|█▏| 382/651 [1:22:34<52:46, 11.77s/it]\u001b[A\n",
      "Average Metric: 307.0 / 383  (80.2):  59%|█▏| 382/651 [1:22:51<52:46, 11.77s/it]\u001b[A\n",
      "Average Metric: 307.0 / 383  (80.2):  59%|▌| 383/651 [1:22:52<1:00:15, 13.49s/it\u001b[A\n",
      "Average Metric: 308.0 / 384  (80.2):  59%|▌| 383/651 [1:23:15<1:00:15, 13.49s/it\u001b[A\n",
      "Average Metric: 308.0 / 384  (80.2):  59%|▌| 384/651 [1:23:15<1:13:34, 16.53s/it\u001b[A\n",
      "Average Metric: 309.0 / 385  (80.3):  59%|▌| 384/651 [1:23:24<1:13:34, 16.53s/it\u001b[A\n",
      "Average Metric: 309.0 / 385  (80.3):  59%|▌| 385/651 [1:23:25<1:03:44, 14.38s/it\u001b[A\n",
      "Average Metric: 310.0 / 386  (80.3):  59%|▌| 385/651 [1:23:43<1:03:44, 14.38s/it\u001b[A\n",
      "Average Metric: 310.0 / 386  (80.3):  59%|▌| 386/651 [1:23:45<1:10:39, 16.00s/it\u001b[A\n",
      "Average Metric: 311.0 / 387  (80.4):  59%|▌| 386/651 [1:23:54<1:10:39, 16.00s/it\u001b[A\n",
      "Average Metric: 311.0 / 387  (80.4):  59%|▌| 387/651 [1:23:56<1:04:41, 14.70s/it\u001b[A\n",
      "Average Metric: 312.0 / 388  (80.4):  59%|▌| 387/651 [1:24:01<1:04:41, 14.70s/it\u001b[A\n",
      "Average Metric: 312.0 / 388  (80.4):  60%|█▏| 388/651 [1:24:03<52:31, 11.98s/it]\u001b[A\n",
      "Average Metric: 312.0 / 389  (80.2):  60%|█▏| 388/651 [1:24:10<52:31, 11.98s/it]\u001b[A\n",
      "Average Metric: 312.0 / 389  (80.2):  60%|█▏| 389/651 [1:24:12<49:15, 11.28s/it]\u001b[A\n",
      "Average Metric: 313.0 / 390  (80.3):  60%|█▏| 389/651 [1:24:39<49:15, 11.28s/it]\u001b[A\n",
      "Average Metric: 313.0 / 390  (80.3):  60%|▌| 390/651 [1:24:40<1:10:30, 16.21s/it\u001b[A\n",
      "Average Metric: 314.0 / 391  (80.3):  60%|▌| 390/651 [1:24:41<1:10:30, 16.21s/it\u001b[A\n",
      "Average Metric: 314.0 / 391  (80.3):  60%|█▏| 391/651 [1:24:45<56:03, 12.94s/it]\u001b[A\n",
      "Average Metric: 314.0 / 392  (80.1):  60%|█▏| 391/651 [1:24:58<56:03, 12.94s/it]\u001b[A\n",
      "Average Metric: 314.0 / 392  (80.1):  60%|█▏| 392/651 [1:25:00<58:56, 13.66s/it]\u001b[A\n",
      "Average Metric: 315.0 / 393  (80.2):  60%|█▏| 392/651 [1:25:05<58:56, 13.66s/it]\u001b[A\n",
      "Average Metric: 315.0 / 393  (80.2):  60%|█▏| 393/651 [1:25:08<50:53, 11.84s/it]\u001b[A\n",
      "Average Metric: 316.0 / 394  (80.2):  60%|█▏| 393/651 [1:25:31<50:53, 11.84s/it]\u001b[A\n",
      "Average Metric: 316.0 / 394  (80.2):  61%|▌| 394/651 [1:25:33<1:07:49, 15.83s/it\u001b[A\n",
      "Average Metric: 317.0 / 395  (80.3):  61%|▌| 394/651 [1:25:35<1:07:49, 15.83s/it\u001b[A\n",
      "Average Metric: 317.0 / 395  (80.3):  61%|█▏| 395/651 [1:25:38<53:09, 12.46s/it]\u001b[A\n",
      "Average Metric: 318.0 / 396  (80.3):  61%|█▏| 395/651 [1:25:43<53:09, 12.46s/it]\u001b[A\n",
      "Average Metric: 318.0 / 396  (80.3):  61%|█▏| 396/651 [1:25:44<45:50, 10.79s/it]\u001b[A\n",
      "Average Metric: 318.0 / 397  (80.1):  61%|█▏| 396/651 [1:25:47<45:50, 10.79s/it]\u001b[A\n",
      "Average Metric: 318.0 / 397  (80.1):  61%|█▏| 397/651 [1:25:50<39:00,  9.21s/it]\u001b[A\n",
      "Average Metric: 319.0 / 398  (80.2):  61%|█▏| 397/651 [1:26:16<39:00,  9.21s/it]\u001b[A\n",
      "Average Metric: 319.0 / 398  (80.2):  61%|▌| 398/651 [1:26:16<1:01:02, 14.48s/it\u001b[A\n",
      "Average Metric: 320.0 / 399  (80.2):  61%|▌| 398/651 [1:26:39<1:01:02, 14.48s/it\u001b[A\n",
      "Average Metric: 320.0 / 399  (80.2):  61%|▌| 399/651 [1:26:39<1:11:45, 17.09s/it\u001b[A\n",
      "Average Metric: 321.0 / 400  (80.2):  61%|▌| 399/651 [1:26:43<1:11:45, 17.09s/it\u001b[A\n",
      "Average Metric: 321.0 / 400  (80.2):  61%|█▏| 400/651 [1:26:45<56:07, 13.42s/it]\u001b[A\n",
      "Average Metric: 322.0 / 401  (80.3):  61%|█▏| 400/651 [1:26:58<56:07, 13.42s/it]\u001b[A\n",
      "Average Metric: 322.0 / 401  (80.3):  62%|█▏| 401/651 [1:27:00<57:28, 13.79s/it]\u001b[A\n",
      "Average Metric: 323.0 / 402  (80.3):  62%|█▏| 401/651 [1:27:09<57:28, 13.79s/it]\u001b[A\n",
      "Average Metric: 323.0 / 402  (80.3):  62%|█▏| 402/651 [1:27:10<53:26, 12.88s/it]\u001b[A\n",
      "Average Metric: 323.0 / 403  (80.1):  62%|█▏| 402/651 [1:27:11<53:26, 12.88s/it]\u001b[A\n",
      "Average Metric: 323.0 / 403  (80.1):  62%|█▏| 403/651 [1:27:15<42:40, 10.33s/it]\u001b[A\n",
      "Average Metric: 323.0 / 404  (80.0):  62%|█▏| 403/651 [1:27:20<42:40, 10.33s/it]\u001b[A\n",
      "Average Metric: 323.0 / 404  (80.0):  62%|█▏| 404/651 [1:27:21<38:32,  9.36s/it]\u001b[A\n",
      "Average Metric: 324.0 / 405  (80.0):  62%|█▏| 404/651 [1:27:38<38:32,  9.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during API call: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 324.0 / 405  (80.0):  62%|█▏| 405/651 [1:27:40<49:05, 11.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t shapes (0,) and (1536,) not aligned: 0 (dim 0) != 1536 (dim 0)\n",
      "Error during API call: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t shapes (0,) and (1536,) not aligned: 0 (dim 0) != 1536 (dim 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 324.0 / 406  (79.8):  62%|█▏| 405/651 [1:27:58<49:05, 11.97s/it]\u001b[A\n",
      "Average Metric: 324.0 / 406  (79.8):  62%|█▏| 406/651 [1:27:59<57:52, 14.17s/it]\u001b[A\n",
      "Average Metric: 325.0 / 407  (79.9):  62%|█▏| 406/651 [1:28:08<57:52, 14.17s/it]\u001b[A\n",
      "Average Metric: 325.0 / 407  (79.9):  63%|█▎| 407/651 [1:28:11<53:45, 13.22s/it]\u001b[A\n",
      "Average Metric: 326.0 / 408  (79.9):  63%|█▎| 407/651 [1:28:19<53:45, 13.22s/it]\u001b[A\n",
      "Average Metric: 326.0 / 408  (79.9):  63%|█▎| 408/651 [1:28:21<50:32, 12.48s/it]\u001b[A\n",
      "Average Metric: 327.0 / 409  (80.0):  63%|█▎| 408/651 [1:28:46<50:32, 12.48s/it]\u001b[A\n",
      "Average Metric: 327.0 / 409  (80.0):  63%|▋| 409/651 [1:28:49<1:08:53, 17.08s/it\u001b[A\n",
      "Average Metric: 328.0 / 410  (80.0):  63%|▋| 409/651 [1:28:50<1:08:53, 17.08s/it\u001b[A\n",
      "Average Metric: 328.0 / 410  (80.0):  63%|█▎| 410/651 [1:28:53<52:27, 13.06s/it]\u001b[A\n",
      "Average Metric: 329.0 / 411  (80.0):  63%|█▎| 410/651 [1:28:57<52:27, 13.06s/it]\u001b[A\n",
      "Average Metric: 329.0 / 411  (80.0):  63%|█▎| 411/651 [1:29:02<44:58, 11.25s/it]\u001b[A\n",
      "Average Metric: 330.0 / 412  (80.1):  63%|█▎| 411/651 [1:29:16<44:58, 11.25s/it]\u001b[A\n",
      "Average Metric: 330.0 / 412  (80.1):  63%|█▎| 412/651 [1:29:19<54:00, 13.56s/it]\u001b[A\n",
      "Average Metric: 330.0 / 413  (79.9):  63%|█▎| 412/651 [1:29:45<54:00, 13.56s/it]\u001b[A\n",
      "Average Metric: 330.0 / 413  (79.9):  63%|▋| 413/651 [1:29:46<1:10:55, 17.88s/it\u001b[A\n",
      "Average Metric: 331.0 / 414  (80.0):  63%|▋| 413/651 [1:30:01<1:10:55, 17.88s/it\u001b[A\n",
      "Average Metric: 331.0 / 414  (80.0):  64%|▋| 414/651 [1:30:01<1:07:18, 17.04s/it\u001b[A\n",
      "Average Metric: 332.0 / 415  (80.0):  64%|▋| 414/651 [1:30:24<1:07:18, 17.04s/it\u001b[A\n",
      "Average Metric: 332.0 / 415  (80.0):  64%|▋| 415/651 [1:30:25<1:15:24, 19.17s/it\u001b[A\n",
      "Average Metric: 333.0 / 416  (80.0):  64%|▋| 415/651 [1:30:37<1:15:24, 19.17s/it\u001b[A\n",
      "Average Metric: 333.0 / 416  (80.0):  64%|▋| 416/651 [1:30:38<1:07:33, 17.25s/it\u001b[A\n",
      "Average Metric: 334.0 / 417  (80.1):  64%|▋| 416/651 [1:30:46<1:07:33, 17.25s/it\u001b[A\n",
      "Average Metric: 334.0 / 417  (80.1):  64%|█▎| 417/651 [1:30:47<57:35, 14.77s/it]\u001b[A\n",
      "Average Metric: 334.0 / 418  (79.9):  64%|█▎| 417/651 [1:30:49<57:35, 14.77s/it]\u001b[A\n",
      "Average Metric: 334.0 / 418  (79.9):  64%|█▎| 418/651 [1:30:52<44:46, 11.53s/it]\u001b[A\n",
      "Average Metric: 335.0 / 419  (80.0):  64%|█▎| 418/651 [1:30:59<44:46, 11.53s/it]\u001b[A\n",
      "Average Metric: 335.0 / 419  (80.0):  64%|█▎| 419/651 [1:31:01<42:10, 10.91s/it]\u001b[A\n",
      "Average Metric: 335.0 / 420  (79.8):  64%|█▎| 419/651 [1:31:16<42:10, 10.91s/it]\u001b[A\n",
      "Average Metric: 335.0 / 420  (79.8):  65%|█▎| 420/651 [1:31:18<49:28, 12.85s/it]\u001b[A\n",
      "Average Metric: 336.0 / 421  (79.8):  65%|█▎| 420/651 [1:31:40<49:28, 12.85s/it]\u001b[A\n",
      "Average Metric: 336.0 / 421  (79.8):  65%|▋| 421/651 [1:31:43<1:02:38, 16.34s/it\u001b[A\n",
      "Average Metric: 337.0 / 422  (79.9):  65%|▋| 421/651 [1:31:56<1:02:38, 16.34s/it\u001b[A\n",
      "Average Metric: 337.0 / 422  (79.9):  65%|▋| 422/651 [1:31:58<1:02:00, 16.25s/it\u001b[A\n",
      "Average Metric: 338.0 / 423  (79.9):  65%|▋| 422/651 [1:32:00<1:02:00, 16.25s/it\u001b[A\n",
      "Average Metric: 338.0 / 423  (79.9):  65%|█▎| 423/651 [1:32:01<46:17, 12.18s/it]\u001b[A\n",
      "Average Metric: 339.0 / 424  (80.0):  65%|█▎| 423/651 [1:32:37<46:17, 12.18s/it]\u001b[A\n",
      "Average Metric: 339.0 / 424  (80.0):  65%|▋| 424/651 [1:32:39<1:14:19, 19.64s/it\u001b[A\n",
      "Average Metric: 340.0 / 425  (80.0):  65%|▋| 424/651 [1:32:44<1:14:19, 19.64s/it\u001b[A\n",
      "Average Metric: 340.0 / 425  (80.0):  65%|▋| 425/651 [1:32:46<1:00:43, 16.12s/it\u001b[A\n",
      "Average Metric: 341.0 / 426  (80.0):  65%|▋| 425/651 [1:32:49<1:00:43, 16.12s/it\u001b[A\n",
      "Average Metric: 341.0 / 426  (80.0):  65%|█▎| 426/651 [1:32:51<48:01, 12.81s/it]\u001b[A\n",
      "Average Metric: 342.0 / 427  (80.1):  65%|█▎| 426/651 [1:32:55<48:01, 12.81s/it]\u001b[A\n",
      "Average Metric: 342.0 / 427  (80.1):  66%|█▎| 427/651 [1:32:56<38:54, 10.42s/it]\u001b[A\n",
      "Average Metric: 343.0 / 428  (80.1):  66%|█▎| 427/651 [1:33:01<38:54, 10.42s/it]\u001b[A\n",
      "Average Metric: 343.0 / 428  (80.1):  66%|█▎| 428/651 [1:33:03<34:37,  9.32s/it]\u001b[A\n",
      "Average Metric: 343.0 / 429  (80.0):  66%|█▎| 428/651 [1:33:09<34:37,  9.32s/it]\u001b[A\n",
      "Average Metric: 343.0 / 429  (80.0):  66%|█▎| 429/651 [1:33:12<33:47,  9.13s/it]\u001b[A\n",
      "Average Metric: 344.0 / 430  (80.0):  66%|█▎| 429/651 [1:33:31<33:47,  9.13s/it]\u001b[A\n",
      "Average Metric: 344.0 / 430  (80.0):  66%|█▎| 430/651 [1:33:32<46:14, 12.55s/it]\u001b[A\n",
      "Average Metric: 345.0 / 431  (80.0):  66%|█▎| 430/651 [1:34:20<46:14, 12.55s/it]\u001b[A\n",
      "Average Metric: 345.0 / 431  (80.0):  66%|▋| 431/651 [1:34:22<1:28:07, 24.03s/it\u001b[A\n",
      "Average Metric: 346.0 / 432  (80.1):  66%|▋| 431/651 [1:34:50<1:28:07, 24.03s/it\u001b[A\n",
      "Average Metric: 346.0 / 432  (80.1):  66%|▋| 432/651 [1:34:53<1:33:17, 25.56s/it\u001b[A\n",
      "Average Metric: 347.0 / 433  (80.1):  66%|▋| 432/651 [1:35:07<1:33:17, 25.56s/it\u001b[A\n",
      "Average Metric: 347.0 / 433  (80.1):  67%|▋| 433/651 [1:35:09<1:23:17, 22.92s/it\u001b[A\n",
      "Average Metric: 348.0 / 434  (80.2):  67%|▋| 433/651 [1:35:11<1:23:17, 22.92s/it\u001b[A\n",
      "Average Metric: 348.0 / 434  (80.2):  67%|▋| 434/651 [1:35:14<1:02:28, 17.27s/it\u001b[A\n",
      "Average Metric: 349.0 / 435  (80.2):  67%|▋| 434/651 [1:35:40<1:02:28, 17.27s/it\u001b[A\n",
      "Average Metric: 349.0 / 435  (80.2):  67%|▋| 435/651 [1:35:43<1:15:22, 20.94s/it\u001b[A\n",
      "Average Metric: 350.0 / 436  (80.3):  67%|▋| 435/651 [1:35:55<1:15:22, 20.94s/it\u001b[A\n",
      "Average Metric: 350.0 / 436  (80.3):  67%|▋| 436/651 [1:35:58<1:09:17, 19.34s/it\u001b[A\n",
      "Average Metric: 351.0 / 437  (80.3):  67%|▋| 436/651 [1:36:20<1:09:17, 19.34s/it\u001b[A\n",
      "Average Metric: 351.0 / 437  (80.3):  67%|▋| 437/651 [1:36:22<1:13:44, 20.68s/it\u001b[A\n",
      "Average Metric: 352.0 / 438  (80.4):  67%|▋| 437/651 [1:36:45<1:13:44, 20.68s/it\u001b[A\n",
      "Average Metric: 352.0 / 438  (80.4):  67%|▋| 438/651 [1:36:47<1:18:11, 22.03s/it\u001b[A\n",
      "Average Metric: 353.0 / 439  (80.4):  67%|▋| 438/651 [1:36:53<1:18:11, 22.03s/it\u001b[A\n",
      "Average Metric: 353.0 / 439  (80.4):  67%|▋| 439/651 [1:36:56<1:04:02, 18.12s/it\u001b[A\n",
      "Average Metric: 354.0 / 440  (80.5):  67%|▋| 439/651 [1:36:57<1:04:02, 18.12s/it\u001b[A\n",
      "Average Metric: 354.0 / 440  (80.5):  68%|█▎| 440/651 [1:36:59<47:02, 13.38s/it]\u001b[A\n",
      "Average Metric: 355.0 / 441  (80.5):  68%|█▎| 440/651 [1:37:33<47:02, 13.38s/it]\u001b[A\n",
      "Average Metric: 355.0 / 441  (80.5):  68%|▋| 441/651 [1:37:34<1:10:38, 20.18s/it\u001b[A\n",
      "Average Metric: 356.0 / 442  (80.5):  68%|▋| 441/651 [1:37:46<1:10:38, 20.18s/it\u001b[A\n",
      "Average Metric: 356.0 / 442  (80.5):  68%|▋| 442/651 [1:37:46<1:01:54, 17.77s/it\u001b[A\n",
      "Average Metric: 357.0 / 443  (80.6):  68%|▋| 442/651 [1:37:50<1:01:54, 17.77s/it\u001b[A\n",
      "Average Metric: 357.0 / 443  (80.6):  68%|█▎| 443/651 [1:37:53<49:08, 14.18s/it]\u001b[A\n",
      "Average Metric: 358.0 / 444  (80.6):  68%|█▎| 443/651 [1:38:07<49:08, 14.18s/it]\u001b[A\n",
      "Average Metric: 358.0 / 444  (80.6):  68%|█▎| 444/651 [1:38:09<51:01, 14.79s/it]\u001b[A\n",
      "Average Metric: 359.0 / 445  (80.7):  68%|█▎| 444/651 [1:38:25<51:01, 14.79s/it]\u001b[A\n",
      "Average Metric: 359.0 / 445  (80.7):  68%|█▎| 445/651 [1:38:27<53:31, 15.59s/it]\u001b[A\n",
      "Average Metric: 360.0 / 446  (80.7):  68%|█▎| 445/651 [1:38:29<53:31, 15.59s/it]\u001b[A\n",
      "Average Metric: 360.0 / 446  (80.7):  69%|█▎| 446/651 [1:38:30<41:30, 12.15s/it]\u001b[A\n",
      "Average Metric: 361.0 / 447  (80.8):  69%|█▎| 446/651 [1:38:48<41:30, 12.15s/it]\u001b[A\n",
      "Average Metric: 361.0 / 447  (80.8):  69%|█▎| 447/651 [1:38:48<47:27, 13.96s/it]\u001b[A\n",
      "Average Metric: 362.0 / 448  (80.8):  69%|█▎| 447/651 [1:38:55<47:27, 13.96s/it]\u001b[A\n",
      "Average Metric: 362.0 / 448  (80.8):  69%|█▍| 448/651 [1:38:56<40:51, 12.08s/it]\u001b[A\n",
      "Average Metric: 363.0 / 449  (80.8):  69%|█▍| 448/651 [1:39:23<40:51, 12.08s/it]\u001b[A\n",
      "Average Metric: 363.0 / 449  (80.8):  69%|█▍| 449/651 [1:39:23<56:13, 16.70s/it]\u001b[A\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /SymbolSetEncoding not implemented yet\n",
      "  warnings.warn(\n",
      "Average Metric: 363.0 / 450  (80.7):  69%|█▍| 449/651 [1:39:25<56:13, 16.70s/it]\u001b[A\n",
      "Average Metric: 363.0 / 450  (80.7):  69%|█▍| 450/651 [1:39:27<42:36, 12.72s/it]\u001b[A\n",
      "Average Metric: 363.0 / 451  (80.5):  69%|█▍| 450/651 [1:39:54<42:36, 12.72s/it]\u001b[A\n",
      "Average Metric: 363.0 / 451  (80.5):  69%|█▍| 451/651 [1:39:54<57:30, 17.25s/it]\u001b[A\n",
      "Average Metric: 364.0 / 452  (80.5):  69%|█▍| 451/651 [1:39:55<57:30, 17.25s/it]\u001b[A\n",
      "Average Metric: 364.0 / 452  (80.5):  69%|█▍| 452/651 [1:39:55<41:05, 12.39s/it]\u001b[A\n",
      "Average Metric: 365.0 / 453  (80.6):  69%|█▍| 452/651 [1:40:00<41:05, 12.39s/it]\u001b[A\n",
      "Average Metric: 365.0 / 453  (80.6):  70%|█▍| 453/651 [1:40:02<35:17, 10.70s/it]\u001b[A\n",
      "Average Metric: 365.0 / 454  (80.4):  70%|█▍| 453/651 [1:40:04<35:17, 10.70s/it]\u001b[A\n",
      "Average Metric: 365.0 / 454  (80.4):  70%|█▍| 454/651 [1:40:05<26:52,  8.18s/it]\u001b[A\n",
      "Average Metric: 365.0 / 455  (80.2):  70%|█▍| 454/651 [1:40:07<26:52,  8.18s/it]\u001b[A\n",
      "Average Metric: 365.0 / 455  (80.2):  70%|█▍| 455/651 [1:40:09<22:26,  6.87s/it]\u001b[A\n",
      "Average Metric: 366.0 / 456  (80.3):  70%|█▍| 455/651 [1:40:14<22:26,  6.87s/it]\u001b[A\n",
      "Average Metric: 366.0 / 456  (80.3):  70%|█▍| 456/651 [1:40:16<22:26,  6.91s/it]\u001b[A\n",
      "Average Metric: 367.0 / 457  (80.3):  70%|█▍| 456/651 [1:40:16<22:26,  6.91s/it]\u001b[A\n",
      "Average Metric: 367.0 / 457  (80.3):  70%|█▍| 457/651 [1:40:18<17:52,  5.53s/it]\u001b[A\n",
      "Average Metric: 368.0 / 458  (80.3):  70%|█▍| 457/651 [1:40:21<17:52,  5.53s/it]\u001b[A\n",
      "Average Metric: 368.0 / 458  (80.3):  70%|█▍| 458/651 [1:40:23<16:54,  5.26s/it]\u001b[A\n",
      "Average Metric: 369.0 / 459  (80.4):  70%|█▍| 458/651 [1:40:27<16:54,  5.26s/it]\u001b[A\n",
      "Average Metric: 369.0 / 459  (80.4):  71%|█▍| 459/651 [1:40:28<17:02,  5.32s/it]\u001b[A\n",
      "Average Metric: 370.0 / 460  (80.4):  71%|█▍| 459/651 [1:40:30<17:02,  5.32s/it]\u001b[A\n",
      "Average Metric: 370.0 / 460  (80.4):  71%|█▍| 460/651 [1:40:32<15:46,  4.96s/it]\u001b[A\n",
      "Average Metric: 371.0 / 461  (80.5):  71%|█▍| 460/651 [1:40:36<15:46,  4.96s/it]\u001b[A\n",
      "Average Metric: 371.0 / 461  (80.5):  71%|█▍| 461/651 [1:40:40<18:50,  5.95s/it]\u001b[A\n",
      "Average Metric: 372.0 / 462  (80.5):  71%|█▍| 461/651 [1:40:43<18:50,  5.95s/it]\u001b[A\n",
      "Average Metric: 372.0 / 462  (80.5):  71%|█▍| 462/651 [1:40:47<17:43,  5.63s/it]\u001b[A\n",
      "Average Metric: 373.0 / 463  (80.6):  71%|█▍| 462/651 [1:40:49<17:43,  5.63s/it]\u001b[A\n",
      "Average Metric: 373.0 / 463  (80.6):  71%|█▍| 463/651 [1:40:51<17:56,  5.73s/it]\u001b[A\n",
      "Average Metric: 374.0 / 464  (80.6):  71%|█▍| 463/651 [1:40:54<17:56,  5.73s/it]\u001b[A\n",
      "Average Metric: 374.0 / 464  (80.6):  71%|█▍| 464/651 [1:40:58<18:30,  5.94s/it]\u001b[A\n",
      "Average Metric: 374.0 / 465  (80.4):  71%|█▍| 464/651 [1:41:51<18:30,  5.94s/it]\u001b[A\n",
      "Average Metric: 374.0 / 465  (80.4):  71%|▋| 465/651 [1:41:51<1:03:21, 20.44s/it\u001b[A\n",
      "Average Metric: 375.0 / 466  (80.5):  71%|▋| 465/651 [1:42:04<1:03:21, 20.44s/it\u001b[A\n",
      "Average Metric: 375.0 / 466  (80.5):  72%|█▍| 466/651 [1:42:07<57:43, 18.72s/it]\u001b[A\n",
      "Average Metric: 376.0 / 467  (80.5):  72%|█▍| 466/651 [1:42:19<57:43, 18.72s/it]\u001b[A\n",
      "Average Metric: 376.0 / 467  (80.5):  72%|█▍| 467/651 [1:42:20<52:15, 17.04s/it]\u001b[A\n",
      "Average Metric: 377.0 / 468  (80.6):  72%|█▍| 467/651 [1:42:23<52:15, 17.04s/it]\u001b[A\n",
      "Average Metric: 377.0 / 468  (80.6):  72%|█▍| 468/651 [1:42:25<40:31, 13.29s/it]\u001b[A\n",
      "Average Metric: 378.0 / 469  (80.6):  72%|█▍| 468/651 [1:42:26<40:31, 13.29s/it]\u001b[A\n",
      "Average Metric: 378.0 / 469  (80.6):  72%|█▍| 469/651 [1:42:28<31:34, 10.41s/it]\u001b[A\n",
      "Average Metric: 379.0 / 470  (80.6):  72%|█▍| 469/651 [1:42:29<31:34, 10.41s/it]\u001b[A\n",
      "Average Metric: 379.0 / 470  (80.6):  72%|█▍| 470/651 [1:42:32<25:09,  8.34s/it]\u001b[A\n",
      "Average Metric: 380.0 / 471  (80.7):  72%|█▍| 470/651 [1:42:35<25:09,  8.34s/it]\u001b[A\n",
      "Average Metric: 380.0 / 471  (80.7):  72%|█▍| 471/651 [1:42:37<22:18,  7.44s/it]\u001b[A\n",
      "Average Metric: 380.0 / 472  (80.5):  72%|█▍| 471/651 [1:42:40<22:18,  7.44s/it]\u001b[A\n",
      "Average Metric: 380.0 / 472  (80.5):  73%|█▍| 472/651 [1:42:42<19:24,  6.50s/it]\u001b[A\n",
      "Average Metric: 381.0 / 473  (80.5):  73%|█▍| 472/651 [1:43:32<19:24,  6.50s/it]\u001b[A\n",
      "Average Metric: 381.0 / 473  (80.5):  73%|█▍| 473/651 [1:43:32<58:58, 19.88s/it]\u001b[A\n",
      "Average Metric: 382.0 / 474  (80.6):  73%|█▍| 473/651 [1:43:33<58:58, 19.88s/it]\u001b[A\n",
      "Average Metric: 382.0 / 474  (80.6):  73%|█▍| 474/651 [1:43:36<43:51, 14.87s/it]\u001b[A\n",
      "Average Metric: 383.0 / 475  (80.6):  73%|█▍| 474/651 [1:44:06<43:51, 14.87s/it]\u001b[A\n",
      "Average Metric: 383.0 / 475  (80.6):  73%|█▍| 475/651 [1:44:07<58:12, 19.84s/it]\u001b[A\n",
      "Average Metric: 384.0 / 476  (80.7):  73%|█▍| 475/651 [1:44:14<58:12, 19.84s/it]\u001b[A\n",
      "Average Metric: 384.0 / 476  (80.7):  73%|█▍| 476/651 [1:44:16<48:36, 16.66s/it]\u001b[A\n",
      "Average Metric: 385.0 / 477  (80.7):  73%|█▍| 476/651 [1:44:19<48:36, 16.66s/it]\u001b[A\n",
      "Average Metric: 385.0 / 477  (80.7):  73%|█▍| 477/651 [1:44:20<36:57, 12.74s/it]\u001b[A\n",
      "Average Metric: 386.0 / 478  (80.8):  73%|█▍| 477/651 [1:44:27<36:57, 12.74s/it]\u001b[A\n",
      "Average Metric: 386.0 / 478  (80.8):  73%|█▍| 478/651 [1:44:30<33:40, 11.68s/it]\u001b[A\n",
      "Average Metric: 387.0 / 479  (80.8):  73%|█▍| 478/651 [1:44:31<33:40, 11.68s/it]\u001b[A\n",
      "Average Metric: 387.0 / 479  (80.8):  74%|█▍| 479/651 [1:44:33<26:41,  9.31s/it]\u001b[A\n",
      "Average Metric: 388.0 / 480  (80.8):  74%|█▍| 479/651 [1:44:35<26:41,  9.31s/it]\u001b[A\n",
      "Average Metric: 388.0 / 480  (80.8):  74%|█▍| 480/651 [1:44:39<22:54,  8.04s/it]\u001b[A\n",
      "Average Metric: 389.0 / 481  (80.9):  74%|█▍| 480/651 [1:45:11<22:54,  8.04s/it]\u001b[A\n",
      "Average Metric: 389.0 / 481  (80.9):  74%|█▍| 481/651 [1:45:11<44:33, 15.73s/it]\u001b[A\n",
      "Average Metric: 390.0 / 482  (80.9):  74%|█▍| 481/651 [1:45:29<44:33, 15.73s/it]\u001b[A\n",
      "Average Metric: 390.0 / 482  (80.9):  74%|█▍| 482/651 [1:45:31<47:08, 16.74s/it]\u001b[A\n",
      "Average Metric: 391.0 / 483  (81.0):  74%|█▍| 482/651 [1:45:48<47:08, 16.74s/it]\u001b[A\n",
      "Average Metric: 391.0 / 483  (81.0):  74%|█▍| 483/651 [1:45:49<48:59, 17.50s/it]\u001b[A\n",
      "Average Metric: 391.0 / 484  (80.8):  74%|█▍| 483/651 [1:45:59<48:59, 17.50s/it]\u001b[A\n",
      "Average Metric: 391.0 / 484  (80.8):  74%|█▍| 484/651 [1:46:01<43:17, 15.56s/it]\u001b[A\n",
      "Average Metric: 392.0 / 485  (80.8):  74%|█▍| 484/651 [1:46:03<43:17, 15.56s/it]\u001b[A\n",
      "Average Metric: 392.0 / 485  (80.8):  75%|█▍| 485/651 [1:46:08<34:39, 12.53s/it]\u001b[A\n",
      "Average Metric: 393.0 / 486  (80.9):  75%|█▍| 485/651 [1:46:19<34:39, 12.53s/it]\u001b[A\n",
      "Average Metric: 393.0 / 486  (80.9):  75%|█▍| 486/651 [1:46:19<35:24, 12.87s/it]\u001b[A\n",
      "Average Metric: 394.0 / 487  (80.9):  75%|█▍| 486/651 [1:46:30<35:24, 12.87s/it]\u001b[A\n",
      "Average Metric: 394.0 / 487  (80.9):  75%|█▍| 487/651 [1:46:31<33:43, 12.34s/it]\u001b[A\n",
      "Average Metric: 395.0 / 488  (80.9):  75%|█▍| 487/651 [1:46:48<33:43, 12.34s/it]\u001b[A\n",
      "Average Metric: 395.0 / 488  (80.9):  75%|█▍| 488/651 [1:46:49<37:58, 13.98s/it]\u001b[A\n",
      "Average Metric: 396.0 / 489  (81.0):  75%|█▍| 488/651 [1:47:01<37:58, 13.98s/it]\u001b[A\n",
      "Average Metric: 396.0 / 489  (81.0):  75%|█▌| 489/651 [1:47:03<37:03, 13.73s/it]\u001b[A\n",
      "Average Metric: 396.0 / 490  (80.8):  75%|█▌| 489/651 [1:47:11<37:03, 13.73s/it]\u001b[A\n",
      "Average Metric: 396.0 / 490  (80.8):  75%|█▌| 490/651 [1:47:12<34:03, 12.69s/it]\u001b[A\n",
      "Average Metric: 396.0 / 491  (80.7):  75%|█▌| 490/651 [1:47:26<34:03, 12.69s/it]\u001b[A\n",
      "Average Metric: 396.0 / 491  (80.7):  75%|█▌| 491/651 [1:47:27<35:31, 13.32s/it]\u001b[A\n",
      "Average Metric: 397.0 / 492  (80.7):  75%|█▌| 491/651 [1:47:41<35:31, 13.32s/it]\u001b[A\n",
      "Average Metric: 397.0 / 492  (80.7):  76%|█▌| 492/651 [1:47:42<36:53, 13.92s/it]\u001b[A\n",
      "Average Metric: 398.0 / 493  (80.7):  76%|█▌| 492/651 [1:47:54<36:53, 13.92s/it]\u001b[A\n",
      "Average Metric: 398.0 / 493  (80.7):  76%|█▌| 493/651 [1:47:56<36:19, 13.80s/it]\u001b[A\n",
      "Average Metric: 399.0 / 494  (80.8):  76%|█▌| 493/651 [1:47:58<36:19, 13.80s/it]\u001b[A\n",
      "Average Metric: 399.0 / 494  (80.8):  76%|█▌| 494/651 [1:48:00<28:44, 10.98s/it]\u001b[A\n",
      "Average Metric: 399.0 / 495  (80.6):  76%|█▌| 494/651 [1:48:18<28:44, 10.98s/it]\u001b[A\n",
      "Average Metric: 399.0 / 495  (80.6):  76%|█▌| 495/651 [1:48:21<35:40, 13.72s/it]\u001b[A\n",
      "Average Metric: 399.0 / 496  (80.4):  76%|█▌| 495/651 [1:48:31<35:40, 13.72s/it]\u001b[A\n",
      "Average Metric: 399.0 / 496  (80.4):  76%|█▌| 496/651 [1:48:33<34:30, 13.36s/it]\u001b[A\n",
      "Average Metric: 400.0 / 497  (80.5):  76%|█▌| 496/651 [1:48:43<34:30, 13.36s/it]\u001b[A\n",
      "Average Metric: 400.0 / 497  (80.5):  76%|█▌| 497/651 [1:48:45<33:03, 12.88s/it]\u001b[A\n",
      "Average Metric: 400.0 / 498  (80.3):  76%|█▌| 497/651 [1:48:51<33:03, 12.88s/it]\u001b[A\n",
      "Average Metric: 400.0 / 498  (80.3):  76%|█▌| 498/651 [1:48:52<28:53, 11.33s/it]\u001b[A\n",
      "Average Metric: 401.0 / 499  (80.4):  76%|█▌| 498/651 [1:49:15<28:53, 11.33s/it]\u001b[A\n",
      "Average Metric: 401.0 / 499  (80.4):  77%|█▌| 499/651 [1:49:18<39:09, 15.46s/it]\u001b[A\n",
      "Average Metric: 402.0 / 500  (80.4):  77%|█▌| 499/651 [1:49:45<39:09, 15.46s/it]\u001b[A\n",
      "Average Metric: 402.0 / 500  (80.4):  77%|█▌| 500/651 [1:49:46<48:41, 19.35s/it]\u001b[A\n",
      "Average Metric: 403.0 / 501  (80.4):  77%|█▌| 500/651 [1:49:50<48:41, 19.35s/it]\u001b[A\n",
      "Average Metric: 403.0 / 501  (80.4):  77%|█▌| 501/651 [1:49:53<39:22, 15.75s/it]\u001b[A\n",
      "Average Metric: 403.0 / 502  (80.3):  77%|█▌| 501/651 [1:50:07<39:22, 15.75s/it]\u001b[A\n",
      "Average Metric: 403.0 / 502  (80.3):  77%|█▌| 502/651 [1:50:10<39:19, 15.83s/it]\u001b[A\n",
      "Average Metric: 403.0 / 503  (80.1):  77%|█▌| 502/651 [1:50:30<39:19, 15.83s/it]\u001b[A\n",
      "Average Metric: 403.0 / 503  (80.1):  77%|█▌| 503/651 [1:50:31<43:27, 17.62s/it]\u001b[A\n",
      "Average Metric: 404.0 / 504  (80.2):  77%|█▌| 503/651 [1:50:44<43:27, 17.62s/it]\u001b[A\n",
      "Average Metric: 404.0 / 504  (80.2):  77%|█▌| 504/651 [1:50:45<40:29, 16.53s/it]\u001b[A\n",
      "Average Metric: 405.0 / 505  (80.2):  77%|█▌| 504/651 [1:51:01<40:29, 16.53s/it]\u001b[A\n",
      "Average Metric: 405.0 / 505  (80.2):  78%|█▌| 505/651 [1:51:01<40:06, 16.49s/it]\u001b[A\n",
      "Average Metric: 406.0 / 506  (80.2):  78%|█▌| 505/651 [1:51:27<40:06, 16.49s/it]\u001b[A\n",
      "Average Metric: 406.0 / 506  (80.2):  78%|█▌| 506/651 [1:51:29<47:28, 19.64s/it]\u001b[A\n",
      "Average Metric: 407.0 / 507  (80.3):  78%|█▌| 506/651 [1:51:30<47:28, 19.64s/it]\u001b[A\n",
      "Average Metric: 407.0 / 507  (80.3):  78%|█▌| 507/651 [1:51:33<35:36, 14.84s/it]\u001b[A\n",
      "Average Metric: 408.0 / 508  (80.3):  78%|█▌| 507/651 [1:51:49<35:36, 14.84s/it]\u001b[A\n",
      "Average Metric: 408.0 / 508  (80.3):  78%|█▌| 508/651 [1:51:52<38:10, 16.02s/it]\u001b[A\n",
      "Average Metric: 409.0 / 509  (80.4):  78%|█▌| 508/651 [1:52:02<38:10, 16.02s/it]\u001b[A\n",
      "Average Metric: 409.0 / 509  (80.4):  78%|█▌| 509/651 [1:52:04<35:33, 15.03s/it]\u001b[A\n",
      "Average Metric: 410.0 / 510  (80.4):  78%|█▌| 509/651 [1:52:20<35:33, 15.03s/it]\u001b[A\n",
      "Average Metric: 410.0 / 510  (80.4):  78%|█▌| 510/651 [1:52:23<38:16, 16.29s/it]\u001b[A\n",
      "Average Metric: 411.0 / 511  (80.4):  78%|█▌| 510/651 [1:52:45<38:16, 16.29s/it]\u001b[A\n",
      "Average Metric: 411.0 / 511  (80.4):  78%|█▌| 511/651 [1:52:46<42:39, 18.29s/it]\u001b[A\n",
      "Average Metric: 412.0 / 512  (80.5):  78%|█▌| 511/651 [1:53:07<42:39, 18.29s/it]\u001b[A\n",
      "Average Metric: 412.0 / 512  (80.5):  79%|█▌| 512/651 [1:53:09<45:43, 19.74s/it]\u001b[A\n",
      "Average Metric: 413.0 / 513  (80.5):  79%|█▌| 512/651 [1:53:11<45:43, 19.74s/it]\u001b[A\n",
      "Average Metric: 413.0 / 513  (80.5):  79%|█▌| 513/651 [1:53:14<35:12, 15.31s/it]\u001b[A\n",
      "Average Metric: 414.0 / 514  (80.5):  79%|█▌| 513/651 [1:53:24<35:12, 15.31s/it]\u001b[A\n",
      "Average Metric: 414.0 / 514  (80.5):  79%|█▌| 514/651 [1:53:26<32:20, 14.16s/it]\u001b[A\n",
      "Average Metric: 415.0 / 515  (80.6):  79%|█▌| 514/651 [1:53:29<32:20, 14.16s/it]\u001b[A\n",
      "Average Metric: 415.0 / 515  (80.6):  79%|█▌| 515/651 [1:53:32<26:57, 11.89s/it]\u001b[A\n",
      "Average Metric: 416.0 / 516  (80.6):  79%|█▌| 515/651 [1:53:48<26:57, 11.89s/it]\u001b[A\n",
      "Average Metric: 416.0 / 516  (80.6):  79%|█▌| 516/651 [1:53:51<31:06, 13.83s/it]\u001b[A\n",
      "Average Metric: 417.0 / 517  (80.7):  79%|█▌| 516/651 [1:53:53<31:06, 13.83s/it]\u001b[A\n",
      "Average Metric: 417.0 / 517  (80.7):  79%|█▌| 517/651 [1:53:54<24:00, 10.75s/it]\u001b[A\n",
      "Average Metric: 418.0 / 518  (80.7):  79%|█▌| 517/651 [1:54:29<24:00, 10.75s/it]\u001b[A\n",
      "Average Metric: 418.0 / 518  (80.7):  80%|█▌| 518/651 [1:54:32<41:26, 18.69s/it]\u001b[A\n",
      "Average Metric: 419.0 / 519  (80.7):  80%|█▌| 518/651 [1:55:07<41:26, 18.69s/it]\u001b[A\n",
      "Average Metric: 419.0 / 519  (80.7):  80%|█▌| 519/651 [1:55:09<53:41, 24.41s/it]\u001b[A\n",
      "Average Metric: 420.0 / 520  (80.8):  80%|█▌| 519/651 [1:55:29<53:41, 24.41s/it]\u001b[A\n",
      "Average Metric: 420.0 / 520  (80.8):  80%|█▌| 520/651 [1:55:32<52:25, 24.01s/it]\u001b[A\n",
      "Average Metric: 421.0 / 521  (80.8):  80%|█▌| 520/651 [1:55:36<52:25, 24.01s/it]\u001b[A\n",
      "Average Metric: 421.0 / 521  (80.8):  80%|█▌| 521/651 [1:55:39<40:10, 18.54s/it]\u001b[A\n",
      "Average Metric: 422.0 / 522  (80.8):  80%|█▌| 521/651 [1:55:47<40:10, 18.54s/it]\u001b[A\n",
      "Average Metric: 422.0 / 522  (80.8):  80%|█▌| 522/651 [1:55:49<34:47, 16.18s/it]\u001b[A\n",
      "Average Metric: 423.0 / 523  (80.9):  80%|█▌| 522/651 [1:55:53<34:47, 16.18s/it]\u001b[A\n",
      "Average Metric: 423.0 / 523  (80.9):  80%|█▌| 523/651 [1:55:57<29:11, 13.68s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.70s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 424.0 / 524  (80.9):  80%|█▌| 523/651 [1:56:23<29:11, 13.68s/it]\u001b[A\n",
      "Average Metric: 424.0 / 524  (80.9):  80%|█▌| 524/651 [1:56:26<38:19, 18.11s/it]\u001b[A\n",
      "Average Metric: 425.0 / 525  (81.0):  80%|█▌| 524/651 [1:56:29<38:19, 18.11s/it]\u001b[A\n",
      "Average Metric: 425.0 / 525  (81.0):  81%|█▌| 525/651 [1:56:32<30:50, 14.69s/it]\u001b[A\n",
      "Average Metric: 426.0 / 526  (81.0):  81%|█▌| 525/651 [1:56:41<30:50, 14.69s/it]\u001b[A\n",
      "Average Metric: 426.0 / 526  (81.0):  81%|█▌| 526/651 [1:56:45<29:38, 14.23s/it]\u001b[A\n",
      "Average Metric: 426.0 / 527  (80.8):  81%|█▌| 526/651 [1:58:03<29:38, 14.23s/it]\u001b[A\n",
      "Average Metric: 426.0 / 527  (80.8):  81%|▊| 527/651 [1:58:06<1:10:30, 34.12s/it\u001b[A\n",
      "Average Metric: 426.0 / 528  (80.7):  81%|▊| 527/651 [1:59:39<1:10:30, 34.12s/it\u001b[A\n",
      "Average Metric: 426.0 / 528  (80.7):  81%|▊| 528/651 [1:59:40<1:47:25, 52.41s/it\u001b[A\n",
      "Average Metric: 427.0 / 529  (80.7):  81%|▊| 528/651 [1:59:56<1:47:25, 52.41s/it\u001b[A\n",
      "Average Metric: 427.0 / 529  (80.7):  81%|▊| 529/651 [1:59:58<1:25:02, 41.83s/it\u001b[A\n",
      "Average Metric: 428.0 / 530  (80.8):  81%|▊| 529/651 [2:00:20<1:25:02, 41.83s/it\u001b[A\n",
      "Average Metric: 428.0 / 530  (80.8):  81%|▊| 530/651 [2:00:22<1:13:44, 36.57s/it\u001b[A\n",
      "Average Metric: 429.0 / 531  (80.8):  81%|▊| 530/651 [2:00:33<1:13:44, 36.57s/it\u001b[A\n",
      "Average Metric: 429.0 / 531  (80.8):  82%|█▋| 531/651 [2:00:35<59:22, 29.69s/it]\u001b[A\n",
      "Average Metric: 430.0 / 532  (80.8):  82%|█▋| 531/651 [2:00:37<59:22, 29.69s/it]\u001b[A\n",
      "Average Metric: 430.0 / 532  (80.8):  82%|█▋| 532/651 [2:00:38<43:12, 21.79s/it]\u001b[A\n",
      "Average Metric: 430.0 / 533  (80.7):  82%|█▋| 532/651 [2:01:01<43:12, 21.79s/it]\u001b[A\n",
      "Average Metric: 430.0 / 533  (80.7):  82%|█▋| 533/651 [2:01:03<44:25, 22.59s/it]\u001b[A\n",
      "Average Metric: 431.0 / 534  (80.7):  82%|█▋| 533/651 [2:01:04<44:25, 22.59s/it]\u001b[A\n",
      "Average Metric: 431.0 / 534  (80.7):  82%|█▋| 534/651 [2:01:06<32:30, 16.67s/it]\u001b[A\n",
      "Average Metric: 432.0 / 535  (80.7):  82%|█▋| 534/651 [2:01:25<32:30, 16.67s/it]\u001b[A\n",
      "Average Metric: 432.0 / 535  (80.7):  82%|█▋| 535/651 [2:01:27<34:36, 17.90s/it]\u001b[A\n",
      "Average Metric: 432.0 / 536  (80.6):  82%|█▋| 535/651 [2:01:32<34:36, 17.90s/it]\u001b[A\n",
      "Average Metric: 432.0 / 536  (80.6):  82%|█▋| 536/651 [2:01:33<27:57, 14.59s/it]\u001b[A\n",
      "Average Metric: 433.0 / 537  (80.6):  82%|█▋| 536/651 [2:01:50<27:57, 14.59s/it]\u001b[A\n",
      "Average Metric: 433.0 / 537  (80.6):  82%|█▋| 537/651 [2:01:51<29:34, 15.57s/it]\u001b[A\n",
      "Average Metric: 433.0 / 538  (80.5):  82%|█▋| 537/651 [2:02:05<29:34, 15.57s/it]\u001b[A\n",
      "Average Metric: 433.0 / 538  (80.5):  83%|█▋| 538/651 [2:02:07<29:16, 15.55s/it]\u001b[A\n",
      "Average Metric: 434.0 / 539  (80.5):  83%|█▋| 538/651 [2:02:08<29:16, 15.55s/it]\u001b[A\n",
      "Average Metric: 434.0 / 539  (80.5):  83%|█▋| 539/651 [2:02:09<21:28, 11.50s/it]\u001b[A\n",
      "Average Metric: 435.0 / 540  (80.6):  83%|█▋| 539/651 [2:02:28<21:28, 11.50s/it]\u001b[A\n",
      "Average Metric: 435.0 / 540  (80.6):  83%|█▋| 540/651 [2:02:31<27:24, 14.81s/it]\u001b[A\n",
      "Average Metric: 436.0 / 541  (80.6):  83%|█▋| 540/651 [2:02:33<27:24, 14.81s/it]\u001b[A\n",
      "Average Metric: 436.0 / 541  (80.6):  83%|█▋| 541/651 [2:02:35<20:32, 11.20s/it]\u001b[A\n",
      "Average Metric: 437.0 / 542  (80.6):  83%|█▋| 541/651 [2:02:58<20:32, 11.20s/it]\u001b[A\n",
      "Average Metric: 437.0 / 542  (80.6):  83%|█▋| 542/651 [2:02:59<27:55, 15.37s/it]\u001b[A\n",
      "Average Metric: 438.0 / 543  (80.7):  83%|█▋| 542/651 [2:03:04<27:55, 15.37s/it]\u001b[A\n",
      "Average Metric: 438.0 / 543  (80.7):  83%|█▋| 543/651 [2:03:06<23:00, 12.78s/it]\u001b[A\n",
      "Average Metric: 439.0 / 544  (80.7):  83%|█▋| 543/651 [2:03:20<23:00, 12.78s/it]\u001b[A\n",
      "Average Metric: 439.0 / 544  (80.7):  84%|█▋| 544/651 [2:03:22<24:09, 13.54s/it]\u001b[A\n",
      "Average Metric: 439.0 / 545  (80.6):  84%|█▋| 544/651 [2:03:57<24:09, 13.54s/it]\u001b[A\n",
      "Average Metric: 439.0 / 545  (80.6):  84%|█▋| 545/651 [2:03:59<36:16, 20.53s/it]\u001b[A\n",
      "Average Metric: 440.0 / 546  (80.6):  84%|█▋| 545/651 [2:04:01<36:16, 20.53s/it]\u001b[A\n",
      "Average Metric: 440.0 / 546  (80.6):  84%|█▋| 546/651 [2:04:03<27:57, 15.98s/it]\u001b[A\n",
      "Average Metric: 441.0 / 547  (80.6):  84%|█▋| 546/651 [2:04:21<27:57, 15.98s/it]\u001b[A\n",
      "Average Metric: 441.0 / 547  (80.6):  84%|█▋| 547/651 [2:04:23<29:21, 16.94s/it]\u001b[A\n",
      "Average Metric: 441.0 / 548  (80.5):  84%|█▋| 547/651 [2:04:26<29:21, 16.94s/it]\u001b[A\n",
      "Average Metric: 441.0 / 548  (80.5):  84%|█▋| 548/651 [2:04:26<22:24, 13.05s/it]\u001b[A\n",
      "Average Metric: 442.0 / 549  (80.5):  84%|█▋| 548/651 [2:04:46<22:24, 13.05s/it]\u001b[A\n",
      "Average Metric: 442.0 / 549  (80.5):  84%|█▋| 549/651 [2:04:48<26:18, 15.47s/it]\u001b[A\n",
      "Average Metric: 443.0 / 550  (80.5):  84%|█▋| 549/651 [2:04:49<26:18, 15.47s/it]\u001b[A\n",
      "Average Metric: 443.0 / 550  (80.5):  84%|█▋| 550/651 [2:04:50<19:13, 11.42s/it]\u001b[A\n",
      "Average Metric: 444.0 / 551  (80.6):  84%|█▋| 550/651 [2:05:03<19:13, 11.42s/it]\u001b[A\n",
      "Average Metric: 444.0 / 551  (80.6):  85%|█▋| 551/651 [2:05:04<20:33, 12.33s/it]\u001b[A\n",
      "Average Metric: 444.0 / 552  (80.4):  85%|█▋| 551/651 [2:05:13<20:33, 12.33s/it]\u001b[A\n",
      "Average Metric: 444.0 / 552  (80.4):  85%|█▋| 552/651 [2:05:14<19:31, 11.83s/it]\u001b[A\n",
      "Average Metric: 445.0 / 553  (80.5):  85%|█▋| 552/651 [2:05:46<19:31, 11.83s/it]\u001b[A\n",
      "Average Metric: 445.0 / 553  (80.5):  85%|█▋| 553/651 [2:05:47<29:15, 17.92s/it]\u001b[A\n",
      "Average Metric: 446.0 / 554  (80.5):  85%|█▋| 553/651 [2:05:47<29:15, 17.92s/it]\u001b[A\n",
      "Average Metric: 446.0 / 554  (80.5):  85%|█▋| 554/651 [2:05:47<20:38, 12.77s/it]\u001b[A\n",
      "Average Metric: 447.0 / 555  (80.5):  85%|█▋| 554/651 [2:05:57<20:38, 12.77s/it]\u001b[A\n",
      "Average Metric: 447.0 / 555  (80.5):  85%|█▋| 555/651 [2:06:01<19:57, 12.47s/it]\u001b[A\n",
      "Average Metric: 448.0 / 556  (80.6):  85%|█▋| 555/651 [2:06:05<19:57, 12.47s/it]\u001b[A\n",
      "Average Metric: 448.0 / 556  (80.6):  85%|█▋| 556/651 [2:06:08<17:58, 11.35s/it]\u001b[A\n",
      "Average Metric: 449.0 / 557  (80.6):  85%|█▋| 556/651 [2:06:27<17:58, 11.35s/it]\u001b[A\n",
      "Average Metric: 449.0 / 557  (80.6):  86%|█▋| 557/651 [2:06:30<22:21, 14.27s/it]\u001b[A\n",
      "Average Metric: 450.0 / 558  (80.6):  86%|█▋| 557/651 [2:06:37<22:21, 14.27s/it]\u001b[A\n",
      "Average Metric: 450.0 / 558  (80.6):  86%|█▋| 558/651 [2:06:39<19:28, 12.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t PyCryptodome is required for AES algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 451.0 / 559  (80.7):  86%|█▋| 558/651 [2:06:59<19:28, 12.57s/it]\u001b[A\n",
      "Average Metric: 451.0 / 559  (80.7):  86%|█▋| 559/651 [2:06:59<23:14, 15.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t PyCryptodome is required for AES algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 451.0 / 560  (80.5):  86%|█▋| 559/651 [2:07:00<23:14, 15.16s/it]\u001b[A\n",
      "Average Metric: 451.0 / 560  (80.5):  86%|█▋| 560/651 [2:07:01<17:00, 11.21s/it]\u001b[A\n",
      "Average Metric: 452.0 / 561  (80.6):  86%|█▋| 560/651 [2:07:12<17:00, 11.21s/it]\u001b[A\n",
      "Average Metric: 452.0 / 561  (80.6):  86%|█▋| 561/651 [2:07:14<17:25, 11.62s/it]\u001b[A\n",
      "Average Metric: 453.0 / 562  (80.6):  86%|█▋| 561/651 [2:07:36<17:25, 11.62s/it]\u001b[A\n",
      "Average Metric: 453.0 / 562  (80.6):  86%|█▋| 562/651 [2:07:38<22:44, 15.34s/it]\u001b[A\n",
      "Average Metric: 454.0 / 563  (80.6):  86%|█▋| 562/651 [2:07:50<22:44, 15.34s/it]\u001b[A\n",
      "Average Metric: 454.0 / 563  (80.6):  86%|█▋| 563/651 [2:07:51<21:45, 14.84s/it]\u001b[A\n",
      "Average Metric: 455.0 / 564  (80.7):  86%|█▋| 563/651 [2:07:52<21:45, 14.84s/it]\u001b[A\n",
      "Average Metric: 455.0 / 564  (80.7):  87%|█▋| 564/651 [2:07:54<16:06, 11.11s/it]\u001b[A\n",
      "Average Metric: 455.0 / 565  (80.5):  87%|█▋| 564/651 [2:07:55<16:06, 11.11s/it]\u001b[A\n",
      "Average Metric: 455.0 / 565  (80.5):  87%|█▋| 565/651 [2:07:59<12:59,  9.06s/it]\u001b[A\n",
      "Average Metric: 456.0 / 566  (80.6):  87%|█▋| 565/651 [2:08:02<12:59,  9.06s/it]\u001b[A\n",
      "Average Metric: 456.0 / 566  (80.6):  87%|█▋| 566/651 [2:08:05<11:43,  8.28s/it]\u001b[A\n",
      "Average Metric: 457.0 / 567  (80.6):  87%|█▋| 566/651 [2:08:21<11:43,  8.28s/it]\u001b[A\n",
      "Average Metric: 457.0 / 567  (80.6):  87%|█▋| 567/651 [2:08:22<15:32, 11.10s/it]\u001b[A\n",
      "Average Metric: 458.0 / 568  (80.6):  87%|█▋| 567/651 [2:08:28<15:32, 11.10s/it]\u001b[A\n",
      "Average Metric: 458.0 / 568  (80.6):  87%|█▋| 568/651 [2:08:30<13:47,  9.97s/it]\u001b[A\n",
      "Average Metric: 459.0 / 569  (80.7):  87%|█▋| 568/651 [2:08:36<13:47,  9.97s/it]\u001b[A\n",
      "Average Metric: 459.0 / 569  (80.7):  87%|█▋| 569/651 [2:08:38<12:42,  9.29s/it]\u001b[A\n",
      "Average Metric: 460.0 / 570  (80.7):  87%|█▋| 569/651 [2:09:17<12:42,  9.29s/it]\u001b[A\n",
      "Average Metric: 460.0 / 570  (80.7):  88%|█▊| 570/651 [2:09:18<25:28, 18.87s/it]\u001b[A\n",
      "Average Metric: 461.0 / 571  (80.7):  88%|█▊| 570/651 [2:09:27<25:28, 18.87s/it]\u001b[A\n",
      "Average Metric: 461.0 / 571  (80.7):  88%|█▊| 571/651 [2:09:29<21:56, 16.45s/it]\u001b[A\n",
      "Average Metric: 461.0 / 572  (80.6):  88%|█▊| 571/651 [2:09:33<21:56, 16.45s/it]\u001b[A\n",
      "Average Metric: 461.0 / 572  (80.6):  88%|█▊| 572/651 [2:09:39<18:31, 14.06s/it]\u001b[A\n",
      "Average Metric: 462.0 / 573  (80.6):  88%|█▊| 572/651 [2:09:45<18:31, 14.06s/it]\u001b[A\n",
      "Average Metric: 462.0 / 573  (80.6):  88%|█▊| 573/651 [2:09:48<16:18, 12.55s/it]\u001b[A\n",
      "Average Metric: 463.0 / 574  (80.7):  88%|█▊| 573/651 [2:09:51<16:18, 12.55s/it]\u001b[A\n",
      "Average Metric: 463.0 / 574  (80.7):  88%|█▊| 574/651 [2:09:54<13:19, 10.39s/it]\u001b[A\n",
      "Average Metric: 464.0 / 575  (80.7):  88%|█▊| 574/651 [2:09:59<13:19, 10.39s/it]\u001b[A\n",
      "Average Metric: 464.0 / 575  (80.7):  88%|█▊| 575/651 [2:10:03<12:56, 10.21s/it]\u001b[A\n",
      "Average Metric: 465.0 / 576  (80.7):  88%|█▊| 575/651 [2:10:09<12:56, 10.21s/it]\u001b[A\n",
      "Average Metric: 465.0 / 576  (80.7):  88%|█▊| 576/651 [2:10:12<12:35, 10.08s/it]\u001b[A\n",
      "Average Metric: 465.0 / 577  (80.6):  88%|█▊| 576/651 [2:10:18<12:35, 10.08s/it]\u001b[A\n",
      "Average Metric: 465.0 / 577  (80.6):  89%|█▊| 577/651 [2:10:21<11:51,  9.61s/it]\u001b[A\n",
      "Average Metric: 466.0 / 578  (80.6):  89%|█▊| 577/651 [2:11:13<11:51,  9.61s/it]\u001b[A\n",
      "Average Metric: 466.0 / 578  (80.6):  89%|█▊| 578/651 [2:11:14<27:58, 23.00s/it]\u001b[A\n",
      "Average Metric: 467.0 / 579  (80.7):  89%|█▊| 578/651 [2:11:45<27:58, 23.00s/it]\u001b[A\n",
      "Average Metric: 467.0 / 579  (80.7):  89%|█▊| 579/651 [2:11:48<31:15, 26.05s/it]\u001b[A\n",
      "Average Metric: 468.0 / 580  (80.7):  89%|█▊| 579/651 [2:11:50<31:15, 26.05s/it]\u001b[A\n",
      "Average Metric: 468.0 / 580  (80.7):  89%|█▊| 580/651 [2:11:52<22:48, 19.28s/it]\u001b[A\n",
      "Average Metric: 468.0 / 581  (80.6):  89%|█▊| 580/651 [2:11:58<22:48, 19.28s/it]\u001b[A\n",
      "Average Metric: 468.0 / 581  (80.6):  89%|█▊| 581/651 [2:12:01<18:58, 16.27s/it]\u001b[A\n",
      "Average Metric: 469.0 / 582  (80.6):  89%|█▊| 581/651 [2:12:22<18:58, 16.27s/it]\u001b[A\n",
      "Average Metric: 469.0 / 582  (80.6):  89%|█▊| 582/651 [2:12:24<21:18, 18.53s/it]\u001b[A\n",
      "Average Metric: 470.0 / 583  (80.6):  89%|█▊| 582/651 [2:12:37<21:18, 18.53s/it]\u001b[A\n",
      "Average Metric: 470.0 / 583  (80.6):  90%|█▊| 583/651 [2:12:43<20:45, 18.32s/it]\u001b[A\n",
      "Average Metric: 471.0 / 584  (80.7):  90%|█▊| 583/651 [2:12:59<20:45, 18.32s/it]\u001b[A\n",
      "Average Metric: 471.0 / 584  (80.7):  90%|█▊| 584/651 [2:13:01<20:40, 18.52s/it]\u001b[A\n",
      "Average Metric: 472.0 / 585  (80.7):  90%|█▊| 584/651 [2:13:06<20:40, 18.52s/it]\u001b[A\n",
      "Average Metric: 472.0 / 585  (80.7):  90%|█▊| 585/651 [2:13:09<16:29, 14.99s/it]\u001b[A\n",
      "Average Metric: 473.0 / 586  (80.7):  90%|█▊| 585/651 [2:13:32<16:29, 14.99s/it]\u001b[A\n",
      "Average Metric: 473.0 / 586  (80.7):  90%|█▊| 586/651 [2:13:33<19:41, 18.18s/it]\u001b[A\n",
      "Average Metric: 473.0 / 587  (80.6):  90%|█▊| 586/651 [2:13:36<19:41, 18.18s/it]\u001b[A\n",
      "Average Metric: 473.0 / 587  (80.6):  90%|█▊| 587/651 [2:13:38<14:50, 13.92s/it]\u001b[A\n",
      "Average Metric: 474.0 / 588  (80.6):  90%|█▊| 587/651 [2:14:04<14:50, 13.92s/it]\u001b[A\n",
      "Average Metric: 474.0 / 588  (80.6):  90%|█▊| 588/651 [2:14:06<19:15, 18.33s/it]\u001b[A\n",
      "Average Metric: 475.0 / 589  (80.6):  90%|█▊| 588/651 [2:14:12<19:15, 18.33s/it]\u001b[A\n",
      "Average Metric: 475.0 / 589  (80.6):  90%|█▊| 589/651 [2:14:13<15:38, 15.14s/it]\u001b[A\n",
      "Average Metric: 476.0 / 590  (80.7):  90%|█▊| 589/651 [2:14:19<15:38, 15.14s/it]\u001b[A\n",
      "Average Metric: 476.0 / 590  (80.7):  91%|█▊| 590/651 [2:14:21<13:09, 12.95s/it]\u001b[A\n",
      "Average Metric: 477.0 / 591  (80.7):  91%|█▊| 590/651 [2:14:26<13:09, 12.95s/it]\u001b[A\n",
      "Average Metric: 477.0 / 591  (80.7):  91%|█▊| 591/651 [2:14:27<10:44, 10.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t negative seek value -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 477.0 / 592  (80.6):  91%|█▊| 591/651 [2:14:30<10:44, 10.75s/it]\u001b[A\n",
      "Average Metric: 477.0 / 592  (80.6):  91%|█▊| 592/651 [2:14:32<08:37,  8.76s/it]\u001b[A\n",
      "Average Metric: 478.0 / 593  (80.6):  91%|█▊| 592/651 [2:14:33<08:37,  8.76s/it]\u001b[A\n",
      "Average Metric: 478.0 / 593  (80.6):  91%|█▊| 593/651 [2:14:35<06:49,  7.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t negative seek value -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 479.0 / 594  (80.6):  91%|█▊| 593/651 [2:14:59<06:49,  7.07s/it]\u001b[A\n",
      "Average Metric: 479.0 / 594  (80.6):  91%|█▊| 594/651 [2:15:01<12:17, 12.94s/it]\u001b[A\n",
      "Average Metric: 480.0 / 595  (80.7):  91%|█▊| 594/651 [2:15:15<12:17, 12.94s/it]\u001b[A\n",
      "Average Metric: 480.0 / 595  (80.7):  91%|█▊| 595/651 [2:15:16<12:47, 13.71s/it]\u001b[A\n",
      "Average Metric: 481.0 / 596  (80.7):  91%|█▊| 595/651 [2:15:40<12:47, 13.71s/it]\u001b[A\n",
      "Average Metric: 481.0 / 596  (80.7):  92%|█▊| 596/651 [2:15:42<15:42, 17.14s/it]\u001b[A\n",
      "Average Metric: 481.0 / 597  (80.6):  92%|█▊| 596/651 [2:15:46<15:42, 17.14s/it]\u001b[A\n",
      "Average Metric: 481.0 / 597  (80.6):  92%|█▊| 597/651 [2:15:49<12:37, 14.03s/it]\u001b[A\n",
      "Average Metric: 481.0 / 598  (80.4):  92%|█▊| 597/651 [2:15:55<12:37, 14.03s/it]\u001b[A\n",
      "Average Metric: 481.0 / 598  (80.4):  92%|█▊| 598/651 [2:15:58<11:03, 12.52s/it]\u001b[A\n",
      "Average Metric: 481.0 / 599  (80.3):  92%|█▊| 598/651 [2:16:14<11:03, 12.52s/it]\u001b[A\n",
      "Average Metric: 481.0 / 599  (80.3):  92%|█▊| 599/651 [2:16:15<12:18, 14.21s/it]\u001b[A\n",
      "Average Metric: 482.0 / 600  (80.3):  92%|█▊| 599/651 [2:16:29<12:18, 14.21s/it]\u001b[A\n",
      "Average Metric: 482.0 / 600  (80.3):  92%|█▊| 600/651 [2:16:30<12:10, 14.33s/it]\u001b[A\n",
      "Average Metric: 483.0 / 601  (80.4):  92%|█▊| 600/651 [2:16:31<12:10, 14.33s/it]\u001b[A\n",
      "Average Metric: 483.0 / 601  (80.4):  92%|█▊| 601/651 [2:16:33<08:51, 10.62s/it]\u001b[A\n",
      "Average Metric: 484.0 / 602  (80.4):  92%|█▊| 601/651 [2:16:36<08:51, 10.62s/it]\u001b[A\n",
      "Average Metric: 484.0 / 602  (80.4):  92%|█▊| 602/651 [2:16:39<07:41,  9.43s/it]\u001b[A\n",
      "Average Metric: 485.0 / 603  (80.4):  92%|█▊| 602/651 [2:17:02<07:41,  9.43s/it]\u001b[A\n",
      "Average Metric: 485.0 / 603  (80.4):  93%|█▊| 603/651 [2:17:05<11:18, 14.14s/it]\u001b[A\n",
      "Average Metric: 485.0 / 604  (80.3):  93%|█▊| 603/651 [2:17:13<11:18, 14.14s/it]\u001b[A\n",
      "Average Metric: 485.0 / 604  (80.3):  93%|█▊| 604/651 [2:17:14<10:10, 13.00s/it]\u001b[A\n",
      "Average Metric: 486.0 / 605  (80.3):  93%|█▊| 604/651 [2:17:21<10:10, 13.00s/it]\u001b[A\n",
      "Average Metric: 486.0 / 605  (80.3):  93%|█▊| 605/651 [2:17:24<09:13, 12.03s/it]\u001b[A\n",
      "Average Metric: 486.0 / 606  (80.2):  93%|█▊| 605/651 [2:18:06<09:13, 12.03s/it]\u001b[A\n",
      "Average Metric: 486.0 / 606  (80.2):  93%|█▊| 606/651 [2:18:07<16:00, 21.33s/it]\u001b[A\n",
      "Average Metric: 486.0 / 607  (80.1):  93%|█▊| 606/651 [2:18:20<16:00, 21.33s/it]\u001b[A\n",
      "Average Metric: 486.0 / 607  (80.1):  93%|█▊| 607/651 [2:18:20<13:52, 18.92s/it]\u001b[A\n",
      "Average Metric: 487.0 / 608  (80.1):  93%|█▊| 607/651 [2:18:21<13:52, 18.92s/it]\u001b[A\n",
      "Average Metric: 487.0 / 608  (80.1):  93%|█▊| 608/651 [2:18:23<09:58, 13.92s/it]\u001b[A\n",
      "Average Metric: 487.0 / 609  (80.0):  93%|█▊| 608/651 [2:18:28<09:58, 13.92s/it]\u001b[A\n",
      "Average Metric: 487.0 / 609  (80.0):  94%|█▊| 609/651 [2:18:30<08:18, 11.88s/it]\u001b[A\n",
      "Average Metric: 488.0 / 610  (80.0):  94%|█▊| 609/651 [2:18:57<08:18, 11.88s/it]\u001b[A\n",
      "Average Metric: 488.0 / 610  (80.0):  94%|█▊| 610/651 [2:18:58<11:31, 16.85s/it]\u001b[A\n",
      "Average Metric: 489.0 / 611  (80.0):  94%|█▊| 610/651 [2:18:59<11:31, 16.85s/it]\u001b[A\n",
      "Average Metric: 489.0 / 611  (80.0):  94%|█▉| 611/651 [2:19:00<08:18, 12.47s/it]\u001b[A\n",
      "Average Metric: 490.0 / 612  (80.1):  94%|█▉| 611/651 [2:19:09<08:18, 12.47s/it]\u001b[A\n",
      "Average Metric: 490.0 / 612  (80.1):  94%|█▉| 612/651 [2:19:09<07:26, 11.46s/it]\u001b[A\n",
      "Average Metric: 491.0 / 613  (80.1):  94%|█▉| 612/651 [2:19:11<07:26, 11.46s/it]\u001b[A\n",
      "Average Metric: 491.0 / 613  (80.1):  94%|█▉| 613/651 [2:19:14<05:44,  9.06s/it]\u001b[A\n",
      "Average Metric: 492.0 / 614  (80.1):  94%|█▉| 613/651 [2:19:51<05:44,  9.06s/it]\u001b[A\n",
      "Average Metric: 492.0 / 614  (80.1):  94%|█▉| 614/651 [2:19:52<11:15, 18.26s/it]\u001b[A\n",
      "Average Metric: 493.0 / 615  (80.2):  94%|█▉| 614/651 [2:20:06<11:15, 18.26s/it]\u001b[A\n",
      "Average Metric: 493.0 / 615  (80.2):  94%|█▉| 615/651 [2:20:07<10:19, 17.20s/it]\u001b[A\n",
      "Average Metric: 494.0 / 616  (80.2):  94%|█▉| 615/651 [2:20:18<10:19, 17.20s/it]\u001b[A\n",
      "Average Metric: 494.0 / 616  (80.2):  95%|█▉| 616/651 [2:20:21<09:10, 15.72s/it]\u001b[A\n",
      "Average Metric: 495.0 / 617  (80.2):  95%|█▉| 616/651 [2:20:23<09:10, 15.72s/it]\u001b[A\n",
      "Average Metric: 495.0 / 617  (80.2):  95%|█▉| 617/651 [2:20:26<07:17, 12.86s/it]\u001b[A\n",
      "Average Metric: 496.0 / 618  (80.3):  95%|█▉| 617/651 [2:20:50<07:17, 12.86s/it]\u001b[A\n",
      "Average Metric: 496.0 / 618  (80.3):  95%|█▉| 618/651 [2:20:53<09:24, 17.12s/it]\u001b[A\n",
      "Average Metric: 497.0 / 619  (80.3):  95%|█▉| 618/651 [2:20:55<09:24, 17.12s/it]\u001b[A\n",
      "Average Metric: 497.0 / 619  (80.3):  95%|█▉| 619/651 [2:20:58<07:09, 13.41s/it]\u001b[A/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.26s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\n",
      "Average Metric: 498.0 / 620  (80.3):  95%|█▉| 619/651 [2:21:23<07:09, 13.41s/it]\u001b[A\n",
      "Average Metric: 498.0 / 620  (80.3):  95%|█▉| 620/651 [2:21:25<09:04, 17.57s/it]\u001b[A\n",
      "Average Metric: 499.0 / 621  (80.4):  95%|█▉| 620/651 [2:21:30<09:04, 17.57s/it]\u001b[A\n",
      "Average Metric: 499.0 / 621  (80.4):  95%|█▉| 621/651 [2:21:31<07:02, 14.09s/it]\u001b[A\n",
      "Average Metric: 500.0 / 622  (80.4):  95%|█▉| 621/651 [2:21:47<07:02, 14.09s/it]\u001b[A\n",
      "Average Metric: 500.0 / 622  (80.4):  96%|█▉| 622/651 [2:21:52<07:39, 15.84s/it]/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 2.78s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "\u001b[A\n",
      "Average Metric: 501.0 / 623  (80.4):  96%|█▉| 622/651 [2:21:55<07:39, 15.84s/it]\u001b[A\n",
      "Average Metric: 501.0 / 623  (80.4):  96%|█▉| 623/651 [2:21:59<06:12, 13.32s/it]\u001b[A\n",
      "Average Metric: 502.0 / 624  (80.4):  96%|█▉| 623/651 [2:22:43<06:12, 13.32s/it]\u001b[A\n",
      "Average Metric: 502.0 / 624  (80.4):  96%|█▉| 624/651 [2:22:45<10:25, 23.18s/it]\u001b[A\n",
      "Average Metric: 503.0 / 625  (80.5):  96%|█▉| 624/651 [2:22:46<10:25, 23.18s/it]\u001b[A\n",
      "Average Metric: 503.0 / 625  (80.5):  96%|█▉| 625/651 [2:22:49<07:29, 17.27s/it]\u001b[A\n",
      "Average Metric: 504.0 / 626  (80.5):  96%|█▉| 625/651 [2:22:52<07:29, 17.27s/it]\u001b[A\n",
      "Average Metric: 504.0 / 626  (80.5):  96%|█▉| 626/651 [2:22:54<05:52, 14.11s/it]\u001b[A\n",
      "Average Metric: 505.0 / 627  (80.5):  96%|█▉| 626/651 [2:23:27<05:52, 14.11s/it]\u001b[A\n",
      "Average Metric: 505.0 / 627  (80.5):  96%|█▉| 627/651 [2:23:28<07:57, 19.89s/it]\u001b[A\n",
      "Average Metric: 506.0 / 628  (80.6):  96%|█▉| 627/651 [2:24:04<07:57, 19.89s/it]\u001b[A\n",
      "Average Metric: 506.0 / 628  (80.6):  96%|█▉| 628/651 [2:24:05<09:32, 24.88s/it]\u001b[A\n",
      "Average Metric: 507.0 / 629  (80.6):  96%|█▉| 628/651 [2:24:12<09:32, 24.88s/it]\u001b[A\n",
      "Average Metric: 507.0 / 629  (80.6):  97%|█▉| 629/651 [2:24:13<07:18, 19.95s/it]\u001b[A\n",
      "Average Metric: 507.0 / 630  (80.5):  97%|█▉| 629/651 [2:24:19<07:18, 19.95s/it]\u001b[A\n",
      "Average Metric: 507.0 / 630  (80.5):  97%|█▉| 630/651 [2:24:21<05:40, 16.23s/it]\u001b[A\n",
      "Average Metric: 508.0 / 631  (80.5):  97%|█▉| 630/651 [2:24:32<05:40, 16.23s/it]\u001b[A\n",
      "Average Metric: 508.0 / 631  (80.5):  97%|█▉| 631/651 [2:24:34<05:08, 15.41s/it]\u001b[A\n",
      "Average Metric: 508.0 / 632  (80.4):  97%|█▉| 631/651 [2:24:35<05:08, 15.41s/it]\u001b[A\n",
      "Average Metric: 508.0 / 632  (80.4):  97%|█▉| 632/651 [2:24:37<03:41, 11.68s/it]\u001b[A\n",
      "Average Metric: 509.0 / 633  (80.4):  97%|█▉| 632/651 [2:24:45<03:41, 11.68s/it]\u001b[A\n",
      "Average Metric: 509.0 / 633  (80.4):  97%|█▉| 633/651 [2:24:48<03:28, 11.56s/it]\u001b[A\n",
      "Average Metric: 509.0 / 634  (80.3):  97%|█▉| 633/651 [2:24:51<03:28, 11.56s/it]\u001b[A\n",
      "Average Metric: 509.0 / 634  (80.3):  97%|█▉| 634/651 [2:24:53<02:41,  9.49s/it]\u001b[A\n",
      "Average Metric: 510.0 / 635  (80.3):  97%|█▉| 634/651 [2:25:02<02:41,  9.49s/it]\u001b[A\n",
      "Average Metric: 510.0 / 635  (80.3):  98%|█▉| 635/651 [2:25:03<02:32,  9.53s/it]\u001b[A\n",
      "Average Metric: 511.0 / 636  (80.3):  98%|█▉| 635/651 [2:25:43<02:32,  9.53s/it]\u001b[A\n",
      "Average Metric: 511.0 / 636  (80.3):  98%|█▉| 636/651 [2:25:43<04:44, 18.94s/it]\u001b[A\n",
      "Average Metric: 512.0 / 637  (80.4):  98%|█▉| 636/651 [2:25:55<04:44, 18.94s/it]\u001b[A\n",
      "Average Metric: 512.0 / 637  (80.4):  98%|█▉| 637/651 [2:25:55<03:57, 16.94s/it]\u001b[A\n",
      "Average Metric: 513.0 / 638  (80.4):  98%|█▉| 637/651 [2:26:08<03:57, 16.94s/it]\u001b[A\n",
      "Average Metric: 513.0 / 638  (80.4):  98%|█▉| 638/651 [2:26:10<03:30, 16.19s/it]\u001b[A\n",
      "Average Metric: 514.0 / 639  (80.4):  98%|█▉| 638/651 [2:26:32<03:30, 16.19s/it]\u001b[A\n",
      "Average Metric: 514.0 / 639  (80.4):  98%|█▉| 639/651 [2:26:33<03:38, 18.18s/it]\u001b[A\n",
      "Average Metric: 515.0 / 640  (80.5):  98%|█▉| 639/651 [2:26:35<03:38, 18.18s/it]\u001b[A\n",
      "Average Metric: 515.0 / 640  (80.5):  98%|█▉| 640/651 [2:26:37<02:28, 13.53s/it]\u001b[A\n",
      "Average Metric: 516.0 / 641  (80.5):  98%|█▉| 640/651 [2:26:39<02:28, 13.53s/it]\u001b[A\n",
      "Average Metric: 516.0 / 641  (80.5):  98%|█▉| 641/651 [2:26:43<01:52, 11.28s/it]\u001b[A\n",
      "Average Metric: 516.0 / 642  (80.4):  98%|█▉| 641/651 [2:26:45<01:52, 11.28s/it]\u001b[A\n",
      "Average Metric: 516.0 / 642  (80.4):  99%|█▉| 642/651 [2:26:48<01:27,  9.75s/it]\u001b[A\n",
      "Average Metric: 517.0 / 643  (80.4):  99%|█▉| 642/651 [2:27:12<01:27,  9.75s/it]\u001b[A\n",
      "Average Metric: 517.0 / 643  (80.4):  99%|█▉| 643/651 [2:27:12<01:54, 14.28s/it]\u001b[A\n",
      "Average Metric: 518.0 / 644  (80.4):  99%|█▉| 643/651 [2:27:49<01:54, 14.28s/it]\u001b[A\n",
      "Average Metric: 518.0 / 644  (80.4):  99%|█▉| 644/651 [2:27:50<02:28, 21.26s/it]\u001b[A\n",
      "Average Metric: 518.0 / 645  (80.3):  99%|█▉| 644/651 [2:27:54<02:28, 21.26s/it]\u001b[A\n",
      "Average Metric: 518.0 / 645  (80.3):  99%|█▉| 645/651 [2:27:55<01:37, 16.29s/it]\u001b[A\n",
      "Average Metric: 519.0 / 646  (80.3):  99%|█▉| 645/651 [2:28:02<01:37, 16.29s/it]\u001b[A\n",
      "Average Metric: 519.0 / 646  (80.3):  99%|█▉| 646/651 [2:28:03<01:09, 13.87s/it]\u001b[A\n",
      "Average Metric: 520.0 / 647  (80.4):  99%|█▉| 646/651 [2:28:06<01:09, 13.87s/it]\u001b[A\n",
      "Average Metric: 520.0 / 647  (80.4):  99%|█▉| 647/651 [2:28:06<00:42, 10.68s/it]\u001b[A\n",
      "Average Metric: 521.0 / 648  (80.4):  99%|█▉| 647/651 [2:28:06<00:42, 10.68s/it]\u001b[A\n",
      "Average Metric: 521.0 / 648  (80.4): 100%|█▉| 648/651 [2:28:06<00:22,  7.59s/it]\u001b[A\n",
      "Average Metric: 521.0 / 649  (80.3): 100%|█▉| 648/651 [2:28:06<00:22,  7.59s/it]\u001b[A\n",
      "Average Metric: 522.0 / 650  (80.3): 100%|█▉| 649/651 [2:28:07<00:15,  7.59s/it]\u001b[A\n",
      "Average Metric: 522.0 / 650  (80.3): 100%|█▉| 650/651 [2:28:07<00:04,  4.17s/it]\u001b[A\n",
      "Average Metric: 523.0 / 651  (80.3): 100%|█▉| 650/651 [2:35:27<00:04,  4.17s/it]\u001b[A\n",
      "Average Metric: 523.0 / 651  (80.3): 100%|██| 651/651 [2:35:27<00:00, 14.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 523.0 / 651  (80.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    }
   ],
   "source": [
    "evaluate = Evaluate(devset=trainset, metric=metric, num_threads=8, display_progress=True, display_table=0, max_errors=100, return_outputs=True)\n",
    "outputs = evaluate(pipeline_chunking_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk:  m nmmm vvvv  321CNN Extracted descriptor vector sequence 1st-person activity video … … Time series representation Per-frame feature representation … Temporal pooling Final representation sum pooling ‘histogram of time series gradients’ pooling … … … … … n*m dimensional data (e.g., n = 4096 features, m = 1000 frames) n time series (e.g., 4096) k temporal filters (e.g., 15) n*k-D vector (e.g., 61440) … max pooling Figure 2. Overall representation framework of our pooled time series (PoT). PoT feature representation of the video: x= [xop1 1[ts 1;te 1]; xop2 1[ts 1;te 1];\u0001\u0001\u0001; xopr n[ts k;te k]] (1) wherexopj ispeciﬁes that it is applying the jth pooling op- erator to the ith time series fi(t). Our PoT representation takes advantage of four different types of pooling operators including two newly introduced temporal pooling operators, which we discuss more in Subsection 2.2. Our framework of (i) extracting per-frame descriptors, (ii) interpreting them as a set of time series\n",
      "\n",
      "Candidate Chunk: on the time from the occurrence of the event (ie, the onset of the disease) until a diagnosis is available that fulfills a case definition. Depends on the time it takes for reporting through the stages of a hierarchical reporting structure.Depends on the time from the occurrence of the event (ie, onset of the disease) until the first mention occurs, which might be before diagnostic confirmation is available. Depends on the ability of the system and the time it takes to pick up a signal and to interpret it correctly. Thresholds for Signal GenerationStatistical methods are employed to identify increased numbers (clusters) in time or in space (or combinations of both) to generate aSignals are differentially generated (eg, human indexing in ProMED-mail) but rarely with automated statistical methods that Continued 12 E. Velasco et al. TABLE 1 —Continued Indicator-Based Event-Based signal for potential event-detection.identify increased numbers (clusters) in time or in space (or combinations of both) to generat\n",
      "\n",
      "Context:\n",
      "Query Chunk: indicator variable that chooses to generate either the type of a word or the embedding of a word and GPU-DMM [8] extends DMM with word semantic similarity obtained from embeddings for short texts. Although with improved performance there still exists challenges for existing models: (1) for aggregation- based models, it is usually hard to choose which meta information to use for aggregation; (2) the “single topic” assumption makes DMM models lose the ﬂexibility to capture different topic ingredients of a document; and (3) the incorporation of meta information in the existing models is usually less efﬁcient. To our knowledge, the attempts that jointly leverage zd;i wd;i\u0012d\u000b",
      "d fd;l\u0015l;k\u00160 \u001e",
      "k\f",
      "k\u000el0;kgv;l0\u00170 8k 8l8v 8l0 8i 8d8k Figure 1: The graphical model of MetaLDA document and word meta information are relatively rare. For example, meta information can be incorporated by ﬁrst- order logic in Logit-LDA [22] and score functions in SC- LDA [23]. However, the ﬁrst-order logic and score functions need to be deﬁned\n",
      "Candidate Chunk: are parallel and orthogonal to thelevel curves). Also shown in Fig. 2(a) is the family of solutions that are obtained asvaries from 0 to(red path). Notice that these solutions are not sparse in thesense (i.e., they do not intersect either of the two axes) untilis so large that the signal reconstruction is quite poor. This is the cas e regardless of how smallis, and is due both to the failure of thenorm to approximate thepseudonorm and to the inability of the discrete model to account for continuous event times. In the following sections, we de- velop and demonstrate a proposed solution for these problems. IV. C ONTINUOUS BASIS PURSUIT The motivation for the discretization of the inference problem was tractability. Equation (5) ap proximates the original model well in the limit asgoes to 0, but it is only tractably solv- able in regimes whereis large enough so that correlations are limited and-based relaxations can be employed. In this section, we augment the discrete model by incorporating vari- ables to\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: , and (iii) apply- ing various types of time series pooling and concatenating them provides the following three important abilities: First, (1) it preserves detailed dynamics displayed in each descriptor element as a time series, and allows the rep- resentation to capture both long-term motion and short-term information with multiple temporal ﬁlters. That is, depend- ing on the nature of the time series, our representation is able to capture subtle short-term motion by pooling from a ﬁlter with a small time interval as well as long-term motion by performing pooling with a large time interval. Such ﬂex- ibility is in contrast to previous bag-of-visual-words repre- sentation for global motion descriptors (e.g., the one used in [18]) that abstracts all descriptor values in one frame (or a subsequence of few frames) into a single discretized ‘vi- sual word’. In addition, (2) our representation explicitly im- poses temporal structure of the activity by decomposing the entire time interval to multiple subinterv\n",
      "\n",
      "Candidate Chunk: lligence Service (EIS).4-8 Event-based surveillance continues to offer innovation for public health surveillance, for example, by capturing information about events Social Media and Internet-Based Data for Public Health Surveillance 11 TABLE 1 Indicator-Based and Event-Based Surveillance Systems Indicator-Based Event-Based Timeliness of Data InputInformation is input as soon as it is made available.Information is input as soon as it occurs. Timing is set immediate/ weekly/monthly. Possible delay between identification and notification.Timing varies, depending on when the data are available from those who have the information. Possible delay between identification and reporting. Reporting Clearly defined. Predefined or not Structure Reporting forms. Reporting dates. Teams analyze data at regular intervals. Moderated.predefined structure. Reporting forms flexible for qualitative and quantitative data. Teams analyze data at any time. Moderated or not moderated (eg, automatic). Timeliness of DetectionDepends\n",
      "\n",
      "Context:\n",
      "Query Chunk: nearly identical to the ﬁrst image. The whole video from which these images are taken is included in the supple- mental material for this paper. We also included a video of the whole camera tour, recorded in real-time from a third- person camera perspective. The fact that the objects in this video remain at their position shows that the accumulated camera pose is highly accurate. To evaluate our algorithm quantitatively, we use the pub- licly available RGB-D dataset by Sturm et al. [10]. This dataset contains RGB-D images from a Microsoft Kinect with synchronized camera poses from an external motion capture system. From the large variety of different se- quences, we chose the freiburg1/desk and freiburg2/desk for our experiments as they contain both translational and rota- tional motions in a typical ofﬁce environment at different speeds. We evaluated the drift per frame over these two se- quences, see Tab. 1 and Fig. 2(top). Our approach has a me- dian drift of 5.3 and 1.5 mm, while GICP drifts by 10.3 a\n",
      "Candidate Chunk: t. Keywords: software visualization, social networks, software engi- neering, time-varying data, information visualization, collaborative work Index Terms: H.5.2 [Information Interfaces and Presentation]: User Interfaces—Graphical User Interfaces; D.2.9 [Software Engi- neering]: Management—Productivity 1 I NTRODUCTION Building large software systems involves sustained effort by large teams. Teams of people divide up a large system into manageable components. Then by collaboratively developing each part, they in effect develop the whole system. Communication and collab- oration activities are crucial to this process, and are therefore of critical interest to software engineering researchers. Just as a large system is broken into components, for manageability, a large devel- opment team forms sub-communities of interest in order to collab- orate in an orderly fashion. In commercial projects, these teams are organized by management command; however, the actual commu- nication among team members a) does not n\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: als, which is very important for representing high-level activities. Finally, (3) it allows the system to take advantage of multiple types of pooling operators so that the representation captures differ- ent aspects of the data. As a result of our framework, each video is represented with one single vector having a tractable dimensionality.Activity recognition is performed by training/testing stan- dard classiﬁers (e.g., SVM classiﬁers) with these vectors. Our representation is able to cope with any type of genera- tive and discriminative classiﬁers in principle, and we show its superiority over others in Section 3.2. 2.1. Handling high-dimensional feature descriptors The proposed representation framework is very gen- eral in the aspect that it is able to cope with any types of per-frame image/motion descriptors such as histogram of oriented gradients (HOG) or histogram of optical ﬂows (HOF). Furthermore, it is particularly designed to han- dle high-dimensional per-frame image descriptors: image- based de\n",
      "\n",
      "Candidate Chunk: e this information relies less on data structured and filtered through the aforementioned preestab- lished structures for surveillance. Event-based surveillance can identify events faster than indicator-based reporting procedures can, and it can detect events that occur in populations not able to access formal chan- nels for reporting. In addition, event-based surveillance can be used with other established indicator-based methods, thereby enhancing the combined arsenal for combatting critically prevalent pathogens with a high threat potential, such as influenza virus or Escherichia coli .T h es c i - entific literature recently referred to this comprehensive framework of combined activities from both indicator-based surveillance and event- based surveillance systems as “epidemic intelligence,” a contemporary understanding of the 1950s term with roots in public health innova- tion for surveillance systems at the US Centers for Disease Control and Prevention (CDC) and the establishment of the Epidemic Inte\n",
      "\n",
      "Context:\n",
      "Query Chunk: provide an extensive comparison of allofthewidely-usedsmoothingtechniques.Theyevaluateeach algorithmonawiderangeoftrainingsetsthroughits perplexity on test data. The perplexity of a model on a test set is the reciprocal of the geometric average probability that the model assigns to each word in the test set. They also usethederivativemeasure cross-entropy , which can be interpreted as the average number of bits needed to code each word in the test set using the model . Chenet al.[8], [22] also conducted experiments investigating how the cross-entropyofalanguagemodelisrelatedtoitsperformancewhenusedinaspeechrecognitionsystem.Theyfoundastrong linear correlation between cross-entropy and recognition word errorratewhencomparingmodelsthatonlydifferinsmoothing. In terms of perplexity, Chen and Goodman found that Kneser–Ney smoothing and variations consistently outper- form all other algorithms. More specifically, they present the following four main conclusions. • The primary reason for the superior performanc\n",
      "Candidate Chunk: asets, on  the other hand, typically serve to accelerate research in a particular area. For instance, the face recognition community has benefited from a series of U.S. Governme nt funded technology development  efforts and evaluation cycles, beginning with the Facial Recognition Technology (FERET) program in 1993 [6]. The evaluations have documented two orders of magnitude improvement in performance from th e start of the FERET program  through the Face Recognition Vendor Test (FRVT) in 2006. In this effort, our intent is to stimul ate research in automated food  recognition by providing the research community with a comprehensive, public dataset of  common fast food items acquired  under both controlled and natural conditions.  Currently, there is no public dataset dedicated to automated  visual food recognition. Automated food recognition is a key technology for measuring dietary and supplement intake in obesity study and treatment. Accurate and passive acquisition of dietary   data from free-living in\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: ep learning features which are also called convolu- tional neural network (CNN) features [7, 19]. These deep learning image features are obtained by snatching interme- diate outputs from internal convolutional layers of a CNN, pre-trained on image datasets. They can also be viewed as cascades of automatically learned image ﬁlters. These im- age descriptors are trained from large scale image datasets and have obtained highly successful results on image clas- siﬁcation/detection [4] as well as video classiﬁcation [8], performing superior to state-of-the-art hand-designed im- age descriptors such as HOG even without re-training the networks. Our motivation was to design a general representation that best takes advantage of such high-dimensional descrip- tors and conﬁrm that these CNN features are able to in- crease ﬁrst-person activity recognition performance signif- icantly together with other features. Each element of a CNN feature vector abstracts particular local/global appear- ance for a single frame, a\n",
      "\n",
      "Candidate Chunk: e a signal for potential event-detection. Trigger for Follow-up or ActionCrossing a predefined threshold leads to an in-depth analysis and further information gathering.A confirmed event or hint at an event leads to further information gathering, verification. that may not otherwise be detected in the routine collection of data from indicator-based surveillance. Events that may be detected in event-based surveillance include the following: rEvents, such as SARS, that are emerging or rarely occur and thus are not specifically part of the purview of standard indicator-based surveillance.rEvents that occur in real time but have not been detected by indicator-based surveillance, such as those events delayed by the required reporting procedures of notifying the designated health authority.rEvents that occur in populations that do not access health care through formal channels or in which formal, indicator-based sys- tems do not exist, such as events that occur in populations in rural areas or countries with a\n",
      "\n",
      "Context:\n",
      "Query Chunk: provide an extensive comparison of allofthewidely-usedsmoothingtechniques.Theyevaluateeach algorithmonawiderangeoftrainingsetsthroughits perplexity on test data. The perplexity of a model on a test set is the reciprocal of the geometric average probability that the model assigns to each word in the test set. They also usethederivativemeasure cross-entropy , which can be interpreted as the average number of bits needed to code each word in the test set using the model . Chenet al.[8], [22] also conducted experiments investigating how the cross-entropyofalanguagemodelisrelatedtoitsperformancewhenusedinaspeechrecognitionsystem.Theyfoundastrong linear correlation between cross-entropy and recognition word errorratewhencomparingmodelsthatonlydifferinsmoothing. In terms of perplexity, Chen and Goodman found that Kneser–Ney smoothing and variations consistently outper- form all other algorithms. More specifically, they present the following four main conclusions. • The primary reason for the superior performanc\n",
      "Candidate Chunk: asets, on  the other hand, typically serve to accelerate research in a particular area. For instance, the face recognition community has benefited from a series of U.S. Governme nt funded technology development  efforts and evaluation cycles, beginning with the Facial Recognition Technology (FERET) program in 1993 [6]. The evaluations have documented two orders of magnitude improvement in performance from th e start of the FERET program  through the Face Recognition Vendor Test (FRVT) in 2006. In this effort, our intent is to stimul ate research in automated food  recognition by providing the research community with a comprehensive, public dataset of  common fast food items acquired  under both controlled and natural conditions.  Currently, there is no public dataset dedicated to automated  visual food recognition. Automated food recognition is a key technology for measuring dietary and supplement intake in obesity study and treatment. Accurate and passive acquisition of dietary   data from free-living in\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: nd (by extension) its time se- ries models how this local/global appearance is changing over time. As a human in the scene moves (e.g., changes his/her posture) and the camera changes its viewpoint be- cause of egomotion, certain CNN feature values will be- come activated/deactivated and our idea is to keep track of such changes to represent the activity video. In our experi- ments, we explicitly conﬁrm this while comparing our rep- resentation with the conventional representations. When us- ing CNN features as our base per-frame descriptors, we get a 4096-dimensional feature vector (i.e., n=4096) for each image frame by obtaining outputs of the last convolutional layer (e.g., stage 7 in [19]). 2.2. Temporal pooling operators Our PoT representation is constructed by applying multi- ple types of temporal pooling operators over each temporal ﬁlter (i.e., time interval). In this paper, we take advantage of four different types of pooling operators: conventional max pooling and sum pooling, and two types of o\n",
      "\n",
      "Candidate Chunk: on the time from the occurrence of the event (ie, the onset of the disease) until a diagnosis is available that fulfills a case definition. Depends on the time it takes for reporting through the stages of a hierarchical reporting structure.Depends on the time from the occurrence of the event (ie, onset of the disease) until the first mention occurs, which might be before diagnostic confirmation is available. Depends on the ability of the system and the time it takes to pick up a signal and to interpret it correctly. Thresholds for Signal GenerationStatistical methods are employed to identify increased numbers (clusters) in time or in space (or combinations of both) to generate aSignals are differentially generated (eg, human indexing in ProMED-mail) but rarely with automated statistical methods that Continued 12 E. Velasco et al. TABLE 1 —Continued Indicator-Based Event-Based signal for potential event-detection.identify increased numbers (clusters) in time or in space (or combinations of both) to generat\n",
      "\n",
      "Context:\n",
      "Query Chunk: indicator variable that chooses to generate either the type of a word or the embedding of a word and GPU-DMM [8] extends DMM with word semantic similarity obtained from embeddings for short texts. Although with improved performance there still exists challenges for existing models: (1) for aggregation- based models, it is usually hard to choose which meta information to use for aggregation; (2) the “single topic” assumption makes DMM models lose the ﬂexibility to capture different topic ingredients of a document; and (3) the incorporation of meta information in the existing models is usually less efﬁcient. To our knowledge, the attempts that jointly leverage zd;i wd;i\u0012d\u000b",
      "d fd;l\u0015l;k\u00160 \u001e",
      "k\f",
      "k\u000el0;kgv;l0\u00170 8k 8l8v 8l0 8i 8d8k Figure 1: The graphical model of MetaLDA document and word meta information are relatively rare. For example, meta information can be incorporated by ﬁrst- order logic in Logit-LDA [22] and score functions in SC- LDA [23]. However, the ﬁrst-order logic and score functions need to be deﬁned\n",
      "Candidate Chunk: are parallel and orthogonal to thelevel curves). Also shown in Fig. 2(a) is the family of solutions that are obtained asvaries from 0 to(red path). Notice that these solutions are not sparse in thesense (i.e., they do not intersect either of the two axes) untilis so large that the signal reconstruction is quite poor. This is the cas e regardless of how smallis, and is due both to the failure of thenorm to approximate thepseudonorm and to the inability of the discrete model to account for continuous event times. In the following sections, we de- velop and demonstrate a proposed solution for these problems. IV. C ONTINUOUS BASIS PURSUIT The motivation for the discretization of the inference problem was tractability. Equation (5) ap proximates the original model well in the limit asgoes to 0, but it is only tractably solv- able in regimes whereis large enough so that correlations are limited and-based relaxations can be employed. In this section, we augment the discrete model by incorporating vari- ables to\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "for x in outputs[1]:\n",
    "    if type(x[1])==dspy.Prediction:\n",
    "        all_predictions.append(x[1].resolved)\n",
    "    else:\n",
    "        all_predictions.append(np.nan)\n",
    "    \n",
    "\n",
    "all_labels = [x[0].cites for x in outputs[1]]\n",
    "print(len(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./predictions_one_shot_prompt_1000.txt', 'w') as f:\n",
    "    for pred in all_predictions:\n",
    "        f.write(str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651\n"
     ]
    }
   ],
   "source": [
    "print(len(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Recall:  0.07\n",
      "Precision: 0.21\n",
      "F1 Score: 0.10\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the final predictions\n",
    "correct_predictions = [prediction == label for prediction, label in zip(all_predictions, all_labels)]\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Compute the recall of the final predictions\n",
    "true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "false_negatives = sum([not prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "print(f'Recall: {recall: .2f}')\n",
    "\n",
    "# Compute the precision of the final predictions\n",
    "true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "false_positives = sum([prediction and not label for prediction, label in zip(all_predictions, all_labels)])\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "print(f'Precision: {precision:.2f}')\n",
    "\n",
    "# F1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
