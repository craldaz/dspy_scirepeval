{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning for Citation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# from operator import add\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from dspy.evaluate import Evaluate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_candidate_data = pd.read_csv('~/test.qrel.cid', sep=' ', header=None, names=['query', 'candidate', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(query_papers): 115\n",
      "len(candidate_papers): 637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('/Users/jamie/qpaper_to_emb', 'r') as f:\n",
    "    query_papers = [line.strip() for line in f]\n",
    "\n",
    "with open('/Users/jamie/cpaper_to_emb', 'r') as f:\n",
    "    candidate_papers = [line.strip() for line in f]\n",
    "\n",
    "print(f'len(query_papers): {len(query_papers)}')\n",
    "print(f'len(candidate_papers): {len(candidate_papers)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     query candidate  bool\n",
      "0  3498240   1824499     1\n",
      "1  3498240  53645322     0\n",
      "2  3498240   1915951     0\n",
      "3  3498240   3048298     0\n",
      "4  3498240   3627503     0\n",
      "Number of query candidate pairs with valid files: 651\n"
     ]
    }
   ],
   "source": [
    "counter_4 = 0\n",
    "valid_rows = pd.DataFrame()\n",
    "query_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers'\n",
    "candidate_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined'\n",
    "# Iterate over the rows of the data\n",
    "for _, row in query_candidate_data.iterrows():\n",
    "    query_file = os.path.join(query_dir, str(row['query']) + '.pdf')\n",
    "    candidate_file = os.path.join(candidate_dir, str(row['candidate']) + '.pdf')\n",
    "\n",
    "    # Check if both files exist\n",
    "    if os.path.isfile(query_file) and os.path.isfile(candidate_file):\n",
    "        # If both files exist, append the row to valid_rows\n",
    "        valid_rows = valid_rows._append(row)\n",
    "        \n",
    "# Reset the index of valid_rows\n",
    "valid_rows.reset_index(drop=True, inplace=True)\n",
    "print(valid_rows.head())\n",
    "print(f'Number of query candidate pairs with valid files: {len(valid_rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"query_file\": query_file, \"candidate_file\": candidate_file, \"cites\": bool(bool_)} for query_file, candidate_file, bool_ in zip(valid_rows['query'], valid_rows['candidate'], valid_rows['bool'])]\n",
    "data = [dspy.Example(**x).with_inputs('query_file', 'candidate_file') for x in data]\n",
    "\n",
    "def split_data(data, split_ratio, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    split_index = int(split_ratio * len(data))\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "    trainset = [data[i] for i in train_indices]\n",
    "    testset = [data[i] for i in test_indices]\n",
    "    return trainset, testset\n",
    "\n",
    "# trainset, testset = split_data(data, 0)\n",
    "trainset = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "dspy.settings.configure(lm=llm, rm=None)\n",
    "\n",
    "client = OpenAI(\n",
    "    # this is also the default, it can be omitted\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunker:\n",
    "    def __init__(self, context_window=3000, max_windows=5):\n",
    "        self.context_window = context_window\n",
    "        self.max_windows = max_windows\n",
    "        self.window_overlap = 0.02\n",
    "\n",
    "    def __call__(self, paper):\n",
    "        snippet_idx = 0\n",
    "\n",
    "        while snippet_idx < self.max_windows and paper:\n",
    "            endpos = int(self.context_window * (1.0 + self.window_overlap))\n",
    "            snippet, paper = paper[:endpos], paper[endpos:]\n",
    "\n",
    "            next_newline_pos = snippet.rfind('\\n')\n",
    "            if paper and next_newline_pos != -1 and next_newline_pos >= self.context_window // 2:\n",
    "                paper = snippet[next_newline_pos+1:] + paper\n",
    "                snippet = snippet[:next_newline_pos]\n",
    "\n",
    "            yield snippet_idx, snippet.strip()\n",
    "            snippet_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\", save_file=None):\n",
    "    if save_file and Path(save_file).exists():\n",
    "        with open(save_file, 'r') as f:\n",
    "            # print(f\"Loading embeddings from {save_file}\")\n",
    "            embeddings = [ast.literal_eval(line.strip()) for line in f]\n",
    "        return embeddings\n",
    "        \n",
    "    try:\n",
    "        response = client.embeddings.create(input=texts, model=model)\n",
    "        embeddings = [embedding.embedding for embedding in response.data]\n",
    "        if save_file: # Save the embeddings to a file\n",
    "            with open(save_file, 'w') as f:\n",
    "                # print(f\"Saving embeddings to {save_file}\")\n",
    "                for embedding in embeddings:\n",
    "                    f.write(str(embedding) + '\\n')\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(\"Error during API call:\", e)\n",
    "        return []\n",
    "    \n",
    "def get_most_similar_chunk(query_embedding, candidate_embeddings, candidate_chunks):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return candidate_chunks[most_similar_idx]\n",
    "    \n",
    "    \n",
    "class PredictCitation(dspy.Signature):\n",
    "    __doc__ = \"\"\"Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\"\"\"   \n",
    "    query_chunk: str = dspy.InputField(desc='Query chunk to compare to the candidate chunk.')\n",
    "    candidate_chunk: str = dspy.InputField(desc='Candidate chunk to compare to the query chunk.')\n",
    "    answer: bool = dspy.OutputField(desc=\"either True or False\", prefix=\"Answer:\")\n",
    "\n",
    "\n",
    "class PredictCitationAndResolve(dspy.Module):\n",
    "    def __init__(self, context_window=3000, max_windows=5, resolve_function=any,\n",
    "                 candidate_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined', \n",
    "                 query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers',\n",
    "                 reset_embedding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chunk = Chunker(context_window=context_window, max_windows=max_windows)\n",
    "        # self.predict = dspy.TypedPredictor(PredictCitation)\n",
    "        # self.predict = dspy.TypedChainOfThought(PredictCitation)\n",
    "        self.predict = dspy.ChainOfThought(PredictCitation)\n",
    "        self.resolve_function = resolve_function\n",
    "        self.query_folder = query_folder\n",
    "        self.candidate_folder = candidate_folder\n",
    "        os.makedirs('embeddings', exist_ok=True)\n",
    "        if reset_embedding:\n",
    "            for emb_file in os.listdir('embeddings'):\n",
    "                os.remove(f'embeddings/{emb_file}')\n",
    "\n",
    "    def forward(self, query_file, candidate_file):\n",
    "        predictions = []\n",
    "        \n",
    "        # Get the text from the pdfs\n",
    "        query_pdf = PdfReader(f'{self.query_folder}/{query_file}.pdf')\n",
    "        query_text = \"\"\n",
    "        for page in query_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "        query_text = query_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        candidate_pdf = PdfReader(f'{self.candidate_folder}/{candidate_file}.pdf')\n",
    "        candidate_text = \"\"\n",
    "        for page in candidate_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                candidate_text += page_text + \" \"\n",
    "        candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # for each chunk in the paper\n",
    "        query_chunks = [snippet for _, snippet in self.chunk(query_text)]\n",
    "        candidate_chunks = [snippet for _, snippet in self.chunk(candidate_text)]\n",
    "        \n",
    "        # Create embeddings for the chunks\n",
    "        candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "        query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "        \n",
    "        for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "            # Get the candidate chunk that is most similar to the snippet\n",
    "            candidate_chunk = get_most_similar_chunk(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "            prediction = self.predict(query_chunk=snippet, candidate_chunk=candidate_chunk)\n",
    "            # print(prediction)\n",
    "            predictions.append(prediction.answer=='True')\n",
    "\n",
    "        return dspy.Prediction(predictions=predictions, resolved=self.resolve_function(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_chunking = PredictCitationAndResolve(max_windows=15, context_window=1000, reset_embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker = Chunker(context_window=1000, max_windows=15)\n",
    "# query_pdf = PdfReader(f'/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers/1323414.pdf')\n",
    "# query_text = \"\"\n",
    "# for page in query_pdf.pages:\n",
    "#     page_text = page.extract_text()\n",
    "#     if page_text:\n",
    "#         query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "# query_text = query_text.replace(\"\\n\", \" \")\n",
    "# query_chunks = [snippet for _, snippet in chunker(query_text)]\n",
    "# print(query_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(query_chunks[0]))\n",
    "# print(len(query_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get an example\n",
    "# example = trainset[-2]\n",
    "# example_x = example.inputs()\n",
    "# example_y = example.labels()\n",
    "# print(example_x)\n",
    "# print(example_y)\n",
    "\n",
    "# prediction = pipeline_chunking(**example_x)\n",
    "# print(prediction)\n",
    "# print(example_y.cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, result):\n",
    "    '''Match metric'''\n",
    "    return 1 if example.cites == result.resolved else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate = Evaluate(devset=trainset, metric=metric, num_threads=8, display_progress=True, display_table=0, max_errors=100, return_outputs=True)\n",
    "# outputs = evaluate(pipeline_chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions = []\n",
    "# for x in outputs[1]:\n",
    "#     if type(x[1])==dspy.Prediction:\n",
    "#         all_predictions.append(x[1].resolved)\n",
    "#     else:\n",
    "#         all_predictions.append(np.nan)\n",
    "    \n",
    "\n",
    "# all_labels = [x[0].cites for x in outputs[1]]\n",
    "# print(len(all_predictions))\n",
    "\n",
    "# with open('darwin/eval/predictions_COT_large_prompt_1000.txt', 'w') as f:\n",
    "#     for pred in all_predictions:\n",
    "#         f.write(str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the accuracy of the final predictions\n",
    "# correct_predictions = [prediction == label for prediction, label in zip(all_predictions, all_labels)]\n",
    "# accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# # Compute the recall of the final predictions\n",
    "# true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# false_negatives = sum([not prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# recall = true_positives / (true_positives + false_negatives)\n",
    "# print(f'Recall: {recall: .2f}')\n",
    "\n",
    "# # Compute the precision of the final predictions\n",
    "# true_positives = sum([prediction and label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# false_positives = sum([prediction and not label for prediction, label in zip(all_predictions, all_labels)])\n",
    "# precision = true_positives / (true_positives + false_positives)\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "\n",
    "# # F1 score\n",
    "# f1 = 2 * (precision * recall) / (precision + recall)\n",
    "# print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_predictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PdfReader('darwin/query_papers/53079158.pdf').pages[-1].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the dataset where dspy will retrieve from\n",
    "#### Each sample has the following format. \"Query Chunk: ...\\n Candidate Chunk: ...\\n Answer: ...\\n  \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_papers = []\n",
    "r_papers = []\n",
    "with open('/Users/jamie/link-recorder-final-1', 'r') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        tpaper = temp[0].strip()\n",
    "        rpaper = temp[1].strip()\n",
    "        test_papers.append(tpaper)\n",
    "        r_papers.append(rpaper)\n",
    "test_retrieved_data = pd.DataFrame({'tpaper': test_papers, 'rpaper': r_papers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tpaper     rpaper\n",
      "0  16897790     252854\n",
      "1   2538574  253145517\n",
      "2  11633392  113541825\n",
      "3   4655781    2011582\n",
      "4   6833818  189960050\n",
      "Number of query candidate pairs with valid files: 738\n"
     ]
    }
   ],
   "source": [
    "valid_rows_retrieved = pd.DataFrame()\n",
    "retrieved_dir = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/r-paper-final'\n",
    "for _, row in test_retrieved_data.iterrows():\n",
    "    test_file_1 = os.path.join(query_dir, str(row['tpaper']) + '.pdf')\n",
    "    test_file_2 = os.path.join(candidate_dir, str(row['tpaper']) + '.pdf')\n",
    "    r_file = os.path.join(retrieved_dir, str(row['rpaper']) + '.pdf')\n",
    "\n",
    "    # Check if both files exist\n",
    "    if (os.path.isfile(test_file_1) or os.path.isfile(test_file_2)) and os.path.isfile(r_file):\n",
    "        # If both files exist, append the row to valid_rows\n",
    "        valid_rows_retrieved = valid_rows_retrieved._append(row)\n",
    "valid_rows_retrieved.reset_index(drop=True, inplace=True)\n",
    "print(valid_rows_retrieved.head())\n",
    "print(f'Number of query candidate pairs with valid files: {len(valid_rows_retrieved)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     query  candidate label\n",
      "0  6869636  235125640     1\n",
      "1  6869636  114250615     0\n",
      "2  6869636  111334241     0\n",
      "3  6869636  189960050     0\n",
      "4  6869636    2439435     0\n",
      "Number of query candidate pairs in dspy retrieval set: 900\n"
     ]
    }
   ],
   "source": [
    "# Randoly select 100 to set up the retrieval dataset\n",
    "dspy_r_set = pd.DataFrame(columns=['query', 'candidate', 'label'])\n",
    "valid_r_papers = valid_rows_retrieved['rpaper'].to_numpy()\n",
    "sampled_df = valid_rows_retrieved.sample(n=100)\n",
    "for _, row in sampled_df.iterrows():\n",
    "    test_set_paper = row['tpaper']\n",
    "    retrieved_paper = row['rpaper']\n",
    "    new_row = {'query': test_set_paper, 'candidate': retrieved_paper, 'label': 1}\n",
    "    dspy_r_set = pd.concat([dspy_r_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Get 8 negative samples for one positive sample as in the SPECTER svm experiment.\n",
    "    neg_papers = np.random.choice(valid_r_papers, size=8)\n",
    "    for neg_p in neg_papers:\n",
    "        new_row = {'query': test_set_paper, 'candidate': neg_p, 'label': 0}\n",
    "        dspy_r_set = pd.concat([dspy_r_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "print(dspy_r_set.head())\n",
    "print(f'Number of query candidate pairs in dspy retrieval set: {len(dspy_r_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return candidate_chunks[most_similar_idx], candidate_embeddings[most_similar_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_emb_idx(query_embedding, candidate_embeddings):\n",
    "    similarities = np.dot(candidate_embeddings, query_embedding) / (norm(candidate_embeddings, axis=1) * norm(query_embedding))\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return most_similar_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce retrieval set\n",
    "query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers'\n",
    "query_folder_2 = '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined'\n",
    "candidate_folder= '/Users/jamie/s2-folks/examples/python/get_open_access_pdf/r-paper-final'\n",
    "chunk = Chunker(context_window=1000, max_windows=15)\n",
    "dspy_r_emb = []\n",
    "dspy_r_text = []\n",
    "for _, row in dspy_r_set.iterrows():\n",
    "    # Get the text from the pdfs\n",
    "    query_file_1 = os.path.join(query_folder, str(row['query']) + '.pdf')\n",
    "    query_file_2 = os.path.join(query_folder_2, str(row['query']) + '.pdf')\n",
    "    if os.path.isfile(query_file_1):\n",
    "        query_file_path = query_file_1\n",
    "    if os.path.isfile(query_file_2):\n",
    "        query_file_path = query_file_2\n",
    "    try:\n",
    "        query_pdf = PdfReader(query_file_path)\n",
    "    except:\n",
    "        cotinue\n",
    "    query_text = \"\"\n",
    "    for page in query_pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "    query_text = query_text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    candidate_file = row['candidate']\n",
    "    try:\n",
    "        candidate_pdf = PdfReader(f'{candidate_folder}/{candidate_file}.pdf')\n",
    "    except:\n",
    "        continue\n",
    "    candidate_text = \"\"\n",
    "    for page in candidate_pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            candidate_text += page_text + \" \"\n",
    "    candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # for each chunk in the paper\n",
    "    query_chunks = [snippet for _, snippet in chunk(query_text)]\n",
    "    candidate_chunks = [snippet for _, snippet in chunk(candidate_text)]\n",
    "    \n",
    "    # Create embeddings for the chunks\n",
    "    candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "    query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "    \n",
    "    for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "        # Get the candidate chunk that is most similar to the snippet\n",
    "        candidate_chunk, c_emb = get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "        dspy_r_emb.append((query_embedding, c_emb, row['label']))\n",
    "        dspy_r_text.append((snippet, candidate_chunk, row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1080)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dspy_r_emb), len(dspy_r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_r_emb_concat = []\n",
    "for q_emb, c_emb, label in dspy_r_emb:\n",
    "    concat_emb = q_emb + c_emb\n",
    "    dspy_r_emb_concat.append(concat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCitationWithRetrieval(dspy.Signature):\n",
    "    __doc__ = \"\"\"Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\"\"\"   \n",
    "    query_chunk: str = dspy.InputField(desc='Query chunk to compare to the candidate chunk.')\n",
    "    candidate_chunk: str = dspy.InputField(desc='Candidate chunk to compare to the query chunk.')\n",
    "    answer: bool = dspy.OutputField(desc=\"either True or False\", prefix=\"Answer:\")\n",
    "    context: str = dspy.InputField(desc=\"A good example to learn from.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCitationRetrieveAndResolve(dspy.Module):\n",
    "    def __init__(self, context_window=3000, max_windows=5, resolve_function=any,\n",
    "                 candidate_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/cand_papers_combined', \n",
    "                 query_folder='/Users/jamie/s2-folks/examples/python/get_open_access_pdf/query_papers',\n",
    "                 reset_embedding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chunk = Chunker(context_window=context_window, max_windows=max_windows)\n",
    "        # self.predict = dspy.TypedPredictor(PredictCitation)\n",
    "        # self.predict = dspy.TypedChainOfThought(PredictCitation)\n",
    "        self.predict = dspy.Predict(PredictCitationWithRetrieval)\n",
    "        self.resolve_function = resolve_function\n",
    "        self.query_folder = query_folder\n",
    "        self.candidate_folder = candidate_folder\n",
    "        os.makedirs('embeddings', exist_ok=True)\n",
    "        if reset_embedding:\n",
    "            for emb_file in os.listdir('embeddings'):\n",
    "                os.remove(f'embeddings/{emb_file}')\n",
    "\n",
    "    def forward(self, query_file, candidate_file):\n",
    "        predictions = []     \n",
    "        # Get the text from the pdfs\n",
    "        query_pdf = PdfReader(f'{self.query_folder}/{query_file}.pdf')\n",
    "        query_text = \"\"\n",
    "        for page in query_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                query_text += page_text + \" \"  # Adding space to separate text between pages\n",
    "        query_text = query_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        candidate_pdf = PdfReader(f'{self.candidate_folder}/{candidate_file}.pdf')\n",
    "        candidate_text = \"\"\n",
    "        for page in candidate_pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                candidate_text += page_text + \" \"\n",
    "        candidate_text = candidate_text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # for each chunk in the paper\n",
    "        query_chunks = [snippet for _, snippet in self.chunk(query_text)]\n",
    "        candidate_chunks = [snippet for _, snippet in self.chunk(candidate_text)]\n",
    "        \n",
    "        # Create embeddings for the chunks\n",
    "        candidate_embeddings = get_embeddings(candidate_chunks, save_file=f'embeddings/candidate_{candidate_file}.emb')\n",
    "        query_embeddings = get_embeddings(query_chunks, save_file=f'embeddings/query_{query_file}.emb')\n",
    "        \n",
    "        for snippet, query_embedding in zip(query_chunks, query_embeddings):\n",
    "            # Get the candidate chunk that is most similar to the snippet\n",
    "            candidate_chunk, candidate_chunk_emb = get_most_similar_chunk_emb(query_embedding, candidate_embeddings, candidate_chunks)\n",
    "            original_emb_concat = query_embedding + candidate_chunk_emb\n",
    "            context_idx = get_most_similar_emb_idx(original_emb_concat, dspy_r_emb_concat)\n",
    "            context_text = dspy_r_text[context_idx]\n",
    "            context = f\"Query Chunk: {context_text[0]}\\nCandidate Chunk: {context_text[1]}\\nAnswer: {context_text[2]}\\n\"\n",
    "            prediction = self.predict(query_chunk=snippet, candidate_chunk=candidate_chunk, context=context)\n",
    "            # print(prediction)\n",
    "            predictions.append(prediction.answer=='True')\n",
    "        return dspy.Prediction(context=context, predictions=predictions, resolved=self.resolve_function(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(example, result):\n",
    "    '''Match metric'''\n",
    "    return 1 if example.cites == result.resolved else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_chunking_retrieval = PredictCitationRetrieveAndResolve(max_windows=15, context_window=1000, reset_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=trainset, metric=metric, num_threads=8, display_progress=True, display_table=0, max_errors=100, return_outputs=True)\n",
    "outputs = evaluate(pipeline_chunking_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Socket operation on non-socket\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: ersity of Verona. M.A. is supported by an APIF grant from the University of Barcelona. P.R. is partly supported by an ICREA Academia grant.Other studies shows that clothing correlates with the personality traits of people in a way that people with formal clothing perceive actions and objects, the inter-relationship and the intra-relationship between them in a more meaningful manner [36]. Clothing can make a person feel comfortable or not in a social situation [16] and can be considered as a determinant of how long it takes for strangers to trust one and how much they may trust them [40]. Aforementioned studies are evidences of the importance of clothing in so- cial signaling. Arguably, clothing can be considered as the most evident blueprint of individuals, which is completely dependent on their conscious choices, is not as transient as a gesture, and is more evident than any micro-signals such as a sarcastic smile among the facial expressions. Various experiments have been previously performed to measure\n",
      "\n",
      "Candidate Chunk: e may informally regard α(k) ias be- ing a deep summary of the preﬁx x0:i, it actually depends indirectly on all of x(except when k= 1). 5 The Arc Weight Function Given the vector γi:j, we can now compute the weight of the edit arc i,h/circlecopyrts:t−→j,h/prime/circlecopyrtinG, namelyw= f(s,t,h,h/prime,x,i,j). Many reasonable functions are possible. Here we use one that is inspired by the log- bilinear language model (Mnih and Hinton, 2007): w= (es,γi,j,exi,exj+1)·rt,h,h/prime,type(s:t) (1) The ﬁrst argument to the inner product is an em- bedding es∈Rd(1)of the source substring s, con-catenated to the edit’s neural context and also (for good measure) its local context.3For example, if |s|= 1, i.e.sis a single character, then we would use the embedding of that character as es. Note that the embeddings esfor|s|= 1 are also used to encode the local context characters and the level- 1 BiLSTM input. We learn these embeddings, and they form part of our model’s parameter vector θ. The second argument is a joint\n",
      "\n",
      "Context:\n",
      "Query Chunk: the experts. In [ 16], the expert networks are taken as parallel layers and mapped directly onto the outputs with the generalist. In [ 1,19], the output of the model is the distribution given by taking the weighted some of the distributions of the experts by the distribution given by the generalist over the coarse classes. Our model as in Figure 1 can be considered as a simple form of [ 1,19], in the sense that the generalist outputs a delta, and we simply take the output of the relevant expert as the output. Our model as in 2 is somewhat different than any of these, as an expert outputs feature vector that is fed to a shared decoder. Concurrent with this work is [ 14], which also puts a hard mixture model as a component in a deep learning architecture for large scale language modeling. In this work, we work on a different problem domain, and we use a simpler gating scheme. 4. Experiments We will discuss three sets of experiments. In the first, we train and test on 1000 category labeled Imagenet [ 13]. In\n",
      "Candidate Chunk: (x) :=Wdr(x) +bd; generateSstandard normal samples \u000fK\u0018N(0K;IK\u0002K),\u000fR\u0018N(0R;IR\u0002R); ifis-parameter-efﬁcient then compute heteroscedastic low-rank component v(x) :=Wvr(x) +bv; load homoscedastic low-rank component V; U(x) :=\u0016(x) +d(x)\f",
      "\u000fK+v(x)\f",
      "V\u000fR; else compute low-rank parameters V(x) =WVr(x) +bV; V(x) :=reshape (V(x);[K;R]); U(x) :=\u0016(x) +d(x)\f",
      "\u000fK+V(x)\u000fR; end pc=mean (softmax\u001c",
      "U(x);axis = 1)[c] image classiﬁcation datasets. For example, for Imagenet- 21kthe number of parameters required to compute V(x) is44:8M, a 50\u0002reduction. See Algorithm 1 for a full speciﬁcation of our method. 3. Related work 3.1. Heteroscedastic modelling Heteroscedastic regression. Heteroscedastic regression is common in the Gaussian Processes [ 45] and economet- rics literature [ 42]. Bishop and Quazaz [3]introduced a heteroscedastic regression model where a neural network outputs the mean and variance parameters of a Gaussian like- lihood:y\u0018N(\u0016(x);\u001b(x)2). The negative log-likelihood of the model is particularly amenable to interpretation\n",
      "Answer: 1\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: the life of signs in society” [6]. Semiotics investigates signs and analyzes them to provide signiﬁcance to a speciﬁc problem. There are three main elements in semiotics: the sign, what it refers to, and the people who use it. The people as social species and biological entities, instinctively evolved to survive better through facilitating living in a disciplined society by deﬁning new signs and giving them an appropriate interpretation. Social semiotics is a subcategory of semiotics that studies how people design and interpret meanings and how these meanings are shaped by a speciﬁc social situation [15]. In social semiotics, the term resource is preferred over the term sign and represents a used signiﬁer by the people to produce and to interpret communicative artefacts. In this respect, social semiotics is particularly useful in disclosing unnoticed signiﬁcance and functionality of social resources and each individual is a semiotician, since everybody constantly interprets the meaning of signs around the\n",
      "\n",
      "Candidate Chunk: e perception, memory imagery, and in the context of tangible reward and punishment, and using the electroencephalograph (EEG) and functional magnetic resonance imaging (fMRI), explores the brain’s motivational circuits that determine human emotion. The general thesis examined here is that experienced emotions are founded on the activation of neural circuits that evolved in the mammalian brain to ensure the survival of individuals and their progeny. Primitively, these motive circuits were engaged by external stimuli that are appetitive and potentially life sustaining, or alternatively, represent threats to the organism’s survival. The psychobiological consequences of this neural firing are potentially twofold: On the one hand, they engage sensory systems that increase attention and facilitate perceptual processing, and on the other, they initiate reflex responses that mobilize the organism and prompt motor action. Research with animals (e.g., Davis, 2000; Fanselow & Poulos, 2005; Kapp, Supple & Whalen, 199\n",
      "\n",
      "Context:\n",
      "Query Chunk: educe the computation time and make the search process focus on the intensification phase,weuseadecreasingacceptancethresholdtoactasthecoolingprocedure of simulated annealing. The procedure of theproposedSA VNSalgorithmisillustratedin Algorithm1 . In the PSOVNSproposed by Tasgetiren et al. [ 31], the stochastic VNS is applied on the global best particle foundat each iteration. However, a drawback of such applicationis that the starting point of the stochastic VNS may be thesame solution if the global best particle cannot be improvedforanumberofconsecutiveiterations,andconsequentlytheexploration ability of the PSO may be decreased. Therefore,for a given population at iteration 𝑡,w ep r o p o s et ou s et h e followingstrategy. Step1.ApplytheSA VNSonthepromisingparticlessatisfy- ing(𝑓(𝜋𝑡)−𝑓(𝑔best))/𝑓(𝑔best) ≤0.02,i nw h i c h𝑓(𝜋𝑡)is the objectivevalueofparticle 𝜋𝑡. Step 2.Update the global best particle. If a new global best particleisfound,thenfurtherimproveitusingtheSA VNS. 2.5. Population Update Method.\n",
      "Candidate Chunk: highly in this ability will be very  sensitive to the emotions of others and predict  the emotional responses of others.  Regulation  of Emotion which means  the regulation of  emotion in oneself that relates to the ability of  a person to regulate his or her emotions,  enabling a more rapid recovery from  psychological mood swings either positive or  negative. A person with high ability in this  area would be able to return quickl y to a  normal psychological state after rejoicing or  experiencing distress. People who rate highly  in this ability would be less likely to lose their  temper. Use of Emotion which  means the use  of emotion to improve performance that  relates to an individual 's ability to make use of  his or her emotions by directing them toward  constructive activities and personal performance. A person who is highly capable  in this dimension maintains positive emotions  most of the time and is able to encourage him -  or herself to do better continuously (3, 9).   Emotional intelligenc\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: the life of signs in society” [6]. Semiotics investigates signs and analyzes them to provide signiﬁcance to a speciﬁc problem. There are three main elements in semiotics: the sign, what it refers to, and the people who use it. The people as social species and biological entities, instinctively evolved to survive better through facilitating living in a disciplined society by deﬁning new signs and giving them an appropriate interpretation. Social semiotics is a subcategory of semiotics that studies how people design and interpret meanings and how these meanings are shaped by a speciﬁc social situation [15]. In social semiotics, the term resource is preferred over the term sign and represents a used signiﬁer by the people to produce and to interpret communicative artefacts. In this respect, social semiotics is particularly useful in disclosing unnoticed signiﬁcance and functionality of social resources and each individual is a semiotician, since everybody constantly interprets the meaning of signs around the\n",
      "\n",
      "Candidate Chunk: Leonardos—Research supported by ERC project CODAMODA. N. Leonardos—Work completed while at the National and Kapodistrian University of Athens. The full version of this paper can be found at the Cryptology ePrint Archive [ 22]. International Association for Cryptologic Research 2015 E. Oswald and M. Fischlin (Eds.): EUROCRYPT 2015, Part II, LNCS 9057, pp. 281–310, 2015.DOI: 10.1007/978-3-662-46803-6 10 282 J. Garay et al. a protocol that maintains and extends a distributed data structure called the blockchain . The protocol requires from miners to solve a “proof of work” (POW, aka “cryptographic puzzle” — see, e.g., [ 4,16,24,38]), which essentially amounts to brute-forcing a hash inequality based on SHA-256, in order to generate new blocks for the blockchain. The blocks that comprise the blockchain contain sets oftransactions that are generated at will by owners of bitcoins, who issue transac- tions that credit any entity of their choice who accepts payments in bitcoin. Pay- ers broadcast transactions and\n",
      "\n",
      "Context:\n",
      "Query Chunk: e\f",
      "k;v=QLword l0=1\u000egv;l0 l0;k d) Draw\u001e",
      "k\u0018DirV(\f",
      "k) 2) For each document d: a) For each topic k: Compute\u000b",
      "d;k=QLdoc l=1\u0015fd;l l;k b) Draw\u0012d\u0018DirK(\u000b",
      "d) c) For each word in document d: i) Draw topic zd;i\u0018CatK(\u0012d) ii) Draw word wd;i\u0018CatV(\u001e",
      "zd;i) where Ga (\u0001;\u0001), Dir(\u0001), Cat (\u0001)are the gamma distribution, the Dirichlet distribution, and the categorical distribution respectively. K,\u00160, and\u00170are the hyper-parameters. To incorporate document labels, MetaLDA learns a spe- ciﬁc Dirichlet prior over the topics for each document by using the label information. Speciﬁcally, the information of documentd’s labels is incorporated in \u000b",
      "d, the parameter of Dirichlet prior on \u0012d. As shown in Step 2a, \u000b",
      "d;kis computed as a log linear combination of the labels fd;l. Sincefd;lis binary,\u000b",
      "d;kis indeed the multiplication of \u0015l;kover all the active labels of document d, i.e.,fljfd;l= 1g. Drawn from the gamma distribution with mean 1, \u0015l;kcontrols the impact of label lon topick. If labellhas no or less impact on topic k,\u0015l;kis expected to be\n",
      "Candidate Chunk: gation & correlation [97], anti - malware [70, 77], architecture [24, 48, 53, 82, 85],  authenti cation & authorization [6, 19, 20, 23, 48, 55, 62, 64,  250  Internat ional Journal of Communication Networks and Information Security (IJCNIS)                                       Vol. 11, No. 2, August  2019     66, 70, 78, 82, 84, 85, 92, 98], automata [93], binary function  [42], certificate [11, 20, 68, 77, 82, 83, 92], circuit defense  [1, 7], coding [66, 78, 79], configuration [1, 75], context - based [44], e lliptic curve cryptography [11, 47, 48, 99]  [100], encryption [7, 11, 19, 22, 47, 48, 55, 63, 66, 69, 70,  79, 92, 99 -102], forward security [48], framework [12, 18,  44, 45, 54, 60, 76], freshness [48], fuzzy logic [43], game  theory [81], general [1, 72], g roup signature [47], hardware  security [99], HARM [40], Hash [21, 47, 66], Homomorphic  [47, 78, 79, 97, 101], IPS/IDS [1, 43, 67, 68, 70], isolation  [1], key management [6, 22, 67, 69, 73, 78, 92, 102, 103],  matrix [42], pairing [86], p\n",
      "Answer: 0\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: the life of signs in society” [6]. Semiotics investigates signs and analyzes them to provide signiﬁcance to a speciﬁc problem. There are three main elements in semiotics: the sign, what it refers to, and the people who use it. The people as social species and biological entities, instinctively evolved to survive better through facilitating living in a disciplined society by deﬁning new signs and giving them an appropriate interpretation. Social semiotics is a subcategory of semiotics that studies how people design and interpret meanings and how these meanings are shaped by a speciﬁc social situation [15]. In social semiotics, the term resource is preferred over the term sign and represents a used signiﬁer by the people to produce and to interpret communicative artefacts. In this respect, social semiotics is particularly useful in disclosing unnoticed signiﬁcance and functionality of social resources and each individual is a semiotician, since everybody constantly interprets the meaning of signs around the\n",
      "\n",
      "Candidate Chunk: ue /)/+ /\u0015g /\u000e /( /!i /; gr e en /)/(/1/./2/)where the w eigh t /\u0015r /;g is adjusted to re/ ect our sp eci/ c observ ations on the colors of adjacen tv ertices/, and an y necessary readjustmen ts are made to the w eigh ts /\u0015r /; /\u0015b /; /\u0015g to resp ect ourearlier observ ations/. A t the exp ense of an increasing n um b er of parameters that need tob e adjusted/, an increasingly detailed set of features of the images can b e c haracterizedb y the distribution/. But whic h features should the mo del c haracterize/, and ho w shouldthe w eigh ts b e c hosen/? In this pap er w e presen t a general framew ork for addressing thesequestions/.As another illustration/, supp ose w e wish to automatically c haracterize sp ellings ofw ords according to a statistical mo del/; this is the application w e dev elop in Section /5/. A/ eld with no features is simply a uniform distribution on ASCI I strings /(where w e tak e thedistribution of string lengths as giv en/)/. The most conspicuous feature of English sp ellings/1 is\n",
      "\n",
      "Context:\n",
      "Query Chunk: ME smoothing performs as well as or better than all other algorithmsunder consideration. We contrast this method with previous -gram smoothing methods to explain its superior performance. Index Terms— Exponential models, language modeling, max- imum entropy, minimum divergence, -grammodels, smoothing. I. INTRODUCTION MAXIMUM entropy (ME) modeling has been success- fully applied to a wide range of domains, including language modeling as well as many other natural language tasks[2]–[5].Formanyproblems,thistypeof modelingcanbeviewed as maximum likelihood (ML) training for exponentialmodels, and like other ML methods is prone to overfitting of training data. While several smoothing methods for ME modelshavebeenproposedtoaddressthisproblem[1],[5]–[7],previous results do not make it clear how these smoothingmethods compare with smoothing methods for other types of related models. However,therehasabeengreatdealofresearchinsmoothing -gram language models, and it can be shown that ME -gram modelsarecloselyrelatedt\n",
      "Candidate Chunk: estimating the frequency of English bigrams. Consider a bigram that does not occur in the trainingdata,say,“ PIGDOG.”Wehave ,butin- tuitively we want since this bigram has some chance of occurring. This is an example of a property of 1063–6676/00$10.00 © 2000 IEEE 38 IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 8, NO. 1, JANUARY 2000 that we do not deem significant and thus do not want to match exactly with . However, let us assume that we observe that the word THEoccurs with frequency 0.05 in the training data, i.e., Because of the abundance of the word THE, this is presumably anaccurateestimateofthisfrequencyanditseemsreasonabletorequirethatourselecteddistribution satisfiestheanalogous constraints (1) More generally, we can select a number of nonnegative random variables or features and require that the expected value of each feature over the model is equal to that of the empirical distribution : (2) The constraints represented in (1) can be expressed with two such features, if otherwise for .\n",
      "Answer: 1\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predict if the two chunks are related by a citation. Consider all possible ways in which a citation could occur, such as direct quotes, paraphrasing, or referring to the same ideas or data. Don't be afraid to predict that the chunks are related by a citation. If you're not sure, it's better to predict that they are related.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Query Chunk: Query chunk to compare to the candidate chunk.\n",
      "\n",
      "Candidate Chunk: Candidate chunk to compare to the query chunk.\n",
      "\n",
      "Context: A good example to learn from.\n",
      "\n",
      "Answer: either True or False\n",
      "\n",
      "---\n",
      "\n",
      "Query Chunk: Clothing and People - A Social Signal Processing Perspective Maedeh Aghaei1, Federico Parezzan3, Mariella Dimiccoli1, Petia Radeva1;2, Marco Cristani3 1Faculty of Mathematics and Computer Science, University of Barcelona, Barcelona, Spain 2Computer Vision Center, Barcelona, Spain 3Department of Computer Science, University of Verona, Verona, Italy Abstract — In our society and century, clothing is not anymore used only as a means for body protection. Our paper builds upon the evidence, studied within the social sciences, that cloth- ing brings a clear communicative message in terms of social signals, inﬂuencing the impression and behaviour of others towards a person. In fact, clothing correlates with personality traits, both in terms of self-assessment and assessments that unacquainted people give to an individual. The consequences of these facts are important: the inﬂuence of clothing on the decision making of individuals has been investigated in the literature, showing that it represents a discriminativ\n",
      "\n",
      "Candidate Chunk: ily Mail dataset, our uniﬁed model achieves state-of-the- art ROUGE scores and outperforms a strong ex- tractive baseline (i.e., lead-3). Finally, to en- sure the quality of our uniﬁed model, we con- duct a solid human evaluation and conﬁrm that our method signiﬁcantly outperforms recent state-of- the-art methods in informativity and readability. To summarize, our contributions are twofold: \u000fWe propose a uniﬁed model combiningsentence-level and word-level attentions to take advantage of both extractive and abstrac- tive summarization approaches. \u000fWe propose a novel inconsistency loss func- tion to ensure our uniﬁed model to be mutu- ally beneﬁcial to both extractive and abstrac- tive summarization. The uniﬁed model with inconsistency loss achieves the best ROUGE scores on CNN/Daily Mail dataset and out- performs recent state-of-the-art methods in informativity and readability on human eval- uation. 2 Related Work Text summarization has been widely studied in re- cent years. We ﬁrst introduce the related w\n",
      "\n",
      "Context:\n",
      "Query Chunk: thosefoundinatrainingset.Whenconstrainingexpectationstoalternatevalues, theMEmodelwillnotbetheMLmodel,andtheMEmodelwillnotexistifthe constraints are inconsistent.Whilemodelswithhighentropy tendtoberatheruniformor smooth and we may only constrain properties of we con- sidersignificant,aMEmodelcanstilloverfittrainingdata,even whenthenumberofconstraintsissmall.Forexample,consider constraintsonthefrequencyoftheword MATEOandthebigram SAN MATEO , andassume that theword MATEOoccurs only after the word SANin the training data. Then, we will have and whichimplies forall .Intuitively, we want for all since all bigrams have some chance of occurring. Zero probabilities lead to infinite loss in log-lossobjectivefunctionsandcanleadtopoorperformancein manyapplications,e.g.,when representsalanguagemodel tobeusedinspeechrecognition.Thus,itisdesirableto smooth ME models, or adjust parameter values away from their ML estimates. II. SMOOTHING -GRAMLANGUAGE MODELS While there has been relatively little work in smoothing ME m\n",
      "Candidate Chunk: oconventional -grammodels.Con- sequently,thisdomainiswell-suitedtogaugingtheperformance of ME smoothing methods relative to other smoothing tech-niques. In this work, we survey previous work in ME smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing -gram language models. Evaluating the perplexity of each method over a largenumber of data sets, we find that fuzzy ME smoothing [1] per- formsaswellasorbetterthanallotheralgorithmsunderconsid- eration.Thismethodcanbeviewedasrelaxingtherequirement Manuscript receivedMarch 5, 1999; revised August 11, 1999. The associate editor coordinating the review of this manuscript and approving it for publica- tion was Dr. James R. Glass. S.F.CheniswiththeIBMT.J.WatsonResearchCenter,YorktownHeights, NY 10598 USA (e-mail: stanchen@watson.ibm.com). R.RosenfeldiswiththeSchoolofComputerScience,CarnegieMellonUni- versity, Pittsburgh, PA 15213 USA. Publisher Item Identifier S 1063-6676(00)00276-5.of exact constraint sati\n",
      "Answer: 1\n",
      "\n",
      "\n",
      "Answer:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.12s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.19s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.04s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.67s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.30s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.29s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.90s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.86s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.20s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.22s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.07s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.76s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 0.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
      "/opt/anaconda3/envs/nlu/lib/python3.9/site-packages/dsp/modules/gpt3.py:264: UserWarning: Persisting input arguments took 1.00s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return v1_cached_gpt3_turbo_request_v2(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
